# Chapter 7: 内存
系统主内存存储应用程序和内核指令、它们的工作数据和文件系统缓存。此数据的辅助存储通常是存储设备（磁盘），其运行速度要慢几个数量级。一旦主内存填满，系统就可以开始在主内存和存储设备之间切换数据。这是一个缓慢的过程，通常会成为系统瓶颈，从而显著降低性能。系统还可能终止内存消耗最大的进程，从而导致应用程序中断。

其他需要考虑的性能因素包括分配和释放内存、复制内存以及管理内存地址空间映射的 CPU 开销。在多套接字架构上，内存局部性可能成为一个因素，因为附加到本地套接字的内存比远程套接字具有更低的访问延迟

本章的学习目标是：

■ 了解内存概念。
■ 熟悉内存硬件内部结构。
■ 熟悉内核和用户分配器的内部结构。
■ 具备 MMU 和 TLB 的工作知识。
■ 遵循不同的内存分析方法。
■ 描述系统范围和每个进程的内存使用情况。
■ 确定由可用内存不足引起的问题。
■ 在进程地址空间和内核 slab 中找到内存使用情况。
■ 使用分析器、跟踪器和火焰图调查内存使用情况。
■ 了解内存的可调参数。

本章有五个部分，前三个部分为内存分析提供基础，后两个部分介绍其在基于 Linux 的系统中的实际应用。这些部分如下：

■ 背景介绍了与内存相关的术语和关键内存性能概念。
■ 架构 提供硬件和软件内存架构的通用描述。
■ 方法解释了性能分析方法。
■ 可观测性工具 介绍了用于内存分析的性能工具。
■ 优化 介绍了优化和可调参数示例。

CPU 上的内存缓存（级别 1/2/3，TLB）在第 6 章 “CPU”中介绍。

# 7.1 术语
作为参考，本章中使用的内存相关术语包括以下内容

■ 主内存：也称为物理内存，它描述了计算机的快速数据存储区域，通常以 DRAM 的形式提供。
■ 虚拟内存：主内存的抽象，它（几乎）是无限的，并且是无争用的。虚拟内存不是真正的内存。
■ 驻留内存：当前驻留在主内存中的内存。
■ 匿名内存：没有文件系统位置或路径名的内存。它包括进程地址空间（称为堆）的工作数据。
■ 地址空间：内存上下文。每个进程和内核都有虚拟地址空间。
■ Segment：为特定目的而标记的虚拟内存区域，例如用于存储可执行或可写页面。
■ 指令文本：指内存中的 CPU 指令，通常位于一个段中。
■ OOM：当内核检测到可用内存不足时，内存不足。
■ 页：操作系统和 CPU 使用的内存单位。从历史上看，它是 4 KB 或 8 KB。现代处理器对较大的尺寸具有多种页面大小支持。
■ Page fault：无效的内存访问。这些是使用按需虚拟内存时发生的正常情况。
■ 分页：主内存和存储设备之间的页面传输。
■ 交换：Linux 使用术语 swapping 来指代到交换设备的匿名分页（交换页的传输）。在 Unix 和其他操作系统中，交换是主内存和交换设备之间整个进程的传输。本书使用了该术语的 Linux 版本。
■ 交换：用于分页匿名数据的磁盘区域。它可以是存储设备上的一个区域（也称为物理交换设备）或文件系统文件（称为交换文件）。一些工具使用术语 swap 来指代虚拟内存（这令人困惑且不正确）。

本章介绍了其他术语。术语表包括基本术语以供参考，包括地址、缓冲区和 DRAM。另请参阅第 2 章和第 3 章中的术语部分

# 7.2 概念
以下是有关内存和内存性能的一些重要概念

# #7.2.1 虚拟内存
虚拟内存是一种抽象，它为每个进程和内核提供自己的大型线性私有地址空间。它简化了软件开发，将物理内存放置留给操进行管理。它还支持多任务处理（虚拟地址空间按设计进行分离）和超额订阅（使用中的内存可以扩展到主内存之外）。虚拟内存在第 3 章操作系统的 第 3.2.8 节 虚拟内存 中介绍。有关历史背景，请参见 [丹宁 70]

图 7.1 显示了虚拟内存在具有交换设备（辅助存储）的系统上对进程的作用。显示一页内存，因为大多数虚拟内存实现都是基于页面的。

进程地址空间由虚拟内存子系统映射到主内存和物理交换设备。内核可以根据需要在它们之间移动内存页，Linux 进程称为交换（其他操作系统称为匿名分页）。这允许 kernel 超额订阅 main memory。

内核可能会对超额订阅施加限制。常用的限制是主内存加上物理交换设备的大小。尝试超过此限制的内核可能会使分配失败。这种 “out of virtual memory” 错误乍一看可能会让人感到困惑，因为虚拟内存本身就是一种抽象资源。

Linux 还允许其他行为，包括对内存分配没有限制。这称为 overcommit，在以下有关分页和需求分页的章节中进行了介绍，这是 overcommit 工作所必需的。

# #7.2.2 分页
分页是页面移入和移出主内存的过程，分别称为 page-in 和 page-out。它于 1962 年由 Atlas Computer 首次引入 [Corbató 68]，允许：

■ 部分加载的程序执行 
■ 大于主内存的程序执行 
■ 在主内存和存储设备之间高效移动程序

这些能力在今天仍然适用。与早期的换出整个程序的技术不同，分页是一种管理和释放主内存的精细方法，因为页面大小单位相对较小（例如 4 KB）。

使用虚拟内存分页（分页虚拟内存）通过 BSD [Babaoglu 79] 引入 Unix，并成为标准

随着后来添加用于共享文件系统页面的页面高速缓存（参见第 8 章 文件系统），两种不同类型的分页变得可用：文件系统分页和匿名分页。

# 文件系统分页
文件系统分页是由内存映射文件中的页面的读取和写入引起的。对于使用文件内存映射的应用程序 （mmap（2）） 和使用页面高速缓存的文件系统 （大多数这样做;请参阅第 8 章 文件系统），这是正常行为。它被称为“好的”分页 [McDougall 06a]。

需要时，内核可以通过分页来释放内存。这就是术语变得有点棘手的地方：如果文件系统页面在主内存中被修改（称为 dirty），则页面输出将需要将其写入磁盘。相反，如果文件系统页面尚未被修改（称为 clean），则 page-out 仅释放内存以供立即重用，因为磁盘上已存在副本。因此，术语 page-out 表示页面已移出内存 — 这可能包括也可能不包含对存储设备的写入（您可能会看到术语 page-out 在其他文本中的定义不同）。

# 匿名分页 （交换）
匿名分页涉及进程专用的数据：进程堆和堆栈。它被称为匿名，因为它在操作系统中没有命名位置（即没有文件系统路径名）。匿名分页需要将数据移动到物理交换设备或交换文件。Linux 使用术语 swapping 来指代这种类型的分页。

匿名分页会损害性能，因此被称为“坏的”分页 [McDougall 06a]。当应用程序访问已分页的内存页时，它们会阻塞将它们读回主内存所需的磁盘 I/O。这是一个匿名的分页，

这会给应用程序带来同步延迟。匿名 page-outs 可能不会直接影响应用程序性能，因为它们可以由内核异步执行。

当没有匿名分页 （swapping） 时，性能最佳。这可以通过将应用程序配置为保持在可用的主内存内，并通过监控页面扫描、内存利用率和匿名分页来确保没有内存短缺的迹象来实现。

# #7.2.3 需求分页
支持按需分页的操作系统（大多数）会按需将虚拟内存页面映射到物理内存，如图 7.2 所示。这会将创建 mapping 的 CPU 开销推迟到实际需要和访问它们时，而不是在首次分配内存范围时。

图 7.2 中所示的序列从提供已分配内存的 malloc（） （步骤 1） 开始，然后是指向新分配内存的 store 指令（步骤 2）。为了让 MMU 确定存储的主内存位置，它会对内存页执行虚拟到物理查找（步骤 3），但失败，因为还没有 Map。此故障称为页面错误（步骤 4），它会触发内核创建按需映射（步骤 5）。稍后，可以将内存页分页到交换设备以释放内存（步骤 6）。

步骤 2 也可以是映射文件情况下的加载指令，该文件应包含数据，但尚未映射到此进程地址空间。

如果可以从内存中的另一个页面满足映射，则称为 Minor Fault。在进程的内存增长期间，从可用内存映射新页面时，可能会发生这种情况（如图所示）。映射到另一个现有页面时也可能发生此错误，例如从映射的共享库中读取页面

需要存储设备访问（图中未显示）的页面错误（如访问未缓存的内存映射文件）称为重大错误。

虚拟内存模型和需求分配的结果是，虚拟内存的任何页面都可能处于以下状态之一：

■ A. 未分配 
■ B. 已分配但未映射（未填充且尚未出错） 
■ C. 已分配并映射到主内存 （RAM） 
■ D. 已分配并映射到物理交换设备（磁盘）

如果页面由于系统内存压力而被分页，则达到状态 （D）。从 （B） 到 （C） 的过渡是页面错误。如果它需要磁盘 I/O，则为主要页面错误;否则，为轻微的页面错误。

从这些状态中，还可以定义两个内存使用术语：

■ 驻留集大小 （RSS）：分配的主内存页的大小 （C） 
■ 虚拟内存大小：所有分配区域的大小 （B C D）

需求分页通过 BSD 与 分页虚拟内存一起添加到 Unix 中。它已成为标准并被 Linux 使用。

# #7.2.4 超额使用
Linux 支持过量使用的概念，它允许分配的内存超过系统可能存储的内存，超过物理内存和交换设备的总和。它依赖于需求分页和应用程序不使用已分配太多内存的趋势

使用 overcommit 时，应用程序对内存的请求（例如 malloc（3））将成功，否则它们会失败。应用程序程序员可以慷慨地分配内存，然后根据需要稀疏地使用它，而不是保守地分配内存以保持在虚拟内存限制内。

在 Linux 上，可以使用可调参数配置超额分配的行为。有关详细信息，请参见 第 7.6 节 调优。overcommit 的后果取决于内核如何管理内存压力;请参见部分 7.3， 体系结构中对 OOM 终止程序的讨论。

# #7.2.5 进程交换
进程交换是整个进程在主内存和物理交换设备或交换文件之间的移动。这是用于管理主内存的原始 Unix 技术，也是术语交换 [Thompson 78] 的起源。

要换出进程，必须将其所有私有数据写入交换设备，包括进程堆（匿名数据）、其打开的文件表以及仅在进程处于活动状态时才需要的其他元数据。可以删除源自文件系统且尚未修改的数据，并在需要时再次从原始位置读取

进程交换会严重损害性能，因为已换出的进程需要大量磁盘 I/O 才能再次运行。对于当时的机器，例如 PDP-11，它的最大处理大小为 64 KB [Bach 86]，在早期的 Unix 上更有意义。（现代系统允许以 GB 为单位的进程大小。）

此描述是针对历史背景提供的。Linux 系统根本不交换进程，只依赖于分页。

# #7.2.6 文件系统缓存使用情况
系统引导后内存使用量增加是正常的，因为操作系统使用可用内存来缓存文件系统，从而提高性能。原则是： 如果有备用的主内存，就用它来做一些有用的事情。这可能会让天真的用户感到痛苦，因为他们看到可用的可用内存在启动后的某个时候缩减到接近零。但它不会对应用程序造成问题，因为内核应该能够在应用程序需要时快速从文件系统缓存中释放内存。

有关可能占用主内存的各种文件系统高速缓存的更多信息，请参见第 8 章 “文件系统”。

# #7.2.7 利用率和饱和率
主内存利用率可以计算为已用内存与总内存。文件系统缓存使用的内存可以被视为未使用，因为它可供应用程序重用。

如果内存需求超过主内存量，则主内存将达到饱和。然后，操作系统可以通过使用分页、进程交换（如果支持）以及在 Linux 上使用 OOM killer（稍后介绍）来释放内存。这些活动中的任何一个都是主内存饱和的指标。

如果系统对愿意分配的虚拟内存量施加限制（Linux 不会超额分配），也可以根据容量利用率来研究虚拟内存。如果是这样，一旦虚拟内存耗尽，内核将失败分配;例如，malloc（3） 失败，并将 errno 设置为 ENOMEM。

请注意，系统上当前可用的虚拟内存有时（令人困惑地）称为 available swap。

# #7.2.8 分配器
虽然虚拟内存处理物理内存的多任务处理，但虚拟地址空间中的实际分配和放置通常由分配器处理。这些要么是user-land 库，要么是基于内核的例程，它们为软件程序员提供了一个简单的内存使用接口（例如，malloc（3）、free（3））。

分配器对性能有重大影响，系统可能会提供多个用户级分配器库供您选择。它们可以通过使用包括每线程对象缓存在内的技术来提高性能，但如果分配变得碎片化和浪费，它们也会损害性能。部分 7.3， 体系结构中介绍了具体示例。

# #7.2.9 共享内存
内存可以在进程之间共享。这通常用于系统库，通过与使用只读指令文本的所有进程共享其只读指令文本的一个副本来节省内存。

这给显示每个进程主内存使用情况的可观测性工具带来了困难。在报告进程的总内存大小时，是否应该包括共享内存？Linux 使用的一种技术是提供额外的度量，即比例集大小 （PSS），其中包括私有内存（非共享）加上共享内存除以用户数。请参见第 7.5.9 节 pmap，了解可以显示 PSS 的工具

# #7.2.10 工作集大小
工作集大小 （WSS） 是进程经常用于执行工作的主内存量。这是一个用于内存性能调整的有用概念：如果 WSS 可以放入 CPU 高速缓存而不是主内存，则性能应该会大大提高。此外，如果 WSS 超过主内存大小，性能将大大降低，并且应用程序必须交换才能执行工作

虽然作为一个概念很有用，但在实践中很难衡量：可观测性工具中没有 WSS 统计数据（它们通常报告 RSS，而不是 WSS）。第 7.4.10 节 内存缩减 描述了 WSS 估计的实验方法，第 7.5.12 节 wss 展示了一个实验性的工作集大小估计工具 wss（8）。

# #7.2.11 字长
如第 6 章 CPU 中所述，处理器可以支持多种字长，例如 32 位和 64 位，从而允许软件运行。由于地址空间大小受字大小的可寻址范围限制，因此需要超过 4 GB 内存的应用程序对于 32 位地址空间来说太大，需要编译为 64 位或更高。

根据内核和处理器的不同，一些地址空间可能保留给内核地址，不可用于应用程序。一个极端的情况是字长为 32 位的 Windows，默认情况下为内核保留 2 GB，只留下 2 GB 用于应用程序 [Hall 09]。在 Linux（或启用了 /3GB 选项的 Windows）上，内核预留为 1 GB。对于 64 位字长（如果处理器支持），地址空间要大得多，内核保留应该不是问题。

根据 CPU 架构，使用更大的位宽也可以提高内存性能，因为指令可以在更大的字长上运行。如果数据类型在较大的位宽上具有未使用的位，则可能会浪费少量内存。

# 7.3 架构
本节介绍内存架构，包括硬件和软件，包括处理器和操作系统细节。

这些主题已总结为性能分析和调整的背景。有关更多详细信息，请参阅本章末尾列出的供应商处理器手册和有关操作系统内部结构的文本

# #7.3.1 硬件
内存硬件包括主内存、总线、CPU 缓存和 MMU

# 主内存
目前使用的常见主存储器类型是动态随机存取存储器 （DRAM）。这是一种易失性存储器 — 当断电时，其内容会丢失。DRAM 提供高密度存储，因为每个位仅使用两个逻辑组件实现：电容器和晶体管。电容器需要定期刷新以保持充电

企业服务器根据其用途配置了不同数量的 DRAM，通常从 1 GB 到 1 TB 甚至更大。云计算实例通常较小，每个实例的大小在 512 MB 到 256 GB 之间。3 但是，云计算旨在将负载分散到实例池中，因此它们可以共同为分布式应用程序提供更多的 DRAM 在线，尽管一致性成本要高得多。

# 延迟
主存储器的访问时间可以用列地址选通 （CAS） 延迟来衡量：从向存储器模块发送所需地址（列）到数据可供读取之间的时间。这取决于内存类型（对于 DDR4，大约为 10 到 20ns [Crucial 18]）。对于内存 I/O 传输，内存总线（例如，64 位宽）传输高速缓存行（例如，64 字节宽）可能会多次出现此延迟。此外，CPU 和 MMU 还涉及其他延迟，用于读取新的可用数据。读取指令在从 CPU 缓存返回时可避免这些延迟;如果处理器支持回写缓存（例如，Intel 处理器），则写入指令也可以避免它们

# 主内存架构
图 7.3 显示了通用双处理器统一内存访问 （UMA） 系统的主内存架构示例。

每个 CPU 通过共享系统总线对所有内存具有统一的访问延迟。当由在所有处理器上统一运行的单个操作系统内核实例进行管理时，这也是对称多处理 （SMP） 体系结构。

为了进行比较，图 7.4 显示了一个双处理器非一致性内存访问 （NUMA） 系统示例，它使用成为内存架构一部分的 CPU 互连。对于此体系结构，主内存的访问时间根据其相对于 CPU 的位置而变化。

CPU 1 可以通过其内存总线直接对 DRAM A 执行 I/O。这称为本地内存。CPU 1 通过 CPU 2 和 CPU 互连（两个跃点）对 DRAM B 执行 I/O。这称为远程内存，具有较高的访问延迟。

连接到每个 CPU 的内存组称为内存节点，或简称为节点。操作系统可能会根据处理器提供的信息识别内存节点拓扑。然后，这允许它根据内存位置分配内存和调度线程，从而尽可能偏爱本地内存以提高性能。

# 总线
主内存如何物理连接到系统取决于主内存架构，如前图所示。实际实现可能涉及 CPU 和内存之间的其他控制器和总线。主内存可以通过以下方法之一访问：

■ 共享系统总线：单处理器或多处理器，通过共享系统总线、内存桥接控制器，最后是内存总线。这被描绘成 UMA 示例（图 7.3）和 Intel 前端总线示例（第 6 章 CPU 中的图 6.9）。那个例子中的内存控制者是 Northbridge。
■ 直接：通过内存总线直接连接内存的单个处理器。
■ 互连：多处理器，每个处理器都通过内存总线直接连接内存，处理器通过 CPU 互连连接。这在前面的图 7.4 中作为 NUMA 示例进行说明;CPU 互连在第 6 章 “CPU”中讨论。

如果您怀疑您的系统不是上述任何一种，请找到一个系统功能图并遵循 CPU 和内存之间的数据路径，并在此过程中记下所有组件

# DDR SDRASM
对于任何架构，内存总线的速度通常由处理器和系统主板支持的内存接口标准决定。自 1996 年以来使用的通用标准是双倍数据速率同步动态随机存取存储器 （DDR SDRAM）。术语 double data rate 是指在 clock 信号上升和下降时传输数据（也称为 double-pumped）。术语 synchronous 是指与 CPU 同步计时的内存

DDR SDRAM 标准示例如表 7.1 所示。

DDR5 标准预计将于 2020 年由 JEDEC 固态技术协会发布。这些标准也使用“PC-”后跟以兆字节/秒为单位的数据传输速率命名，例如 PC-1600

# 多通道
系统架构可能支持并行使用多个内存总线，以提高带宽。常见的倍数是双通道、三通道和四通道。例如，Intel Core i7 处理器支持多达四通道 DDR3-1600，最大内存带宽为 51.2 GB/s。

# CPU 缓存
处理器通常包括片上硬件缓存，以提高内存访问性能。缓存可能包括以下级别，即降低速度和增加大小：

■ 1 级：通常分为单独的指令缓存和数据缓存 
■ 2 级：指令和数据缓存 
■ 3 级：另一个更大的缓存级别

根据处理器的不同，级别 1 通常由虚拟内存地址引用，级别 2 通常由物理内存地址引用。

这些缓存在 第 6 章 CPU 中有更深入的讨论。本章将讨论另一种类型的硬件缓存，即 TLB。

# MMU
MMU（内存管理单元）负责虚拟到物理的地址转换。这些作是按页执行的，并且页中的偏移量是直接映射的。MMU 在第 6 章 CPU 中介绍，在附近的 CPU 缓存的上下文中。

图 7.5 中显示了一个通用的 MMU，具有 CPU 缓存和主内存的级别

# 多种页面大小
现代处理器支持多种页面大小，这允许操作系统和 MMU 使用不同的页面大小（例如，4 KB、2 MB、1 GB）。Linux 巨页功能支持更大的页面大小，例如 2 MB 或 1 GB。

# TLB
图 7.5 中所示的 MMU 使用 TLB （translation lookaside buffer） 作为地址转换缓存的第一级，然后是主内存中的页表。TLB 可以分为单独的高速缓存，用于指令页和数据页。

由于 TLB 的映射条目数有限，因此使用较大的页面大小会增加可从其高速缓存转换的内存范围（其范围），从而减少 TLB 未命中并提高系统性能。对于每个页面大小，TLB 可以进一步划分为单独的高速缓存，从而提高在高速缓存中保留较大映射的可能性。

作为 TLB 大小的一个示例，典型的 Intel Core i7 处理器提供表 7.2 [Intel 19a] 中所示的四个 TLB。

此处理器具有一个级别的数据 TLB。Intel Core 微架构支持两个级别，类似于 CPU 提供多级主内存缓存的方式。

TLB 的确切构成特定于处理器类型。请参阅供应商处理器手册，了解有关处理器中 TLB 的详细信息以及有关其作的更多信息。

# #7.3.2 软件
内存管理软件包括虚拟内存系统、地址转换、交换、分页和分配。本节包括与性能最相关的主题：释放内存、空闲列表、页面扫描、交换、进程地址空间和内存分配器。

# 释放内存
当系统上的可用内存变低时，用户可以使用各种方法来释放内存，将其添加到空闲页面列表中。这些方法如图 7.6 所示（对于 Linux），它们通常按照可用内存减少的使用顺序排列。

这些方法是：
■ Free list：未使用（也称为空闲内存）且可立即分配的页面列表。这通常实现为多个免费页面列表，每个区域组 （NUMA） 一个。
■ Page cache：文件系统缓存。名为 swappiness 的可调参数设置系统应倾向于从页面高速缓存中释放内存而不是交换的程度
■ 交换：这是由分页守护程序 kswapd 进行的分页，它查找最近未使用的页面以添加到空闲列表中，包括应用程序内存。这些文件被分页出来，这可能涉及写入基于文件系统的交换文件或交换设备。当然，仅当配置了交换文件或设备时，此选项才可用。
■ 收缩：当超过内存不足阈值时，可以指示内核模块和内核 slab 分配器立即释放任何可以轻松释放的内存。这也称为收缩。
■ OOM killer：内存不足杀手将通过查找并杀死一个牺牲进程来释放内存，该进程使用 select_bad_process（） 找到，然后通过调用 oom_kill_process（） 杀死。这可能会在系统日志 （/var/log/messages） 中记录为 “Out of memory： Kill pro cess” 消息。

Linux swappiness 参数控制是通过分页应用程序还是从页面高速缓存中回收内存来释放内存。它是一个介于 0 和 100 之间的数字（默认值为 60），其中较高的值有利于通过分页来释放内存。控制这些内存释放技术之间的平衡，可以通过保留暖文件系统缓存，同时分页冷应用程序内存来提高系统吞吐量 [Corbet 04]。

询问如果未配置交换设备或交换文件会发生什么情况，也很有趣。这会限制虚拟内存大小，因此，如果已禁用 overcommit，则内存分配将很快失败。在 Linux 上，这也可能意味着 OOM 杀手的使用速度更快

考虑一个存在内存无限增长问题的应用程序。使用 swap 时，由于分页，这可能会首先成为性能问题，这是实时调试问题的机会。如果没有交换，则没有分页宽限期，因此应用程序会遇到“内存不足”错误，或者 OOM 终止程序会终止它。如果仅在使用数小时后才看到问题，这可能会延迟调试问题。

在 Netflix 云中，实例通常不使用交换，因此如果应用程序耗尽内存，它们将被 OOM 终止。应用程序分布在一个大型实例池中，并且具有一个OOM killed 会导致流量立即重定向到其他运行状况良好的实例。这比允许一个实例由于交换而运行缓慢更可取。

当使用内存 cgroups 时，可以使用类似于图 7.6 所示的内存释放技术来管理 cgroup 内存。系统可能具有大量可用内存，但正在交换或遇到 OOM 杀手，因为容器已用尽其 cgroup 控制的限制 [Evans 17]。有关 cgroups 和容器的更多信息，请参阅第 11 章 云计算。

以下各节介绍空闲列表、回收和 page-out 守护程序。

# 空闲列表
最初的 Unix 内存分配器使用内存映射和首次拟合扫描。随着 BSD 中分页虚拟内存的引入，添加了一个空闲列表和一个分页守护进程 [Babaoglu 79]。如图 7.7 所示的空闲列表允许立即找到可用内存。

Memory freed 将添加到列表的头部，以供将来分配。由 page-out 守护程序释放的内存（可能仍包含有用的高速缓存文件系统页）将添加到尾部。如果在有用页面被重用之前发生对其中一个页面的未来请求，则可以将其回收并从空闲列表中删除。

基于 Linux 的系统仍在使用一种形式的空闲列表，如图 7.6 所示。空闲列表通常通过分配器使用，例如内核的 slab 分配器和用户空间的 libc malloc（）（它有自己的空闲列表）。这些 API 反过来使用页面，然后通过其分配器 API 公开它们。

Linux 使用 buddy 分配器来管理页面。这为不同大小的内存分配提供了多个空闲列表，遵循 2 的幂方案。术语 buddy 是指查找空闲内存的相邻页面，以便可以将它们一起分配。有关历史背景，请参见 [Peterson 77]。

伙伴空闲列表位于以下层次结构的底部，从每个内存节点开始 pg_data_t：
■ Nodes：内存组、NUMA 
■ zones：用于特定目的的内存范围（直接内存访问 [DMA]，4 正常、高内存） 
■ Migration：不可移动、可回收、可移动等。
■ Sizess：2 的幂次方页数

在节点空闲列表中进行分配可以提高内存局部性和性能。对于最常见的分配，即单页，好友分配器为每个 CPU 保留单页列表，以减少 CPU 锁争用

# 收缩
Reaping 主要涉及从 kernel slab 分配器缓存中释放内存。这些缓存以 slab 大小的块形式包含未使用的内存，可供重用。Reaping 将此内存返回给系统以进行页面分配。

在 Linux 上，内核模块还可以调用 register_shrinker（） 来注册特定函数来获取自己的内存。

# 页面扫描
通过分页释放内存由内核 page-out 守护程序管理。当可用列表中的可用主内存低于阈值时，page-out 守护程序将开始页面扫描。仅在需要时进行页面扫描。正常平衡的系统可能不会经常进行页面扫描，而可能只会在短时间内进行。

在 Linux 上，page-out 守护程序称为 kswapd，它扫描非活动内存和活动内存的 LRU 页面列表以释放页面。它根据空闲内存和两个阈值进行唤醒，以提供滞后，如图 7.8 所示

一旦空闲内存达到最低阈值，kswapd 就会在前台运行，在请求时同步释放内存页，这种方法有时称为直接回收 [Gorman 04]。这个最低阈值是可调的 （vm.min_free_kbytes），其他阈值根据它进行缩放（低阈值缩放 2 倍，高阈值缩放 3 倍）。对于具有高分配突增速度且速度超过 kswap 回收的工作负载，Linux 提供了额外的可调参数，以实现更积极的扫描、vm.watermark_scale_factor和vm.watermark_boost_factor：请参见部分 7.6.1， 可调参数

页面缓存具有非活动页面和活动页面的单独列表。它们以 LRU 方式运行，允许 kswapd 快速找到空闲页面。它们如图 7.9 所示。

kswapd 首先扫描非活动列表，然后根据需要扫描活动列表。术语 扫描 是指在遍历列表时检查页面：如果页面被锁定/脏污，则可能没有资格被释放。kswapd 使用的术语 scanning 与原始 UNIX page-out 守护程序执行的扫描（扫描所有内存）的含义不同。

# #7.3.3 处理虚拟地址空间
进程虚拟地址空间由硬件和软件共同管理，是根据需要映射到物理页的一系列虚拟页。这些地址被划分为多个区域，称为 segments，用于存储线程堆栈、进程可执行文件、库和堆。图 7.10 显示了 Linux 上 x86 和 SPARC 处理器的 32 位进程示例。

在 SPARC 上，内核驻留在单独的完整地址空间中（图 7.10 中未显示）。请注意，在 SPARC 上，不能仅根据指针值来区分用户地址和内核地址;x86 采用不同的方案，其中用户地址和内核地址不重叠。

程序可执行段包含单独的文本段和数据段。库还由单独的可执行文本和数据段组成。这些不同的细分类型是：

■ 可执行文本：包含进程的可执行 CPU 指令。这是从文件系统上二进制程序的文本段映射的。它是只读的，具有执行权限。
■ 可执行数据：包含从二进制程序的数据段映射的初始化变量。它具有读/写权限，因此可以在程序运行时修改变量。它还具有 private 标志，因此修改不会刷新到磁盘。
■ 堆：这是程序的工作内存，是匿名内存（无文件系统位置）。它根据需要增长，并通过 malloc（3） 进行分配。
■ 堆栈：正在运行的线程的堆栈，映射读/写。

库文本段可以由使用同一库的其他进程共享，每个进程都有库数据段的私有副本

# 堆增长
混淆的一个常见来源是堆的无休止增长。是内存泄漏吗？对于简单分配器，free（3） 不会将内存返回给 os 系统;相反，保留内存以用于将来的分配。这意味着进程驻留内存只能增长，这是正常的。减少系统内存使用的进程的方法包括

■ Re-exec：调用 execve（2） 从空地址空间开始 
■ 内存映射：使用 mmap（2） 和 munmap（2），它们会将内存返回给系统

内存映射文件在第 8 章 “文件系统”的 Section 8.3.10， 内存映射文件中介绍。

Glibc 通常用于 Linux，是一种高级分配器，支持 mmap作模式，以及用于向系统释放空闲内存的 malloc_trim（3） 函数。当堆顶部的空闲内存变大时，free（3） 会自动调用 malloc_trim（3），6 并使用 sbrk（2） 系统调用来释放它。

# 分配器
有各种用户级和内核级分配器用于内存分配。图 7.11 显示了分配器的作用，包括一些常见的类型

页面管理已在前面的部分 7.3.2 软件的 Free List（s） 下介绍。

内存分配器功能可以包括：

■ 简单的 API：例如，malloc（3）、free（3）。
■ 高效的内存使用：在处理各种大小的内存分配时，内存使用可能会变得碎片化，其中有许多未使用的区域会浪费内存。分配器可以努力合并未使用的区域，以便更大的分配可以利用它们，从而提高效率。
■ 性能：内存分配可能很频繁，在多线程环境中，由于对同步基元的争用，它们的性能可能会很差。分配器可以设计为谨慎使用锁，还可以利用每线程或每 CPU 缓存来提高内存局部性。
■ 可观察性：分配器可以提供统计信息和调试模式，以显示其使用情况以及哪些代码路径负责分配。

以下各节介绍内核级分配器 — slab 和 SLUB — 以及用户级分配器 — glibc、TCMalloc 和 jemalloc

# Slab
kernel slab 分配器管理特定大小的对象的缓存，允许它们快速回收，而不会产生页面分配的开销。这对于内核分配特别有效，因为内核分配通常用于固定大小的结构。

作为内核示例，以下两行来自 ZFS arc.c:

# -------------------------------------------------------------
df = kmem_alloc(sizeof (l2arc_data_free_t), KM_SLEEP);          
head = kmem_cache_alloc(hdr_cache, KM_PUSHPAGE);   
# -------------------------------------------------------------

第一个 kmem_alloc（） 显示了一个传统样式的内核分配，其大小作为参数传递。内核根据该大小将其映射到 slab 缓存（非常大的大小由超大 arena 以不同的方式处理）。第二个 kmem_cache_alloc（） 直接在自定义 slab 分配器缓存上运行，在本例中为 （kmem_cache_t *）hdr_cache。

为 Solaris 2.4 [Bonwick 94] 开发的 slab 分配器后来通过称为 magazines [Bonwick 01] 的每 CPU 缓存进行了增强：

我们的基本方法是为每个 CPU 提供一个称为弹匣的 M 元素对象缓存，类似于自动武器。在 CPU 需要重新加载之前，每个 CPU 的弹匣都可以满足 M 分配，也就是说，将其空弹匣换成满弹匣。

除了高性能之外，最初的 slab 分配器还具有调试和分析功能，包括审计以跟踪分配细节和堆栈跟踪

Slab 分配已被各种操作系统采用。BSD 有一个称为通用内存分配器 （UMA） 的内核 slab 分配器，它非常高效且支持 NUMA。在 Linux 2.2 版中还引入了 slab alloca tor，多年来它一直是默认选项。此后，Linux 已将 SLUB 作为选项或默认选项。

# SLUB
Linux 内核 SLUB 分配器基于 slab 分配器，旨在解决各种问题，尤其是关于 slab 分配器的复杂性。改进包括删除对象队列和每 CPU 缓存，将 NUMA 优化留给页面分配器（请参阅前面的 Free List（s） 部分）。

SLUB 分配器在 Linux 2.6.23 中成为默认选项 [Lameter 07]

# glibc
用户级 GNU libc 分配器基于 Doug Lea 的 dlmalloc。其行为取决于分配请求大小。小的分配从内存箱中提供，其中包含相似大小的单元，可以使用类似伙伴的算法进行合并。较大的分配可以使用树查找来有效地查找空间。非常大的 allocation 会切换到使用 mmap（2）。最终结果是一个高性能的分配器，它受益于多个分配策略。

# TCMalloc
TCMalloc 是用户级线程缓存 malloc，它使用每个线程的缓存来处理小的 allo cations，从而减少锁争用并提高性能 [Ghemawat 07]。定期垃圾回收将内存迁移回中央堆进行分配。

# jemalloc
libjemalloc 最初是 FreeBSD 用户级 libc 分配器，也可用于 Linux。它使用多个 arenas、每线程缓存和小对象板等技术来提高可扩展性并减少内存碎片。它可以同时使用 mmap（2） 和 sbrk（2） 来获取系统内存，首选 mmap（2）。Facebook 使用 jemalloc 并添加了分析和其他优化 [Facebook 11]

# 7.4 方法论
本节介绍用于内存分析和调整的各种方法和练习。表 7.3 总结了这些主题。

参见第 2 章 方法论，了解更多策略和其中许多策略的介绍。

这些方法可以单独遵循或组合使用。在排查内存问题时，我的建议是按以下顺序从以下策略开始：性能监控、USE 方法和描述使用情况。

第 7.5 节 可观测性工具介绍了用于应用这些方法的操作系统工具。

# #7.4.1 工具方法
工具方法是迭代可用工具的过程，检查它们提供的关键指标。这是一种简单的方法，它可能会忽略您碰巧可用的工具提供较差或没有可见性的问题，并且执行起来可能很耗时。

对于内存，tools 方法可能涉及检查 Linux 的以下内容：

■ 页面扫描：查找持续页面扫描（超过 10 秒）作为内存压力的迹象。这可以使用 sar -B 并检查 pgscan 列来完成。
■ 压力停顿信息 （PSI）：cat /proc/pressure/memory （Linux 4.20），用于检查内存压力（饱和度）统计数据及其随时间的变化情况。
■ 交换：如果配置了交换，则内存页的交换（Linux 对交换的定义）进一步表明系统内存不足。您可以使用 vmstat（8） 并检查 si 等列。
■ vmstat：运行 vmstat 1 并检查可用内存的 free 列。
■ OOM killer：这些事件可以在系统日志 /var/log/messages 或 dmesg（1） 中看到。搜索 “Out of memory”。
■ top：查看哪些进程和用户是排名靠前的物理内存使用者（驻留）和虚拟内存使用者（请参阅手册页中的列名称，这些名称因版本而异）。top（1） 还总结了空闲内存。
■ perf（1）/BCC/bpftrace：使用堆栈跟踪跟踪内存分配，以确定内存使用的原因。请注意，这可能会花费相当大的开销。一种更便宜但较粗糙的解决方案是执行 CPU 分析（定时堆栈采样）并搜索分配代码路径。

有关每个工具的更多信息，请参见部分 7.5， 可观测性工具

# #7.4.2 USE方法
USE 方法用于在执行结果调查的早期识别所有组件的瓶颈和错误，然后再遵循更深入、更耗时的策略。

检查整个系统：

■ 利用率：正在使用的内存量和可用内存量。应检查物理内存和虚拟内存。
■ 饱和度：执行页面扫描、分页、交换和 Linux OOM 杀手牺牲的程度，作为缓解内存压力的措施。
■ 错误：软件或硬件错误

您可能需要先检查饱和度，因为持续饱和是内存问题的迹象。这些指标通常可以从操作系统工具中轻松获得，包括用于交换统计数据的 vmstat（8） 和 sar（1），以及用于 OOM 杀手牺牲的 dmesg（1）。对于配置了单独磁盘交换设备的系统，对交换设备的任何活动都是内存压力的另一个迹象。Linux 还提供内存饱和度统计数据作为压力失速信息 （PSI） 的一部分。

不同工具可以以不同的方式报告物理内存利用率，具体取决于它们是考虑未引用的文件系统缓存页还是非活动页。系统可能会报告它只有 10 MB 的可用内存，而实际上它有 10 GB 的文件系统缓存，应用程序可以在需要时立即回收这些缓存。查看工具文档，了解包含的内容。

可能还需要检查虚拟内存利用率，具体取决于每个表单的系统是否过量使用。对于不这样做的系统，一旦虚拟内存耗尽，内存分配将失败 - 一种内存错误。

内存错误可能是由软件（如内存分配失败或 Linux OOM 终止程序）或硬件（如 ECC 错误）引起的。从历史上看，内存分配错误一直留给应用程序报告，尽管并非所有应用程序都这样做（而且，由于 Linux 过度使用，开发人员可能认为没有必要）。硬件错误也很难诊断。当使用 ECC 内存时，一些工具可以报告 ECC 可纠正的错误（例如，在 Linux 上，dmidecode（8）、edac-utils、ipmitool sel）。这些可纠正的错误可以用作 USE 方法误差指标，并且可能表明不可纠正的错误可能很快就会发生。与实际 （不可纠正）内存错误，您可能会遇到任意应用程序无法解释的、不可重现的崩溃（包括段错误和总线错误信号）。

对于实施内存限制或配额（资源控制）的环境，如在某些云计算环境中，可能需要以不同的方式测量内存利用率和饱和度。您的 OS 实例可能处于其软件内存限制和交换状态，即使主机上有大量可用物理内存。请参见第 11 章 “云计算”。

# #7.4.3 描述使用情况
在容量规划、基准测试和模拟工作负载时，描述内存使用情况是一项重要的练习。它还可以通过查找和纠正错误配置来获得一些最大的性能提升。例如，数据库缓存可能配置得太小且命中率较低，或者配置得太大并导致系统分页

对于内存，描述使用情况包括确定内存的使用位置和数量：

■ 系统范围的物理和虚拟内存利用率 
■ 饱和程度：交换和 OOM 终止 
■ 内核和文件系统缓存内存使用情况 
■ 每个进程的物理和虚拟内存使用情况 
■ 内存资源控制的使用情况（如果存在）

此示例说明显示了如何一起表示这些属性

该系统具有 256 GB 的主内存，其中 1% 由进程使用（利用），30% 在文件系统缓存中。最大的进程是数据库，消耗 2 GB 的主内存 （RSS），这是从中迁移的上一个系统的配置限制。

这些特性可能会随着时间的推移而变化，因为更多的内存用于缓存工作数据。除了常规缓存增长之外，内核或应用程序内存也可能由于内存泄漏（一种软件错误）而随着时间的推移而持续增长

# 高级使用情况分析/清单

这里列出了其他特征作为需要考虑的问题，在彻底研究记忆问题时，也可以作为检查表：

■ 应用程序的工作集大小 （WSS） 是多少？
■ 内核内存用在哪儿？每块板？
■ 有多少文件系统缓存是活动的，而不是非活动的？
■ 进程内存用于何处（指令、缓存、缓冲区、对象等）？
■ 为什么进程要分配内存（调用路径）？
■ 为什么内核要分配内存（调用路径）？
■ 进程库映射有什么奇怪的地方（例如，随时间变化）？
■ 哪些进程正在积极换出？
■ 以前换掉了哪些进程？
■ 进程或内核是否存在内存泄漏？
■ 在 NUMA 系统中，内存在内存节点之间的分布情况如何？
■ IPC 和内存停顿周期率是多少？
■ 内存总线的平衡程度如何？
■ 与远程内存 I/O 相比，执行了多少本地内存 I/O？

以下部分可以帮助回答其中的一些问题。有关此方法和要测量的特征（谁、为什么、什么、如何）的更高级别摘要，请参见第 2 章 “方法”。

# #7.4.4 周期分析
内存总线负载可以通过检查 CPU 性能监控计数器 （PMC） 来确定，该计数器可以编程以计算内存停顿周期、内存总线使用情况等。首先要考虑的指标是每个周期的指令数 （IPC），它反映了 CPU 负载对内存的依赖程度。请参见第 6 章 “CPU”

# #7.4.5 性能监控
性能监控可以识别一段时间内的活动问题和行为模式。内存的关键指标是

■ 利用率：使用百分比，可从可用内存推断 
■ 饱和度：交换、OOM 终止

对于实施内存限制或配额（资源控制）的环境，可能还需要收集与施加的限制相关的统计信息

还可以监控错误（如果可用），如部分 7.4.2， USE 方法中的利用率和饱和度所述

监控一段时间内的内存使用情况（尤其是按进程监控）有助于识别内存泄漏的存在和速率。

# #7.4.6 泄漏检测
当应用程序或内核模块无休止地增长，消耗来自空闲列表、文件系统缓存并最终来自其他进程的内存时，会出现此问题。这可能首先被注意到，因为系统开始交换或应用程序被 OOM 终止，以响应无尽的内存压力。

此类问题是由以下任一原因引起的

■ 内存泄漏：一种软件错误，其中内存不再使用但从未释放。此问题可通过修改软件代码或通过应用补丁或升级（修改代码）来修复。
■ 内存增长：软件正常消耗内存，但消耗速度远高于系统所需的速度。此问题可以通过更改软件配置或软件开发人员更改应用程序消耗内存的方式来解决。

内存增长问题经常被误认为是内存泄漏。首先要问的问题是：它应该这样做吗？检查内存使用情况、应用程序的配置及其分配器的行为。应用程序可能配置为填充内存缓存，观察到的增长可能是缓存预热。

如何分析内存泄漏取决于软件和语言类型。一些分配器提供用于记录分配详细信息的调试模式，然后可以在事后分析这些详细信息，以确定负责的调用路径。某些运行时具有执行堆转储分析的方法，以及用于执行内存泄漏调查的其他工具。

Linux BCC 跟踪工具包括用于增长和泄漏分析的 memleak（8）：它跟踪分配并记录在间隔内未释放的分配，以及分配代码路径。它无法判断这些是泄漏还是正常增长，因此您的任务是分析代码路径以确定是哪种情况。（请注意，此工具在分配率较高的情况下也会产生高开销。BCC 在 BPF 第 15 章 BCC 第 15.1 节中介绍。

# #7.4.7 静态性能调优
静态性能优化侧重于已配置环境的问题。对于内存性能，请检查 static 配置的以下方面：

■ 总共有多少主内存？
■ 应用程序配置为使用多少内存（它们自己的配置）
■ 应用程序使用哪些内存分配器？
■ 主内存的速度是多少？它是最快的类型 （DDR5） 吗？
■ 主内存是否经过全面测试（例如，使用 Linux memtester）
■ 操系统架构是什么？NUMA，UMA？
■ 操作系统是否能识别 NUMA？它是否提供 NUMA 可调参数？
■ 内存是附加到同一个插槽，还是在多个插槽之间分配？
■ 存在多少内存总线
■ CPU 缓存的数量和大小是多少？TLB？
■ BIOS 设置是什么？
■ 是否配置并使用了大型页面？
■ 超额使用可用且已配置
■ 还有哪些其他系统内存可调参数正在使用中？
■ 是否有软件施加的内存限制（资源控制）？

回答这些问题可能会揭示被忽略的配置选择。

# #7.4.8 资源控制
操作系统可以为将内存分配给进程或进程组提供精细的控制。这些控制可能包括主内存和虚拟内存使用的固定限制。它们的工作原理是特定于实现的，将在第 7.6 节 调优和第 11 章 云计算中讨论。

# #7.4.9 微基准测试
微基准测试可用于确定主内存的速度以及 CPU 缓存和缓存行大小等特性。在分析系统之间的差异时，它可能很有帮助，因为内存访问速度对性能的影响可能比 CPU 时钟速度更大，具体取决于应用程序和工作负载

在第 6 章 CPU 中，CPU 缓存下的延迟部分（在部分 6.4.1 硬件中）显示了对内存访问延迟进行微基准测试的结果，以确定 CPU 缓存的特性。

# #7.4.10 内存缩减
这是一种工作集大小 （WSS） 估计方法，它使用负实验，要求配置交换设备来执行实验。在测量性能和交换时，应用程序的可用主内存会逐渐减少：当 WSS 不再适合可用内存时，将显示性能急剧下降和交换大幅增加的点。

虽然值得一提的是作为负面实验的示例，但不建议将其用于生产环境，因为它会故意损害性能。有关其他 WSS 估计技术，请参阅第 7.5.12 节 wss 中的实验性 wss（8） 工具，以及我关于 WSS 估计的网站 [Gregg 18c]。

# 7.5 观测工具
本节介绍基于 Linux 的操作系统的内存可观测性工具。有关使用它们时要遵循的方法，请参阅上一节。

本节中的工具如表 7.4 所示

这是支持部分 7.4 方法的一系列工具和功能。我们从用于系统范围内存使用情况统计的工具开始，然后深入到每个进程和分配跟踪。一些传统工具可能在它们起源的其他类 Unix操作系统上可用，包括：vmstat（8）、sar（1）、ps（1）、top（1） 和 pmap（1）。drsnoop（8） 是 BCC 的一个 BPF 工具（第 15 章）。

有关其功能的完整参考，请参阅每个工具的文档，包括其手册页。

# #7.5.1 vmstat
虚拟内存统计信息命令 vmstat（8） 提供了系统内存运行状况的高级视图，包括当前可用内存和分页统计信息。此外，还包括 CPU 统计信息，如第 6 章 “CPU”中所述。

当 Bill Joy 和 Ozalp Babaoglu 于 1979 年为 BSD 引入它时，原始手册页包括：

BUGS：打印出的数字太多了，有时很难弄清楚要看什么

以下是 Linux 版本的示例输出：

# $ vmstat 1
 procs -----------memory---------- ---swap-- -----io---- -system--
 ----cpu---
 r  b   
swpd   free   buff  cache   si   so
    bi    bo   in   cs us sy id wa
 4  0      0 34454064 111516 13438596   0   0    0    5    2    0  0  0 100  0
 4  0      0 34455208 111516 13438596   0   0    0    0 2262 15303 16 12 73  0
 5  0      0 34455588 111516 13438596   0   0    0    0 1961 15221 15 11 74  0
 4  0      0 34456300 111516 13438596   0   0    0    0 2343 15294 15 11 73  0
 [...]

这个版本的 vmstat（8） 不会在输出的第一行打印 procs 或 memory 列的 summary-since-boot 值，而是立即显示当前状态。默认情况下，这些列以 KB 为单位，它们是：

■ swpd： 可用换出内存量
■ free： 可用内存
■ buff： 缓冲区缓存中的内存
■ cache：页面缓存中的内存 
■ si：   换入的内存（分页）
■ so：   换出的内存（分页）

缓冲区和页高速缓存在第 8 章 “文件系统”中介绍。系统中的空闲内存在引导后丢弃并被这些缓存用于提高性能是正常的。可以在需要时发布它供应用程序使用。

如果 si 和 so 列一直不为零，则系统面临内存压力，并正在切换到交换设备或文件（参见 swapon（8））。其他工具，包括那些按进程显示内存的工具（例如 top（1）、ps（1）），可用于调查是什么在消耗内存。

在具有大量内存的系统上，列可能会变得未对齐并且有点难以读取。您可以尝试使用 -S 选项将输出单位更改为兆字节（使用 m 表示 1000000，使用 M 表示 1048576）：

# $ vmstat -Sm 1
 procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu---
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa
 4  0      0  35280    114  13761    0    0     0     5    2    1  0  0 100  0
 4  0      0  35281    114  13761    0    0     0     0 2027 15146 16 13 70  0
 [...]

还有一个 -a 选项可以从页面缓存中打印非活动内存和活动内存的细分：

# $ vmstat -a 1
 procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu---
 r  b   swpd   free  inact active
   si   so    bi    bo   in   cs us sy id wa
 5  0      0 34453536 10358040 3201540   0   0    0    5    2    0  0  0 100  0
 4  0      0 34453228 10358040 3200648   0   0    0    0 2464 15261 16 12 71  0
 [...]

这些内存统计信息可以使用小写的 -s 选项打印为列表。

# #7.5.2 PSI
Linux 4.20 中添加的 Linux 压力失速信息 （PSI） 包括内存饱和度的统计信息。这些指标不仅显示是否存在内存压力，还显示内存压力在过去 5 分钟内的变化情况。输出示例：

 # cat /proc/pressure/memory
 some avg10=2.84 avg60=1.23 avg300=0.32 total=1468344
 full avg10=1.85 avg60=0.66 avg300=0.16 total=702578

此输出显示内存压力正在增加，10 秒平均值 （2.84） 高于 300 秒平均值 （0.32）。这些平均值是任务内存停滞的时间百分比。some 行显示某些任务（线程）何时受到影响，而 full 行显示所有可运行任务何时受到影响。

PSI 统计数据也按 cgroup2 进行跟踪（cgroups 在第 11 章 云计算中介绍）[Facebook 19]。

# #7.5.3 swapon
swapon（1） 可以显示是否已配置 swap 设备以及其卷的使用情况。例如：

# $ swapon
 NAME      TYPE      SIZE   USED PRIO
 /dev/dm-2 partition 980M 611.6M   -2
 /swap1    file       30G  10.9M   -3

此输出显示两个交换设备：一个 980 MB 的物理磁盘分区，以及一个名为 30 GB 的 /swap1 文件。输出还显示了两者的使用情况。现在许多系统都没有配置交换;在这种情况下，swapon（1） 不会打印任何输出。

如果交换设备具有活动的 I/O，则可以在 vmstat（1） 的 si 和 so 列中看到，也可以在 iostat（1） 中看到设备 I/O（第 9 章）。

# #7.5.4 sar
系统活动报告器 sar（1） 可用于观察当前活动，并且可以配置为存档和报告历史统计信息。本书的各个章节都提到了它提供的不同统计数据，并在第 4 章 可观测性工具， 第 4.4 节 sar 中介绍。

Linux 版本通过以下选项提供内存统计信息：

■ -B：分页统计信息 
■ -H：大页统计信息 
■ -r：内存利用率
■ -S：交换空间统计信息 
■ -W：交换统计信息

这些作包括内存使用情况、page-out 守护程序的活动和大页面使用情况。有关这些主题的背景信息，请参见部分 7.3 体系结构。

提供的统计数据包括表 7.5 中的统计数据

许多统计信息名称包括测量的单位：pg 表示页面，kb 表示千字节，% 表示百分比，/s 表示每秒。有关完整列表，请参阅手册页，其中包括一些其他基于百分比的统计信息。

重要的是要记住，在需要时，可以获得有关高级内存子系统的使用和作的大量详细信息。要更深入地了解这些内容，您可能需要使用跟踪器来检测内存跟踪点和内核函数，例如以下部分中的 perf（1） 和 bpftrace。您还可以浏览 mm 中的源代码，特别是 mm/vmscan.c。linux-mm 邮件列表中有许多帖子提供了进一步的见解，因为开发人员正在讨论统计数据应该是什么

%vmeff 量度是衡量页面回收效率的有用指标。高 表示页面成功，完全从非活动列表中窃取 （正常）;低 表示系统正在运行。手册页将接近 100% 描述为高，将低于 30% 描述为低。

另一个有用的指标是 pgscand，它有效地显示了应用程序阻止内存分配和进入直接回收的速率（越高越糟糕）。要查看应用程序在直接回收事件期间花费的时间，可以使用跟踪工具：请参见第 7.5.11 节 drsnoop。

# #7.5.5 slabtop
Linux 中的 slabtop（1） 命令从 slab 分配器中打印内核 slab 缓存使用情况。与 top（1） 一样，它会实时刷新屏幕。

以下是一些示例输出：

 # slabtop -sc
 Active / Total Objects (% used)    : 686110 / 867574 (79.1%)
 Active / Total Slabs (% used)      : 30948 / 30948 (100.0%)
 Active / Total Caches (% used)     : 99 / 164 (60.4%)
 Active / Total Size (% used)       : 157680.28K / 200462.06K (78.7%)
 Minimum / Average / Maximum Object : 0.01K / 0.23K / 12.00K
 OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME                   
 45450  33712  74%    1.05K   3030       15     48480K ext4_inode_cache
 161091  81681  50%    0.19K   7671       21     30684K dentry
 222963 196779  88%    0.10K   5717       39     22868K buffer_head
 35763  35471  99%    0.58K   2751       13     22008K inode_cache
 26033  13859  53%    0.57K   1860       14     14880K radix_tree_node
 93330  80502  86%    0.13K   3111       30     12444K kernfs_node_cache
  2104   2081  98%    4.00K    263        8      8416K kmalloc-4k
   528    431  81%    7.50K    132        4      4224K task_struct
 [...]

输出在顶部有一个摘要和一个 slab 列表，包括其对象计数 （OBJS）、活动数量 （ACTIVE）、使用百分比 （USE）、对象大小（OBJ SIZE、字节）和缓存总大小（CACHE SIZE，字节）。在此示例中，-sc 选项用于按缓存大小排序，最大的缓存位于顶部：ext4_inode_cache

slab 统计信息来自 /proc/slabinfo，也可以由 vmstat -m 打印。

# #7.5.6 numastat
numastat（8）工具为非一致性内存访问 （NUMA） 系统提供统计信息，通常是那些具有多个 CPU 插槽的系统。以下是双插槽系统的一些示例输出：

 # numastat
                           node0           node1
 numa_hit            210057224016    151287435161
 numa_miss             9377491084       291611562
 numa_foreign           291611562      9377491084
 interleave_hit             36476           36665
 local_node          210056887752    151286964112
 other_node            9377827348       292082611

该系统有两个 NUMA 节点，每个节点用于连接到每个插槽的每个内存条。Linux 尝试在最近的 NUMA 节点上分配内存，而 numastat（8） 显示了这有多成功。主要统计数据包括：

■ numa_hit：预期 NUMA 节点上的内存分配。
■ numa_miss + numa_foreign：内存分配不在首选 NUMA 节点上。（numa_miss 显示了本应位于其他位置的 Local 分配，
■ numa_foreign 显示了本应位于本地的远程分配。
■ other_node：当进程在其他位置运行时，此节点上的内存分配。

示例输出显示 NUMA 分配策略执行良好：与其他统计数据相比，点击数较高。如果命中率要低得多，您可以考虑在 sysctl（8） 中调整 NUMA 可调参数，或使用其他方法来改善内存局部性（例如，对工作负载或系统进行分区，或选择具有较少 NUMA 节点的不同系统）。如果没有办法改进 NUMA，numastat（8） 至少可以帮助解释内存 I/O 性能不佳的原因。

numastat（8） 支持 -n 以 MB 为单位打印统计数据，以及 -m 以 /proc/meminfo 的样式打印输出。根据您的 Linux 发行版，numastat（8） 可能在 numactl 软件包中可用。

# #7.5.5 ps
进程状态命令 ps（1） 列出了所有进程的详细信息，包括内存使用情况统计。它的用法在第 6 章 CPU 中介绍

例如，使用 BSD 样式选项：

# $ ps aux
 USER       PID %CPU %MEM    VSZ   RSS TTY  STAT START   TIME COMMAND
 [...]
 bind      1152  0.0  0.4 348916 39568 ?    Ssl  Mar27  20:17 /usr/sbin/named -u bind
 root      1371  0.0  0.0  39004  2652 ?    Ss   Mar27  11:04 /usr/lib/postfix/master
 root      1386  0.0  0.6 207564 50684 ?    Sl   Mar27   1:57 /usr/sbin/console-kit
daemon --no-daemon
 rabbitmq  1469  0.0  0.0  10708   172 ?    S    Mar27   0:49 /usr/lib/erlang/erts
5.7.4/bin/epmd -daemon
 rabbitmq  1486  0.1  0.0 150208  2884 ?    Ssl  Mar27 453:29 /usr/lib/erlang/erts
5.7.4/bin/beam.smp -W w -K true -A30 ...

此输出包括以下列

■ %MEM：主内存使用量（物理内存，RSS）占系统中总内存使用量的百分比 
■ RSS：驻留集大小（KB） 
■ VSZ：虚拟内存大小（KB）

虽然 RSS 显示主内存使用情况，但它包括共享内存段，例如系统库，这些段可能由数十个进程映射。如果要对 RSS 列求和，您可能会发现它超过了系统中的可用内存，因为此共享内存计数过多。有关共享内存的背景知识，请参见第 7.2.9 节 共享内存，以及后面的 pmap（1） 命令来分析共享内存使用情况。

这些列可以使用 SVR4 样式的 -o 选项进行选择，例如

# ps -eo pid,pmem,vsz,rss,comm
  PID %MEM  VSZ  RSS COMMAND
 [...]
 13419  0.0 5176 1796 /opt/local/sbin/nginx
 13879  0.1 31060 22880 /opt/local/bin/ruby19
 13418  0.0 4984 1456 /opt/local/sbin/nginx
 15101  0.0 4580   32 /opt/riak/lib/os_mon-2.2.6/priv/bin/memsup
 10933  0.0 3124 2212 /usr/sbin/rsyslogd
 [...]

Linux 版本还可以打印主要和次要故障（maj_flt、min_flt）的列

ps（1） 的输出可以对内存列进行后排序，以便快速识别最高的使用者。或者，尝试 top（1），它提供交互式排序。

# #7.5.8 top
top（1） 命令监控正在运行的顶级进程，并包括内存使用情况统计信息。它在第 6 章 CPU 中介绍。例如，在 Linux 上：

# $ top -o %MEM
 top - 00:53:33 up 242 days,  2:38,  7 users,  load average: 1.48, 1.64, 2.10
 Tasks: 261 total,   1 running, 260 sleeping,   0 stopped,   0 zombie
 Cpu(s):  0.0%us,  0.0%sy,  0.0%ni, 99.9%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
 Mem:   8181740k total,  6658640k used,  1523100k free,   404744k buffers
 Swap:  2932728k total,   120508k used,  2812220k free,  2893684k cached
  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM
    TIME+  COMMAND
 29625 scott     20   0 2983m 2.2g 1232 S   45 28.7  81:11.31 node  
 5121 joshw     20   0  222m 193m  804 S    0  2.4 260:13.40 tmux 
 1386 root      20   0  202m  49m 1224 S    0  0.6   1:57.70 console-kit-dae
 6371 stu       20   0 65196  38m  292 S    0  0.5  23:11.13 screen   
 1152 bind      20   0  340m  38m 1700 S    0  0.5  20:17.36 named   
15841 joshw     20   0 67144  23m  908 S    0  0.3 201:37.91 mosh-server 
18496 root      20   0 57384  16m 1972 S    3  0.2   2:59.99 python    
 1258 root      20   0  125m 8684 8264 S    0  0.1   2052:01 l2tpns   
16295 wesolows  20   0 95752 7396  944 S    0  0.1   4:46.07 sshd     
23783 brendan   20   0 22204 5036 1676 S    0  0.1   0:00.15 bash  
[...]

顶部的摘要显示主内存 （Mem） 和虚拟内存 （Swap） 的 total、used 和 free。还显示了缓冲区缓存 （buffers） 和页面缓存 （cached） 的大小。

在此示例中，每个进程的输出已在 %MEM 上使用 -o 设置排序列进行排序。此示例中最大的进程是 node，它使用 2.2 GB 的主内存和近 3 GB 的虚拟内存。

主内存百分比列 （%MEM）、虚拟内存大小 （VIRT） 和驻留集大小 （RES） 与前面描述的 ps（1） 中的等效列具有相同的含义。了解更多有关 top（1） 内存统计信息的详细信息，请参阅 top（1） 手册页中的 “Linux 内存类型” 部分，其中解释了每个可能的内存列显示的内存类型。你也可以在使用 top（1） 时键入 “？” 来查看其内置的交互式命令摘要。

# #7.5.9 pmap
pmap（1） 命令列出了进程的内存映射，显示了它们的大小、权限和映射的对象。这样可以更详细地检查进程内存的使用情况，并量化共享内存

pmap（1） 命令列出了进程的内存映射，显示了它们的大小、权限和映射的对象。这样可以更详细地检查进程内存使用情况，并量化共享内存。

例如，在基于 Linux 的系统上：

 # pmap -x 5187
 5187:   /usr/sbin/mysqld
 Address           Kbytes     RSS   Dirty Mode  Mapping
 000055dadb0dd000   58284   10748       0 r-x-- mysqld
 000055dade9c8000    1316    1316    1316 r---- mysqld
 000055dadeb11000    3592     816     764 rw--- mysqld
 000055dadee93000    1168    1080    1080 rw---   [ anon ]
 000055dae08b5000    5168    4836    4836 rw---   [ anon ]
 00007f018c000000    4704    4696    4696 rw---   [ anon ]
 00007f018c498000   60832       0       0 -----   [ anon ]
 00007f0190000000     132      24      24 rw---   [ anon ]
 [...]
 00007f01f99da000       4       4       0 r---- ld-2.30.so
 00007f01f99db000     136     136       0 r-x-- ld-2.30.so
 00007f01f99fd000      32      32       0 r---- ld-2.30.so
 00007f01f9a05000       4       0       0 rw-s- [aio] (deleted)
 00007f01f9a06000       4       4       4 r---- ld-2.30.so
 00007f01f9a07000       4       4       4 rw--- ld-2.30.so
 00007f01f9a08000       4       4       4 rw---   [ anon ]
 00007ffd2c528000     132      52      52 rw---   [ stack ]
 00007ffd2c5b3000      12       0       0 r----   [ anon ]
 00007ffd2c5b6000       4       4       0 r-x--   [ anon ]
 ffffffffff600000       4       0       0 --x--   [ anon ]---------------- ------- ------- ------- 
total kB         1828228  450388  434200

这显示了 MySQL 数据库服务器的内存映射，包括虚拟内存 （KB）、主内存 （RSS）、私有匿名内存 （Anon） 和权限 （Mode）。对于许多映射，很少有内存是匿名的，并且许多映射是只读的 （r-...），允许与其他进程共享这些页面。对于系统库来说尤其如此。此示例中消耗的大部分内存位于堆中，显示为 [ anon ] 段的第一波（在此输出中被截断）

-x 选项打印扩展字段。还有 -X 表示更多细节，而 -XX 表示内核提供的 “everything thing”。仅显示这些模式的标头：

 # pmap -X $(pgrep mysqld) | head -2
 5187:   /usr/sbin/mysqld
         Address Perm   Offset Device   Inode    Size    Rss    
Pss Referenced 
Anonymous LazyFree ShmemPmdMapped Shared_Hugetlb Private_Hugetlb Swap SwapPss Locked 
THPeligible ProtectionKey Mapping
 [...]
 # pmap -XX  $(pgrep mysqld) | head -2
 5187:   /usr/sbin/mysqld
         Address Perm   Offset Device   Inode    Size KernelPageSize MMUPageSize    
Rss    Pss Shared_Clean Shared_Dirty Private_Clean Private_Dirty Referenced Anonymous 
LazyFree AnonHugePages ShmemPmdMapped Shared_Hugetlb Private_Hugetlb Swap SwapPss 
Locked THPeligible ProtectionKey                 VmFlags Mapping
 [...]

这些额外的字段取决于内核版本。它们包括大页面使用、交换使用和映射的比例集大小 （Pss） 的详细信息（突出显示）。PSS 显示映射具有多少私有内存，加上共享内存除以用户数。这为主内存使用量提供了更实际的值。

# #7.5.10 perf
perf（1） 是官方的 Linux 分析器，一个具有许多功能的多功能工具。第 13 章对 perf（1） 进行了总结。本节介绍其在内存分析中的用法。另请参阅第 6 章，了解内存 PMC 的 perf（1） 分析

# 单行
以下单行代码都很有用，并演示了用于内存分析的不同 perf（1） 功能。

系统范围内堆栈跟踪的样本页面错误（RSS 增长），直到 Ctrl-C：
# perf record -e page-faults -a -g

记录 PID 1843 的所有页面错误和堆栈跟踪，持续 60 秒：
# perf record -e page-faults -c 1 -p 1843 -g -- sleep 60

通过 brk（2） 记录堆增长，直到 Ctrl-C：
# perf record -e syscalls:sys_enter_brk -a -g

在 NUMA 系统上录制页面迁移
# perf record -e migrate:mm_migrate_pages -a

对所有 kmem 事件进行计数，每秒打印一次报告：
# perf stat -e 'kmem:*' -a -I 1000

对所有 vmscan 事件进行计数，每秒打印一份报告：
# perf stat -e 'vmscan:*' -a -I 1000

计算所有内存压缩事件，每秒打印一次报告：
# perf stat -e 'compaction:*' -a -I 1000

使用堆栈跟踪跟踪 kswapd 唤醒事件，直到 Ctrl-C：
# perf record -e vmscan:mm_vmscan_wakeup_kswapd -ag

分析给定命令的内存访问：
# perf mem record command

总结内存配置文件：
# perf mem report

对于记录或采样事件的命令，请使用 perf report 汇总配置文件，或使用 perf 脚本 --header 打印所有事件。

有关更多 perf（1） 单行代码，请参见第 13 章 perf， 第 13.2 节 单行程序，以及第 7.5.13 节 bpftrace，它为许多相同的事件构建可观测性程序。

# 页面错误采样
perf（1） 可以记录页面错误的堆栈跟踪，显示触发此事件的代码路径。由于页面错误是在进程增加其驻留集大小 （RSS） 时发生的，因此分析它们可以解释进程的主内存增长的原因。有关内存使用期间页面错误的作用，请参见图 7.2。

在以下示例中，使用堆栈跟踪 （-g） 在所有 CPU （-a9） 上跟踪 page-fault 软件事件 60 秒，然后打印堆栈：

# perf record -e page-faults -a -g -- sleep 60
 [ perf record: Woken up 4 times to write data ]
 [ perf record: Captured and wrote 1.164 MB perf.data (2584 samples) ]
 # perf script
 [...]
 sleep  4910 [001] 813638.716924:          1 page-faults: 
        ffffffff9303f31e __clear_user+0x1e ([kernel.kallsyms])
        ffffffff9303f37b clear_user+0x2b ([kernel.kallsyms])
         ffffffff92941683 load_elf_binary+0xf33 ([kernel.kallsyms])
        ffffffff928d25cb search_binary_handler+0x8b ([kernel.kallsyms])
        ffffffff928d38ae __do_execve_file.isra.0+0x4fe ([kernel.kallsyms])
        ffffffff928d3e09 __x64_sys_execve+0x39 ([kernel.kallsyms])
        ffffffff926044ca do_syscall_64+0x5a ([kernel.kallsyms])
        ffffffff9320008c entry_SYSCALL_64_after_hwframe+0x44 ([kernel.kallsyms])
            7fb53524401b execve+0xb (/usr/lib/x86_64-linux-gnu/libc-2.30.so)
 [...]
 mysqld  4918 [000] 813641.075298:          1 page-faults: 
            7fc6252d7001 [unknown] (/usr/lib/x86_64-linux-gnu/libc-2.30.so)
            562cacaeb282 pfs_malloc_array+0x42 (/usr/sbin/mysqld)
            562cacafd582 PFS_buffer_scalable_container<PFS_prepared_stmt, 1024, 1024, 
PFS_buffer_default_array<PFS_prepared_stmt>, 
PFS_buffer_default_allocator<PFS_prepared_stmt> >::allocate+0x262 (/usr/sbin/mysqld)
            562cacafd820 create_prepared_stmt+0x50 (/usr/sbin/mysqld)
            562cacadbbef [unknown] (/usr/sbin/mysqld)
            562cab3719ff mysqld_stmt_prepare+0x9f (/usr/sbin/mysqld)
            562cab3479c8 dispatch_command+0x16f8 (/usr/sbin/mysqld)
            562cab348d74 do_command+0x1a4 (/usr/sbin/mysqld)
            562cab464fe0 [unknown] (/usr/sbin/mysqld)
            562cacad873a [unknown] (/usr/sbin/mysqld)
            7fc625ceb669 start_thread+0xd9 (/usr/lib/x86_64-linux-gnu/libpthread
2.30.so)
 [...]

这里只包含两个堆栈。第一个是来自 perf（1） 调用的虚拟 sleep（1） 命令，第二个是 MySQL 服务器。在系统范围内跟踪时，您可能会看到许多来自短期进程的堆栈，这些堆栈在内存中短暂增长，从而在退出之前触发页面错误。您可以使用 -p PID 而不是 -a 来匹配进程。

完整输出为 222,582 行;perf report 将代码路径总结为一个层次结构，但输出仍为 7,592 行。火焰图可用于更有效地可视化整个配置文件

# Page Fault 火焰图
图 7.12 显示了从上一个配置文件生成的页面错误火焰图。

图 7.12 火焰图显示，MySQL 服务器中超过一半的内存增长来自 JOIN：：optimize（） 代码路径（左侧大塔）。将鼠标悬停在 JOIN：：optimize（） 上显示它及其子调用导致了 3,226 个页面错误;对于 4 KB 的页面，这相当于大约 12 MB 的主内存增长。

用于生成此火焰图的命令（包括用于记录页面错误的 perf（1） 命令）包括：

 # perf record -e page-faults -a -g -- sleep 60
 # perf script --header > out.stacks
 $ git clone https://github.com/brendangregg/FlameGraph; cd FlameGraph
 $ ./stackcollapse-perf.pl < ../out.stacks | ./flamegraph.pl --hash \
    --bgcolor=green --count=pages --title="Page Fault Flame Graph" > out.svg
我将背景颜色设置为绿色，以直观地提醒您，这不是典型的 CPU 火焰图（黄色背景），而是内存火焰图（绿色背景）。

# #7.5.11 drsnoop
drsnoop（8）10 是一个 BCC 工具，用于跟踪释放内存的直接回收方法，显示受影响的进程和延迟：回收所花费的时间。它可用于量化内存受限系统对应用程序性能的影响。例如：

 # drsnoop -T
 TIME(s)       COMM           PID     LAT(ms) PAGES
 0.000000000   java           11266      1.72    57
 0.004007000   java           11266      3.21    57
 0.011856000   java           11266      2.02    43
 0.018315000   java           11266      3.09    55
 0.024647000   acpid          1209       6.46    73
 [...]

此输出显示了 Java 的一些直接回收，需要 1 到 7 毫秒。在量化应用程序影响时，可以考虑这些回收的速率及其持续时间（以毫秒为单位 （LAT（ms））。

此工具的工作原理是跟踪 vmscan mm_vmscan_direct_reclaim_begin并mm_vmscan_ direct_reclaim_end跟踪点。预计这些事件是低频事件（通常以突发方式发生），因此开销应该可以忽略不计。

drsnoop（8） 支持 -T 选项来包含时间戳，以及 -p PID 来匹配单个进程

# #7.5.12 wss
wss（8） 是我开发的一个实验工具，用于展示如何使用页表条目 （PTE） 的“访问”位来测量进程工作集大小 （WSS）。这是一项较长研究的一部分，旨在总结确定工作集大小的不同方式 [Gregg 18c]。我在这里包含 wss（8），因为工作集大小（经常访问的内存量）是了解内存使用情况的重要指标，拥有带有警告的实验工具总比没有工具好。

以下输出显示 wss（8） 测量 MySQL 数据库服务器 （mysqld） 的 WSS，每 1 秒打印一次累积 WSS：

# ./wss.pl $(pgrep -n mysqld) 1
 Watching PID 423 page references grow, output every 1 seconds...
 Est(s)     RSS(MB)    PSS(MB)    Ref(MB)
 1.014       403.66     400.59      86.00
 2.034       403.66     400.59      90.75
 3.054       403.66     400.59      94.29
 4.074       403.66     400.59      97.53
 5.094       403.66     400.59     100.33
 6.114       403.66     400.59     102.44
 7.134       403.66     400.59     104.58
 8.154       403.66     400.59     106.31
 9.174       403.66     400.59     107.76
 10.194      403.66     400.59     109.14
 
输出显示，到 5 秒标记时，mysqld 已经触及了大约 100 MB 的内存。mysqld 的 RSS 为 400 MB。输出还包括间隔的估计时间，包括设置和读取访问位 （Est（s）） 所花费的时间，以及专有集大小 （PSS），该大小考虑了与其他进程共享页面的情况。

此工具的工作原理是重置进程中每个页面的 PTE 访问位，暂停间隔，然后检查位以查看已设置的位。由于这是基于页面的，因此决定是页面大小，通常为 4 KB。考虑它报告的数字已被四舍五入到页面大小。

警告：此工具使用 /proc/PID/clear_refs 和 /proc/PID/smaps，当内核遍历页面结构时，这可能会导致应用程序延迟略高（例如 10%）。对于大型进程 （> 100 GB），这种较高延迟的持续时间可能会持续一秒以上，在此期间，此工具会消耗系统 CPU 时间。请记住这些开销。这个工具还会重置 referenced 标志，这可能会让内核混淆要回收哪些页面，尤其是在 swapping 处于活动状态时。此外，它还会激活一些以前可能未在您的环境中使用过的旧内核代码。首先在实验室环境中进行测试，以确保您了解开销。

# #7.5.13 bpftrace
bpftrace 是一种基于 BPF 的跟踪器，它提供了一种高级编程语言，允许创建强大的单行代码和简短脚本。它非常适合根据其他工具的线索进行自定义应用程序分析。bpftrace 存储库包含用于内存分析的其他工具，包括 oomkill.bt [Robertson 20]。

bpftrace 在第 15 章 BPF 中进行了解释。本节介绍一些内存分析示例。

# 单行
以下单行代码非常有用，并演示了不同的 bpftrace 功能

按用户堆栈和进程对 libc malloc（） 请求字节数求和（开销高）：
#  bpftrace -e 'uprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc {
    @[ustack, comm] = sum(arg0); }'
  
PID 181 的用户堆栈对 libc malloc（） 请求字节数求和 （高开销）
# bpftrace -e 'uprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc /pid == 181/ {
    @[ustack] = sum(arg0); 
}'

将 PID 181 的用户堆栈的 libc malloc（） 请求字节数显示为 2 的幂直方图（高开销）：
#  bpftrace -e 'uprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc /pid == 181/ 
{
    @[ustack] = hist(arg0); 
}'

按内核堆栈跟踪对内核 kmem 缓存分配字节数求和：
bpftrace -e 't:kmem:kmem_cache_alloc { @bytes[kstack] = sum(args->bytes_alloc); }'

按代码路径对进程堆扩展 （brk（2）） 进行计数
#  bpftrace -e 'tracepoint:syscalls:sys_enter_brk { @[ustack, comm] = count(); }'

按进程对页面错误进行计数：
# bpftrace -e 'software:page-fault:1 { @[comm, pid] = count(); }'

按用户级堆栈跟踪对用户页面错误进行计数
# bpftrace -e 't:exceptions:page_fault_user { @[ustack, comm] = count(); }'
按跟踪点对 vmscan作进行计数
# bpftrace -e 'tracepoint:vmscan:* { @[probe] = count(); }'

按进程计数 swapins：
# bpftrace -e 'kprobe:swap_readpage { @[comm, pid] = count(); }'

计数页面迁移：
# bpftrace -e 'tracepoint:migrate:mm_migrate_pages { @ = count(); }'

跟踪压缩事件：
# bpftrace -e 't:compaction:mm_compaction_begin { time(); }'

在 libc 中列出 USDT 探针：
# bpftrace -l 'usdt:/lib/x86_64-linux-gnu/libc.so.6:*'

列出内核 kmem 跟踪点：
# bpftrace -l 't:kmem:*'

列出所有内存子系统 （mm） 跟踪点：
# bpftrace -l 't:*:mm_*'

# 用户分配堆栈
可以从使用的分配函数中跟踪用户级别的分配。在此示例中，将跟踪 libc 中的 malloc（3） 函数以查找 PID 4840（一个 MySQL 数据库服务器）。分配请求大小记录为以用户级堆栈跟踪为键的直方图： 

 # bpftrace -e 'uprobe:/lib/x86_64-linux-gnu/libc.so.6:malloc /pid == 4840/ {
    @[ustack] = hist(arg0); }'
 Attaching 1 probe...
 ^C
 [...]
    __libc_malloc+0
    Filesort_buffer::allocate_sized_block(unsigned long)+52
    0x562cab572344
    filesort(THD*, Filesort*, RowIterator*, Filesort_info*, Sort_result*, unsigned 
long long*)+4017
    SortingIterator::DoSort(QEP_TAB*)+184
    SortingIterator::Init()+42
    SELECT_LEX_UNIT::ExecuteIteratorQuery(THD*)+489
    SELECT_LEX_UNIT::execute(THD*)+266
    Sql_cmd_dml::execute_inner(THD*)+563
    Sql_cmd_dml::execute(THD*)+1062
    mysql_execute_command(THD*, bool)+2380
    Prepared_statement::execute(String*, bool)+2345
    Prepared_statement::execute_loop(String*, bool)+172
    mysqld_stmt_execute(THD*, Prepared_statement*, bool, unsigned long, PS_PARAM*)
 +385
    dispatch_command(THD*, COM_DATA const*, enum_server_command)+5793
    do_command(THD*)+420
    0x562cab464fe0
    0x562cacad873a
    start_thread+217
 ]: 
[32K, 64K)           676 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
 [64K, 128K)          338 |@@@@@@@@@@@@@@@@@@@@@@@@@@                          |
 
输出显示，在跟踪时，此代码路径有 676 个 malloc（） 请求，大小在 32 KB 到 64 KB 之间，338 个请求大小在 64 KB 和 128 KB 之间。

# malloc（） 字节火焰图
上一个单行代码的输出有很多页长，因此更容易理解为火焰图。可以使用以下步骤生成一个：

# bpftrace -e 'u:/lib/x86_64-linux-gnu/libc.so.6:malloc /pid == 4840/ {
    @[ustack] = hist(arg0); }' > out.stacks
 $ git clone https://github.com/brendangregg/FlameGraph; cd FlameGraph
 $ ./stackcollapse-bpftrace.pl < ../out.stacks | ./flamegraph.pl --hash \
    --bgcolor=green --count=bytes --title="malloc() Bytes Flame Graph" > out.svg

警告：用户级分配请求可能是一项频繁的活动，每秒发生数百万次。虽然检测成本很小，但当乘以高速率时，可能会导致跟踪时出现大量 CPU 开销，使目标速度降低两倍或更多，请谨慎使用。因为它成本低，所以我首先使用堆栈跟踪的 CPU 分析来处理分配路径，或下一节中所示的页面错误跟踪。

# Page Fault 火焰图
跟踪页面错误显示进程内存大小何时增加。前面的 malloc（） 单行代码跟踪了分配路径。页面错误跟踪在前面的 Section 7.5.10 perf 中执行，并从中生成了火焰图。改用 bpftrace 的一个优点是，堆栈跟踪可以聚合在内核空间中以提高效率，并且只有唯一的堆栈和计数才会写入用户空间。

以下命令使用 bpftrace 收集页面错误堆栈跟踪，然后从中生成火焰图：

 # bpftrace -e 't:exceptions:page_fault_user { @[ustack, comm] = count(); }
    ' > out.stacks
 $ git clone https://github.com/brendangregg/FlameGraph; cd FlameGraph
 $ ./stackcollapse-bpftrace.pl < ../out.stacks | ./flamegraph.pl --hash \
    --bgcolor=green --count=pages --title="Page Fault Flame Graph" > out.svg
  
请参阅 第 7.5.10 节 perf，以获取页面错误堆栈跟踪和火焰图示例

# 内存内部
如果需要，您可以开发自定义工具来更深入地探索内存分配和内部结构。首先尝试内核内存事件的跟踪点，以及库分配器（如 libc）的 USDT 探测。列表跟踪点：

 # bpftrace -l 'tracepoint:kmem:*'
 tracepoint:kmem:kmalloc
 tracepoint:kmem:kmem_cache_alloc
 tracepoint:kmem:kmalloc_node
 tracepoint:kmem:kmem_cache_alloc_node
 tracepoint:kmem:kfree
 tracepoint:kmem:kmem_cache_free
 [...]
 # bpftrace -l 't:*:mm_*'
 tracepoint:huge_memory:mm_khugepaged_scan_pmd
 tracepoint:huge_memory:mm_collapse_huge_page
 tracepoint:huge_memory:mm_collapse_huge_page_isolate
 tracepoint:huge_memory:mm_collapse_huge_page_swapin
 tracepoint:migrate:mm_migrate_pages
 tracepoint:compaction:mm_compaction_isolate_migratepages
 tracepoint:compaction:mm_compaction_isolate_freepages
 [...]

这些跟踪点中的每一个都有可以使用 -lv 列出的参数。在这个内核 （5.3） 上，有 12 个 kmem 跟踪点，以及 47 个以 “mm_” 开头的跟踪点。

在 Ubuntu 上列出 libc 的 USDT 探针：

 # bpftrace -l 'usdt:/lib/x86_64-linux-gnu/libc.so.6'
 usdt:/lib/x86_64-linux-gnu/libc.so.6:libc:setjmp
 usdt:/lib/x86_64-linux-gnu/libc.so.6:libc:longjmp
 usdt:/lib/x86_64-linux-gnu/libc.so.6:libc:longjmp_target
 usdt:/lib/x86_64-linux-gnu/libc.so.6:libc:lll_lock_wait_private
 usdt:/lib/x86_64-linux-gnu/libc.so.6:libc:memory_mallopt_arena_max
 usdt:/lib/x86_64-linux-gnu/libc.so.6:libc:memory_mallopt_arena_test
 usdt:/lib/x86_64-linux-gnu/libc.so.6:libc:memory_tunable_tcache_max_bytes
 [...]

对于此 libc 版本 （6），有 33 个 USDT 探针

如果 tracepoints 和 USDT 探针不足，请考虑将动态插桩与 kprobes 和 uprobe 一起使用。

还有内存观察点的观察点探测类型：读取、写入或执行指定内存地址时的事件。

由于内存事件可能非常频繁，因此检测它们可能会消耗大量开销。来自用户空间的 malloc（3） 函数每秒可以被调用数百万次，并且根据当前的 uprobes 开销（参见第 4 章 可观测性工具， 第 4.3.7 节 uprobes），跟踪它们会使目标的速度降低两倍或更多。请谨慎使用并找到减少此开销的方法，例如使用地图汇总统计数据而不是打印每个事件的详细信息，并追踪尽可能少的事件。

# #7.5.14 其他工具
表 7.6 中列出了本书其他章节和 BPF 性能工具 [Gregg 19] 中包含的内存可观察性工具。

其他 Linux 内存可观测性工具和源包括：

■ dmesg：检查来自 OOM 杀手的“内存不足”消息。
■ dmidecode：显示内存组的 BIOS 信息。
■ tiptop：top（1） 的一个版本，按进程显示 PMC 统计信息。
■ valgrind：一个性能分析套件，包括 memcheck，一个用于内存使用分析（包括泄漏检测）的用户级分配器的包装器。这会花费大量开销;手册建议它会导致目标运行速度慢 20 到 30 倍 [Valgrind 20]。
■ iostat：如果交换设备是物理磁盘或片，则可以使用 iostat（1） 来观察设备 I/O，这表明系统正在分页。
■ /proc/zoneinfo：内存区（DMA 等）的统计信息。
■ /proc/buddyinfo： 页面的内核伙伴分配器的统计信息。
■ /proc/pagetypeinfo： 内核可用内存页面统计;可用于帮助调试内核内存碎片问题。■ /sys/devices/system/node/node*/numastat：NUMA 节点的统计信息。
■ SysRq m：Magic SysRq 有一个“m”键，用于将内存信息转储到控制台。

这是 dmidecode（8） 的输出示例，显示了一个内存库：

# dmidecode
 [...]
 Memory Device
        Array Handle: 0x0003
        Error Information Handle: Not Provided
        Total Width: 64 bits
        Data Width: 64 bits
        Size: 8192 MB
        Form Factor: SODIMM
        Set: None
        Locator: ChannelA-DIMM0
        Bank Locator: BANK 0
        Type: DDR4
        Type Detail: Synchronous Unbuffered (Unregistered)
        Speed: 2400 MT/s
        Manufacturer: Micron
        Serial Number: 00000000
        Asset Tag: None
        Part Number: 4ATS1G64HZ-2G3A1    
        Rank: 1
        Configured Clock Speed: 2400 MT/s
        Minimum Voltage: Unknown
        Maximum Voltage: Unknown
        Configured Voltage: 1.2 V
 [...]

此输出对于静态性能调整非常有用（例如，它显示类型是 DDR4 而不是 DDR5）。遗憾的是，云来宾通常无法使用此信息

以下是 SysRq “m” 触发器的一些示例输出：

# echo m > /proc/sysrq-trigger
 # dmesg
 [...]
 [334849.389256] sysrq: Show Memory
 [334849.391021] Mem-Info:
 [334849.391025] active_anon:110405 inactive_anon:24 isolated_anon:0
                 active_file:152629 inactive_file:137395 isolated_file:0
                 unevictable:4572 dirty:311 writeback:0 unstable:0
                 slab_reclaimable:31943 slab_unreclaimable:14385
                 mapped:37490 shmem:186 pagetables:958 bounce:0
                 free:37403 free_pcp:478 free_cma:2289
[334849.391028] Node 0 active_anon:441620kB inactive_anon:96kB active_file:610516kB 
inactive_file:549580kB unevictable:18288kB isolated(anon):0kB isolated(file):0kB 
mapped:149960kB dirty:1244kB writeback:0kB shmem:744kB shmem_thp: 0kB 
shmem_pmdmapped: 0kB anon_thp: 2048kB writeback_tmp:0kB unstable:0kB 
all_unreclaimable? no
 [334849.391029] Node 0 DMA free:12192kB min:360kB low:448kB high:536kB ...
 [...]

如果系统已锁定，这可能很有用，因为仍然可以使用控制台键盘上的 SysRq 键序列（如果可用）来请求此信息 [Linux 20g]

应用程序和虚拟机（例如 Java VM）也可能提供自己的内存分析工具。请参见第 5 章 “应用程序”

# 7.6 调试
您可以做的最重要的内存调整是确保应用程序保留在主内存中，并且分页和交换不会频繁发生。第 7.4 节 方法和第 7.5 节 可观测性工具中介绍了如何识别此问题。本节讨论其他内存调整：内核可调参数、配置大页、分配器和资源控制。

优化的细节 — 可用选项和设置选项的内容 — 取决于操作系统版本和预期的工作负载。以下各节（按优化类型组织）提供了哪些可调参数可能可用的示例，以及为什么需要调整这些参数。

# #7.6.1 可调参数
本节介绍最新 Linux 内核的可调参数示例

各种内存可调参数在 Documentation/sysctl/vm.txt 的内核源文档中有描述，可以使用 sysctl（8） 进行设置。表 7.7 中的示例来自 5.3 内核，默认值来自 Ubuntu 19.10（本书第一版中列出的那些从那时起没有更改）。

可调参数使用包含单元的一致命名方案。请注意，dirty_ background_bytes 和 dirty_background_ratio 是互斥的，dirty_bytes 和 dirty_ratio 也是互斥的（当设置一个时，它会覆盖另一个）。

vm.min_free_kbytes的大小被动态设置为主内存的一部分。选择此函数的算法不是线性的，因为对可用内存的需求不会随主内存大小线性扩展。（作为参考，这记录在 Linux 源代码中，以 mm/page_alloc.c.）可以减少vm.min_free_kbytes以释放一些内存供应用程序使用，但这也可能导致内核在内存压力期间不堪重负，并尽快使用 OOM。增加它有助于避免 OOM 终止。

另一个避免 OOM 的参数是 vm.overcommit_memory，可以将其设置为 2 以禁用超额提交并避免导致 OOM 的情况。如果要按进程控制 OOM 终止程序，请检查内核版本中是否存在 /proc 可调参数，例如 oom_adj 或 oom_score_adj。这些在 Documentation/filesystems/proc.txt 中进行了描述。

如果 vm.swappiness 可调参数比预期更早地开始交换应用程序内存，则它可能会显著影响性能。此可调参数的值可以介于 0 和 100 之间，值越高，则有利于交换应用程序，从而保留页面缓存。它可能能够将其设置为零，以便以牺牲页面缓存为代价尽可能长时间地保留应用程序内存。当内存仍然不足时，内核仍然可以使用 swapping。

在 Netflix，早期内核（围绕 Linux 3.13）的 kernel.numa_balancing 设置为零，因为过于激进的 NUMA 扫描消耗了太多的 CPU [Gregg 17d]。这在后来的内核中得到了修复，还有其他可调参数，包括用于调整 NUMA 扫描主动性的 kernel.numa_balancing_scan_size_mb。

# #7.6.2 多种页面大小
较大的页面大小可以通过提高 TLB 缓存的命中率（增加其覆盖范围）来提高内存 I/O 性能。大多数现代处理器都支持多种页面大小，例如默认 4 KB 和 2 MB 大页面。

在 Linux 上，可以通过多种方式配置大页面（称为大页面）。有关参考，请参阅 Documentation/vm/hugetlbpage.txt。

这些通常从创建大页面开始

 # echo 50 > /proc/sys/vm/nr_hugepages
 # grep Huge /proc/meminfo 
AnonHugePages:         0 kB
 HugePages_Total:      50
 HugePages_Free:       50
 HugePages_Rsvd:        0
 HugePages_Surp:        0
 Hugepagesize:       2048 kB

应用程序使用大页面的一种方法是通过共享内存段和 shmget（2） 的 SHM_HUGETLBS 标志。另一种方法涉及为应用程序创建一个巨大的基于页面的文件系统，以便从中映射内存：

 # mkdir /mnt/hugetlbfs
 # mount -t hugetlbfs none /mnt/hugetlbfs -o pagesize=2048K

其他方法包括 MAP_ANONYMOUS|MAP_HUGETLB mmap（2） 的标志并使用 libhugetlbfs API [Gorman 10]。

最后，透明大页面 （THP） 是另一种使用大页面的机制，它通过自动将普通页面提升和降级为大页面，而无需应用程序指定大页面 [Corbet 11]。在 Linux 源代码中，请参阅 Documentation/vm/transhuge.txt 和 admin-guide/ mm/transhuge.rst。

# #7.6.3 分配器
可以使用不同的用户级分配器，从而为多线程应用程序提供改进的性能。可以在编译时选择这些选项，也可以在执行时通过设置 LD_PRELOAD 环境变量来选择这些选项。

例如，可以使用 libtcmalloc 分配器

# export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4

这可以放在其启动脚本中。

# #7.6.4 NUMA 绑定
在 NUMA 系统上，numactl（8） 命令可用于将进程绑定到 NUMA 节点。这可以提高不需要多个主内存的 NUMA 节点的应用程序的性能。用法示例：

 # numactl --membind=0
 
这会将 PID 3161 绑定到 NUMA 节点 0。如果无法从此节点满足此进程的未来内存分配，则这些内存分配将失败。使用此选项时，还应调查使用 --physcpubind 选项，以将 CPU 使用率限制为连接到该 NUMA 节点的 CPU。我通常同时使用 NUMA 和 CPU 绑定将进程限制为单个套接字，以避免 CPU 互连访问的性能损失。

使用 numastat（8） （第 7.5.6 节） 列出可用的 NUMA 节点

# #7.6.5  资源控制
基本的资源控制，包括设置主内存限制和虚拟内存限制，可以使用 ulimit（1） 来实现

对于 Linux，容器组 （cgroups） 内存子系统提供各种附加控件。这些包括：

■ memory.limit_in_bytes：允许的最大用户内存，包括文件缓存使用情况（以字节为单位）
■ memory.memsw.limit_in_bytes：允许的最大内存和交换空间，以字节为单位（使用交换时）
■ memory.kmem.limit_in_bytes：允许的最大内核内存，以字节为单位 
■ memory.tcp.limit_in_bytes：最大 tcp 缓冲区内存，以字节为单位。
■ memory.swappiness：类似于前面描述的 vm.swappiness，但可以为 cgroup 设置
■ memory.oom_control：可以设置为 0，以允许此 cgroup 的 OOM 杀手，或 1 以禁用它

Linux 还允许在 /etc/security/limits.conf 中进行系统范围的配置

有关资源控制的更多信息，请参见第 11 章 “云计算”。

# 7.7 练习
1. 回答以下关于记忆术语的问题：
■ 什么是记忆页？
■ 什么是常驻记忆？
■ 什么是虚拟内存？
■ 使用 Linux 术语，分页和交换之间有什么区别？

2. 回答以下概念性问题：
■ 需求分页的目的是什么？
■ 描述内存利用率和饱和度。
■ MMU 和 TLB 的目的是什么？
■ page-out 守护程序的作用是什么？
■ OOM killer 的作用是什么？

3. 回答以下更深层次的问题：
■ 什么是匿名分页，为什么分析比文件系统分页更重要？
■ 描述在基于 Linux 的系统上，当可用内存耗尽时，内核为释放更多内存而采取的步骤。
■ 描述基于 slab 的分配的性能优势。

4. 为您的操作系统制定以下程序：
■ 内存资源的 USE 方法检查表。包括如何获取每个指标（例如，要执行哪个命令）以及如何解释结果。在安装或使用其他软件产品之前，请尝试使用现有的操作系统可观测性工具。
■ 为内存资源创建工作负载特征检查表。包括如何获取每个指标，并首先尝试使用现有的操作系统可观测性工具
■ 选择一个应用程序，并总结导致内存分配的代码路径 （malloc（3））。
■ 选择具有一定程度内存增长的应用程序（调用 brk（2） 或 sbrk（2）），并总结导致此增长的代码路径。
■ 单独描述以下 Linux 输出中可见的内存活动
 # vmstat 1
 procs -----------memory-------- ---swap-- -----io---- --system-- -----cpu----
 r  b   swpd   free  buff cache   si   so    bi    bo   in   cs us sy id wa st
 2  0 413344  62284    72  6972    0    0    17    12    1    1  0  0 100  0  0
 2  0 418036  68172    68  3808    0 4692  4520  4692 1060 1939 61 38  0  1  0
 2  0 418232  71272    68  1696    0  196 23924   196 1288 2464 51 38  0 11  0
 2  0 418308  68792    76  2456    0   76  3408    96 1028 1873 58 39  0  3  0
 1  0 418308  67296    76  3936    0    0  1060     0 1020 1843 53 47  0  0  0
 1  0 418308  64948    76  3936    0    0     0     0 1005 1808 36 64  0  0  0
 1  0 418308  62724    76  6120    0    0  2208     0 1030 1870 62 38  0  0  0
 1  0 422320  62772    76  6112    0 4012     0  4016 1052 1900 49 51  0  0  0
 1  0 422320  62772    76  6144    0    0     0     0 1007 1826 62 38  0  0  0
 1  0 422320  60796    76  6144    0    0     0     0 1008 1817 53 47  0  0  0
 1  0 422320  60788    76  6144    0    0     0     0 1006 1812 49 51  0  0  0
 3  0 430792  65584    64  5216    0 8472  4912  8472 1030 1846 54 40  0  6  0
 1  0 430792  64220    72  6496    0    0  1124    16 1024 1857 62 38  0  0  0
 2  0 434252  68188    64  3704    0 3460  5112  3460 1070 1964 60 40  0  0  0
 2  0 434252  71540    64  1436    0    0 21856     0 1300 2478 55 41  0  4  0
 1  0 434252  66072    64  3912    0    0  2020     0 1022 1817 60 40  0  0  0
 [...]

6. （可选，高级）查找或开发指标，以显示内核 NUMA 内存局部性策略在实践中的运行情况。开发具有良好或较差内存局部性的 “已知” 工作负载，以测试指标。

# 7.8 引用
 [Corbató 68] Corbató, F. J., A Paging Experiment with the Multics System, MIT Project MAC 
Report MAC-M-384, 1968. 
[Denning 70] Denning, P., “Virtual Memory,” ACM Computing Surveys (CSUR) 2, no. 3, 1970.
 [Peterson 77] Peterson, J., and Norman, T., “Buddy Systems,” Communications of the ACM, 
1977. 
[Thompson 78] Thompson, K., UNIX Implementation, Bell Laboratories, 1978.
 [Babaoglu 79] Babaoglu, O., Joy, W., and Porcar, J., Design and Implementation of the 
Berkeley Virtual Memory Extensions to the UNIX Operating System, Computer Science Division, 
Deptartment of Electrical Engineering and Computer Science, University of California, 
Berkeley, 1979. 
[Bach 86] Bach, M. J., The Design of the UNIX Operating System, Prentice Hall, 1986. 
[Bonwick 94] Bonwick, J., “The Slab Allocator: An Object-Caching Kernel Memory 
Allocator,” USENIX, 1994.
 [Bonwick 01] Bonwick, J., and Adams, J., “Magazines and Vmem: Extending the Slab 
Allocator to Many CPUs and Arbitrary Resources,” USENIX, 2001.
 [Corbet 04] Corbet, J., “2.6 swapping behavior,” LWN.net, http://lwn.net/Articles/83588, 2004
 [Gorman 04] Gorman, M., Understanding the Linux Virtual Memory Manager, Prentice Hall, 
2004. 
 [McDougall 06a] McDougall, R., Mauro, J., and Gregg, B., Solaris Performance and Tools: 
DTrace and MDB Techniques for Solaris 10 and OpenSolaris, Prentice Hall, 2006.
 [Ghemawat 07] Ghemawat, S., “TCMalloc : Thread-Caching Malloc,” https://
 gperftools.github.io/gperftools/tcmalloc.html, 2007.
 [Lameter 07] Lameter, C., “SLUB: The unqueued slab allocator V6,” Linux kernel mailing list, 
http://lwn.net/Articles/229096, 2007.
 [Hall 09] Hall, A., “Thanks for the Memory, Linux,” Andrew Hall, https://www.ibm.com/
 developerworks/library/j-nativememory-linux, 2009.
 [Gorman 10] Gorman, M., “Huge pages part 2: Interfaces,” LWN.net, http://lwn.net/
 Articles/375096, 2010.
 [Corbet 11] Corbet, J., “Transparent huge pages in 2.6.38,” LWN.net, http://lwn.net/
 Articles/423584, 2011.
 [Facebook 11] “Scalable memory allocation using jemalloc,” Facebook Engineering, 
https://www.facebook.com/notes/facebook-engineering/scalable-memory-allocation
using-jemalloc/480222803919, 2011.
 [Evans 17] Evans, J., “Swapping, memory limits, and cgroups,” https://jvns.ca/
 blog/2017/02/17/mystery-swap, 2017.
 [Gregg 17d] Gregg, B., “AWS re:Invent 2017: How Netflix Tunes EC2,” http://
 www.brendangregg.com/blog/2017-12-31/reinvent-netflix-ec2-tuning.html, 2017.
 [Corbet 18a] Corbet, J., “Is it time to remove ZONE_DMA?” LWN.net, https://lwn.net/
 Articles/753273, 2018.
 [Crucial 18] “The Difference between RAM Speed and CAS Latency,” https://www.crucial.
 com/articles/about-memory/difference-between-speed-and-latency, 2018.
 [Gregg 18c] Gregg, B., “Working Set Size Estimation,” http://www.brendangregg.com/
 wss.html, 2018.
  [Facebook 19] “Getting Started with PSI,” Facebook Engineering, https://
 facebookmicrosites.github.io/psi/docs/overview, 2019.
 [Gregg 19] Gregg, B., BPF Performance Tools: Linux System and Application Observability, 
Addison-Wesley, 2019.
 [Intel 19a] Intel 64 and IA-32 Architectures Software Developer’s Manual, Combined Volumes: 
1, 2A, 2B, 2C, 3A, 3B and 3C, Intel, 2019. 
[Amazon 20] “Amazon EC2 High Memory Instances,” https://aws.amazon.com/ec2/
 instance-types/high-memory, accessed 2020.
 [Linux 20g] “Linux Magic System Request Key Hacks,” Linux documentation, https://
 www.kernel.org/doc/html/latest/admin-guide/sysrq.html, accessed 2020.
 [Robertson 20] Robertson, A., “bpftrace,” https://github.com/iovisor/bpftrace, last updated 
2020.
 [Valgrind 20] “Valgrind Documentation,” http://valgrind.org/docs/manual, May 2020