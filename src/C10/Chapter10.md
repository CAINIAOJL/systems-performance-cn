# Chapter 10: 网络
随着系统变得更加分散，尤其是在云计算环境中，网络在性能方面发挥着更大的作用。网络性能中的常见任务包括改善网络延迟和吞吐量，以及消除延迟异常值，这些异常值可能是由数据包丢弃或延迟引起的。

网络分析涵盖硬件和软件。硬件是物理网络，包括网络接口卡、交换机、路由器和网关（这些通常也具有软件）。该软件是内核网络堆栈，包括网络设备驱动程序、数据包队列和数据包调度程序，以及网络协议的实现。较低级别的协议通常是内核软件（IP、TCP、UDP 等），而高级协议通常是库或应用程序软件（例如 HTTP）。

考虑到潜在的拥塞及其固有的复杂性（归咎于未知），网络经常被指责为性能不佳。本章将展示如何弄清楚真正发生的事情，这可能会为网络开脱，以便继续进行分析

本章的学习目标是： 
■ 了解网络延迟的不同度量。
■ 具备常见网络协议的工作知识。
■ 熟悉网络硬件内部结构。
■ 熟悉 sockets 和 devices 的内核路径。
■ 遵循不同的网络分析方法。
■ 描述系统范围和每个进程的网络 I/O 的特征。
■ 识别由 TCP 重新传输引起的问题。
■ 使用跟踪工具调查网络内部。
■ 了解网络可调参数。

本章由六个部分组成，前三个部分为网络分析提供基础，后三个部分介绍其在基于 Linux 的系统中的实际应用。这些部分如下：

■ 背景介绍了与网络相关的术语、模型和关键网络性能概念。
■ 架构提供物理网络组件和网络堆栈的通用描述。
■ 方法描述了性能分析方法，包括观察性和实验性。
■ Observability Tools （可观测性工具） 显示了基于 Linux 的系统的网络性能可观测性工具。
■ 实验总结了网络基准测试和实验工具。
■ 优化 描述了示例可调参数。

网络基础知识（如 TCP 和 IP 的角色）是本章的假定知识

# 10.1 术语
作为参考，本章中使用的与网络相关的术语包括

■ 接口：术语接口端口是指物理网络连接器。术语 interface 或 link 是指 OS 所看到和所描述的网络接口端口的逻辑实例。（并非所有 OS 接口都由硬件支持：有些是虚拟的。
■ 数据包： 术语数据包是指数据包交换网络中的消息，例如 IP 数据包。
■ 帧：物理网络级消息，例如以太网帧。
■ 套接字：源自 BSD 的网络端点的 API。
■ 带宽：网络类型的最大数据传输速率，通常以每秒位数为单位。“100 GbE”是指带宽为 100 Gbits/s 的以太网。每个方向可能都有带宽限制，因此 100 GbE 可能能够并行进行 100 Gbits/s 的传输和 100 Gbit/s 的接收（200 Gbit/s 的总吞吐量）。
■ 吞吐量：网络终端节点之间的当前数据传输速率，以每秒位数或每秒字节数为单位。
■ 延迟：网络延迟可以指消息在终端节点之间往返所需的时间，或建立连接（例如 TCP 握手）所需的时间，不包括随后的数据传输时间。

本章介绍了其他术语。术语表包括供参考的基本术语，包括客户端、以太网、主机、IP、RFC、服务器、SYN、ACK。另请参阅第 2 章和第 3 章中的术语部分。

# 10.2 模型
以下简单模型说明了联网和网络性能的一些基本原理。第 10.4 节 体系结构 进行了更深入的挖掘，包括特定于实现的细节

# #10.2.1 网络接口
网络接口是网络连接的操作系统端点;它是由系统管理员配置和管理的抽象。

网络接口如图 10.1 所示。网络接口作为其配置的一部分映射到物理网络端口。端口连接到网络，通常具有单独的传输和接收通道。

# #10.2.2 控制器
网络接口卡 （NIC） 为系统提供一个或多个网络端口，并容纳一个网络控制器：一个用于在端口之间传输数据包和系统 I/O 传输的微处理器。图 10.2 中显示了一个具有四个端口的示例控制器，显示了所涉及的物理组件

控制器通常作为单独的扩展卡提供，或内置于系统主板中。（其他选项包括通过 USB。

# #10.2.3 协议栈
联网是通过一堆协议完成的，每一层都有特定的用途。图 10.3 显示了两种堆栈模型，并附有示例协议

较低层绘制得更宽，以指示协议封装。发送的消息沿堆栈从应用程序向下移动到物理网络。收到的消息向上移动。

请注意，以太网标准还描述了物理层，以及如何使用铜缆或光纤。

可能还有其他层，例如，如果使用 Internet 协议安全 （IPsec） 或 Linux WireGuard，则它们位于 Internet 层之上，以在 IP 端点之间提供安全性。此外，如果正在使用隧道（例如，虚拟可扩展 LAN （VXLAN）），则一个协议栈可以封装在另一个协议栈中。

虽然 TCP/IP 堆栈已成为标准，但我认为简要考虑一下 OSI 模型也很有用，因为它显示了应用程序中的协议层。“层”术语来自 OSI，其中第 3 层是指网络协议

不同层的消息也使用不同的术语。使用 OSI 模型：在传输层，消息是段或数据报;在网络层，消息是一个数据包;在数据链路层，消息是一个帧。

# 10.3 概念
以下是网络和网络性能中的一些重要概念

# #10.3.1 网络与路由
网络是一组连接的主机，由网络协议地址关联。拥有多个网络（而不是一个巨大的全球网络）是可取的，原因有很多，特别是可扩展性。某些网络消息将广播到所有相邻主机。通过创建较小的子网，可以在本地隔离此类广播消息，因此它们不会造成大规模泛洪问题。这也是将常规消息的传输隔离到源和目标之间的网络的基础，从而更有效地利用网络基础设施。

路由管理跨这些网络的消息（称为数据包）的传递。路由的作用如图 10.4 所示。

从主机 A 的角度来看，localhost 就是主机 A 本身。图中的所有其他主机均为远程主机

主机 A 可以通过本地网络连接到主机 B，通常由网络交换机驱动（请参见第 10.4 节 体系结构）。主机 A 可以通过路由器 1 连接到主机 C，通过路由器 1、2 和 3 连接到主机 D。由于路由器等网络组件是共享的，因此来自其他流量（例如，主机 C 到主机 E）的争用可能会损害性能

主机对之间的连接涉及单播传输。多点传送允许发送方同时向多个目标传输数据，这些目标可能跨越多个网络。路由器配置必须支持此功能才能允许传递。在公共云环境中，它可能会被阻止。

除了路由器，典型的网络还会使用防火墙来提高安全性，阻止主机之间不需要的连接。

路由数据包所需的地址信息包含在 IP 报头中。

# #10.3.2 协议
网络协议标准（如 IP、TCP 和 UDP 标准）是系统和设备之间通信的必要要求。通信是通过传输称为数据包的可路由消息来执行的，通常是通过封装有效负载数据

网络协议具有不同的性能特征，这些特征源于原始协议设计、扩展或软件或硬件的特殊处理。例如，不同版本的 IP 协议（IPv4 和 IPv6）可能由不同的内核代码路径处理，并可能表现出不同的性能特征。其他协议在设计上的表现不同，当它们适合工作负载时可以选择：示例包括流控制传输协议 （SCTP）、多路径 TCP （MPTCP） 和 QUIC。

通常，还有一些系统可调参数可以通过更改设置（例如缓冲区大小、算法和各种计时器）来影响协议性能。特定协议的这些差异将在后面的部分中介绍。

协议通常使用封装来传输数据。

# #10.3.3 封装
封装会在开头（页眉）、结尾（页脚）或两者处将元数据添加到有效负载中。这不会更改有效负载数据，但确实会略微增加消息的总大小，从而消耗一些传输开销。

图 10.5 显示了使用以太网的 TCP/IP 堆栈的封装示例。

E.H. 是以太网页眉，E.F. 是可选的以太网页脚。

# #10.3.4 数据包尺寸
数据包的大小及其有效负载会影响性能，较大的数据包大小会提高吞吐量并减少数据包开销。对于 TCP/IP 和以太网，数据包可以介于 54 到 9,054 字节之间，包括 54 字节（或更多，取决于选项或版本）的原型列标头

数据包大小通常受网络接口最大传输单元 （MTU） 大小的限制，对于许多以太网网络，MTU 配置为 1,500 字节。1500 MTU 大小的来源是早期版本的以太网，以及平衡 NIC 缓冲区等因素的需要内存成本和传输延迟 [Nosachev 20]。主机争相使用共享介质（同轴电缆或以太网集线器），更大的尺寸增加了主机等待轮到他们的延迟。

以太网现在支持最大约 9,000 字节的较大数据包（帧），称为巨型帧。这些可以通过减少数据包的要求来提高网络吞吐量性能，以及数据传输的延迟

两个组件的汇合干扰了巨型帧的采用：较旧的网络硬件和配置错误的防火墙。不支持巨型帧的旧硬件可以使用 IP 协议对数据包进行分段（导致数据包重组的性能成本），或者使用 ICMP“can't fragment”错误进行响应，让发送方知道要减小数据包大小。现在，配置错误的防火墙开始发挥作用：过去曾发生过基于 ICMP 的攻击（包括“死亡之 ping”），一些防火墙管理员通过阻止所有 ICMP 来应对这些攻击。这可以防止有用的“can't fragment”消息到达发送方，并导致网络数据包在数据包大小增加到 1,500 以上时被静默丢弃。如果收到 ICMP 消息并发生分片，则还存在分片数据包被不支持分片的设备丢弃的风险。为避免这些问题，许多系统坚持使用 1,500 MTU 默认值。

网络接口卡功能（包括 TCP 卸载和大分段卸载）提高了 1500 个 MTU 帧的性能。这些缓冲区将较大的缓冲区发送到网卡，然后网卡可以使用专用和优化的硬件将它们拆分为更小的帧。这在一定程度上缩小了 1,500 和 9,000 MTU 网络性能之间的差距。

# #10.3.5 延迟
延迟是衡量网络性能的重要指标，可以通过不同的方式进行测量，包括名称解析延迟、ping 延迟、连接延迟、首字节延迟、往返时间和连接寿命。这些被描述为由连接到服务器的客户端测量。

# 名称解析延迟
在与远程主机建立连接时，主机名通常被解析为 IP 地址，例如，通过 DNS 解析。这所花费的时间可以单独衡量为名称解析延迟。此延迟的最坏情况涉及名称解析超时，这可能需要数十秒。

操作系统通常提供提供缓存的名称解析服务，以便后续的 DNS 查找可以从缓存中快速解析。有时应用程序只使用 IP 地址而不使用名称，因此完全避免了 DNS 延迟

# ping延迟
这是 ICMP 回应请求回应响应的时间，由 ping（1） 命令测量。此时间用于测量主机之间的网络延迟（包括主机之间的跃点），并测量为网络请求进行往返所需的时间。它之所以被广泛使用，是因为它简单且通常很容易获得：许多操作系统会通过以下默认方式响应 ping。它可能无法准确反映应用程序请求的往返时间，因为路由器可能会以不同的优先级处理 ICMP。

表 10.1 显示了 ping 延迟示例。

为了更好地说明所涉及的数量级，Scaled 列显示了基于一秒的虚构 localhost ping 延迟的比较。

# 连接延迟
连接延迟是指在传输任何数据之前建立网络连接的时间。对于 TCP 连接延迟，这是 TCP 握手时间。从客户端开始测量，是从发送 SYN 到收到相应的 SYN-ACK 的时间。连接延迟可能更适合称为连接建立延迟，以明确区分它与连接生命周期。

连接延迟类似于 ping 延迟，尽管它会执行更多的内核代码来建立连接，并且包括重新传输任何丢弃的数据包的时间。如果 TCP SYN 数据包的积压已满，则服务器可以丢弃 TCP SYN 数据包，从而导致客户端发送基于计时器的 SYN 重新传输。这发生在 TCP 握手期间，因此连接延迟可能包括重新传输延迟，从而增加一秒或多秒。

连接延迟后跟首字节延迟

# 首字节延迟
首字节延迟也称为首字节时间 （TTFB），是指从建立连接到收到第一个字节数据的时间。这包括远程主机接受连接、调度为其提供服务的线程以及该线程执行和发送第一个字节的时间。

ping 和连接延迟衡量的是网络产生的延迟，而首字节延迟包括目标服务器的思考时间。如果服务器过载并且需要时间来处理请求（例如 TCP 积压）和调度服务器（CPU 调度程序延迟），这可能包括延迟。

# 往返时间(RTT)
往返时间 （RTT） 描述网络请求在终端节点之间进行往返所需的时间。这包括信号传播时间和每个网络跃点的处理时间。预期用途是确定网络的延迟，因此理想情况下，RTT 由请求和回复数据包在网络上花费的时间（而不是远程主机为请求提供服务的时间）决定。经常研究 ICMP 回显请求的 RTT，因为远程主机处理时间最短

# 连接寿命
连接生存期是从建立网络连接到关闭网络连接的时间。一些协议使用保持连接策略，延长连接的持续时间，以便未来的作可以使用现有连接，并避免连接建立（和 TLS 建立）的开销和延迟。

有关更多网络延迟测量，请参见部分 10.5.4， 延迟分析，其中介绍了如何使用它们来诊断网络性能

# #10.3.6 缓冲
尽管可能会遇到各种网络延迟，但通过在发送方和接收方上使用缓冲，可以以高速率保持网络吞吐量。较大的缓冲区可以通过在阻塞和等待确认之前继续发送数据来减轻较长往返时间的影响

TCP 使用缓冲和滑动发送窗口来提高吞吐量。网络套接字也有缓冲区，应用程序也可以使用自己的缓冲区，在发送之前聚合数据。

缓冲也可以由外部网络组件（如交换机和路由器）执行，以提高其自身的吞吐量。遗憾的是，在这些组件上使用大型缓冲区可能会导致缓冲区膨胀，即数据包会排队很长时间。这会导致主机上避免 TCP 拥塞，从而限制性能。Linux 3.x 内核中添加了一些功能来解决这个问题（包括字节队列限制、CoDel 排队规则 [Nichols 12] 和 TCP 小队列）。还有一个网站用于讨论这个问题 [Bufferbloat 20]。

缓冲（或大型缓冲）的功能可能最好由端点（主机）而不是中间网络节点提供，遵循称为端到端参数的原则 [Saltzer 84]

# #10.3.7 连接积压
另一种类型的缓冲用于初始连接请求。TCP 实现了一个 backlog，其中 SYN 请求可以在被 user-land 进程接受之前在内核中排队。当进程无法及时接受的 TCP 连接请求过多时，积压将达到限制，并且 SYN 数据包将被丢弃，稍后由客户端重新传输。这些数据包的 retrans 任务会导致客户端连接时间延迟。limit 是可调的：它是 listen（2） syscall 的一个参数，内核也可以提供系统范围的限制。

积压丢弃和 SYN 重新传输是主机过载的指标。

# #10.3.8 接口协商
网络接口可以以不同的模式运行，在连接的收发器之间自动协商。一些例子是

■ 带宽：例如，10、100、1000、10000、40000、100000 Mbits/s
■ 双工：半双工或全双工

这些示例来自以太网，以太网倾向于使用整数 10 基数来表示带宽限制。其他物理层协议（如 SONET）具有一组不同的可能带宽。

网络接口通常根据其最高带宽和协议进行描述，例如 1 Gbit/s 以太网 （1 GbE）。但是，如果需要，此接口可能会自动协商以降低速度。如果另一个终端节点无法更快地运行，或者为了适应连接介质的物理问题（接线不良），则可能会发生这种情况

全双工模式允许双向同步传输，具有单独的发送和接收路径，每个路径都可以在全带宽下运行。半双工模式一次只允许一个方向。

# #10.3.9 拥塞避免
网络是共享资源，当流量负载较高时，网络可能会变得拥塞。这可能会导致性能问题：例如，路由器或交换机可能会丢弃数据包，从而导致延迟导致 TCP 重新传输。主机在接收高数据包速率时也会不堪重负，并且可能会自行丢弃数据包。

有许多机制可以避免这些问题;应研究这些机制，并在必要时进行调整，以提高负载下的可伸缩性。不同协议的示例包括：

■ 以太网：不堪重负的主机可能会向发射器发送暂停帧，请求它们暂停传输 （IEEE 802.3x）。每个类还有优先级类和优先级暂停帧。
■ IP：包括显式拥塞通知 （ECN） 字段。
■ TCP：包括一个拥塞窗口，可以使用各种拥塞控制算法

后面的部分将更详细地介绍 IP ECN 和 TCP 拥塞控制算法。

# #10.3.10 利用率
网络接口利用率可以计算为最大带宽上的当前吞吐量。考虑到由于自动协商而导致的可变带宽和双工，计算这并不像听起来那么简单。

对于全双工，利用率适用于每个方向，并按当前协商带宽上该方向的当前吞吐量来衡量。通常，最重要的只是一个方向，因为主机通常是不对称的：服务器是传输密集型的，而客户端是接收密集型的

一旦网络接口方向达到 100% 利用率，它就会成为瓶颈，从而限制性能

某些性能工具仅以数据包为单位报告活动，而不是以字节为单位。由于数据包大小可能会有很大差异（如前所述），因此无法将数据包计数与字节计数相关联来计算吞吐量或（基于吞吐量的）利用率

# #10.3.11 本地连接
网络连接可以在同一系统上的两个应用程序之间进行。这些是 localhost 连接，并使用虚拟网络接口：loopback。

分布式应用程序环境通常被拆分为通过网络进行通信的逻辑部分。这些服务器可以包括 Web 服务器、数据库服务器、缓存服务器、代理服务器和应用程序服务器。如果它们在同一主机上运行，则它们的连接将连接到 localhost。

通过 IP 连接到 localhost 是进程间通信 （IPC） 的 IP 套接字技术。另一种技术是 Unix 域套接字 （UDS），它在文件系统上创建一个文件以进行通信。使用 UDS 的性能可能会更好，因为可以绕过内核 TCP/IP 堆栈，跳过内核代码和协议数据包封装的开销。

对于 TCP/IP 套接字，内核可能会在握手后检测到 localhost 连接，然后为 TCP/IP 堆栈创建快捷方式以进行数据传输，从而提高性能。这是作为 Linux 内核功能开发的，称为 TCP 朋友，但未合并 [Corbet 12]。BPF 现在可以在 Linux 上用于此目的，就像 Cilium 软件一样，用于容器网络性能和安全性 [Cilium 20a]。

# 10.4 架构
本节介绍网络架构：协议、硬件和软件。这些被总结为性能分析和调整的背景，重点是性能特征。有关更多详细信息，包括一般网络主题，请参阅网络文本 [Stevens 93][Hassan 03]、RFC 和网络硬件的供应商手册。本章末尾列出了其中一些。

# 10.4.1 协议
本节总结了 IP、TCP、UDP 和 QUIC 的性能特性和特点。后面的硬件和软件部分将介绍如何在硬件和软件中实现这些协议（包括分段卸载、连接队列和缓冲等功能）。

# IP
Internet 协议 （IP） 版本 4 和 6 包括一个用于设置所需连接性能的字段：IPv4 中的 Type of Service 字段和 IPv6 中的 Traffic Class 字段。此后，这些字段被重新定义为包含差分服务代码点 （DSCP） （RFC 2474） [Nichols 98] 和显式拥塞通知 （ECN） 字段 （RFC 3168） [Ramakrishnan 01]。

DSCP 旨在支持不同的服务类，每个服务类都有不同的特性，包括丢包概率。示例服务类包括：电话、广播视频、低延迟数据、高吞吐量数据和低优先级数据

ECN 是一种机制，它允许路径上的服务器、路由器或交换机通过在 IP 报头中设置一个位来显式发出拥塞存在的信号，而不是丢弃数据包。接收器会将此信号回声给发送者，然后发送者可以限制传输。这提供了避免拥塞的好处，而不会产生丢包的惩罚（前提是 ECN 位在整个网络中正确使用）。

# TCP
传输控制协议 （TCP） 是用于创建可靠网络连接的常用 Internet 标准。TCP 由 RFC 793 [Postel 81] 和更高版本的添加指定。

在性能方面，TCP 通过使用缓冲和滑动窗口，即使在高延迟网络上也可以提供高吞吐量。TCP 还采用拥塞控制和发送者设置的拥塞窗口，因此它可以在不同和不同的网络中保持高但可靠的传输速率。拥塞控制可避免发送过多数据包，否则会导致拥塞和性能下降。

以下是 TCP 性能特性的摘要，包括自原始规范以来的新增功能：

■ 滑动窗口：这允许在收到确认之前在网络上发送多个最大窗口大小的数据包，即使在高延迟网络上也能提供高吞吐量。接收方公布窗口的大小，以指示当时愿意接收的数据包数。
■ 拥塞避免：防止发送过多数据并导致饱和，这可能会导致丢包和性能下降。
■ 慢启动：TCP 拥塞控制的一部分，它从一个小的拥塞窗口开始，然后在一定时间内收到确认 （ACK） 时增加它。如果不是，则拥塞窗口会减少。
■ 选择性确认 （SACK）：允许 TCP 确认不连续的数据包，从而减少所需的重传次数。
■ 快速重新传输：TCP 可以根据重复 ACK 的到达重新传输丢弃的数据包，而不是等待计时器。这些是往返时间的函数，而不是通常慢得多的计时器。
■ Fast recovery：在检测到重复的 ACK 后，通过重置连接以执行慢启动来恢复 TCP 性能。
■ TCP 快速打开：允许客户端在 SYN 数据包中包含数据，以便服务器请求处理可以提前开始，而无需等待 SYN 握手 （RFC7413）。这可以使用加密 Cookie 对客户端进行身份验证。
■ TCP 时间戳：包括在 ACK 中返回的已发送数据包的时间戳，以便测量往返时间 （RFC 1323） [Jacobson 92]。
■ TCP SYN Cookie：在可能的 SYN 泛洪攻击（完全积压）期间向客户端提供加密 Cookie，以便合法客户端可以继续连接，而无需服务器为这些连接尝试存储额外的数据。

在某些情况下，这些功能是通过使用添加到协议标头的扩展 TCP 选项来实现的

TCP 性能的重要主题包括三次握手、重复 ACK 检测、拥塞控制算法、Nagle、延迟 ACK、SACK 和 FACK。

# 三次握手
使用主机之间的三次握手建立连接。一个主机被动地侦听连接;另一个主动启动连接。澄清术语：被动和主动来自 RFC 793 [Postel 81];但是，它们通常分别称为 listen 和 connect，位于 socket API 之后。对于 client/server 模型，server 执行 listen，client 执行 connect。

三次握手如图 10.6 所示。

指示来自客户端的连接延迟，该延迟在发送最终 ACK 时完成。之后，数据传输可能会开始。

此图显示了握手的最佳延迟情况。数据包可能会被丢弃，从而在超时和重新传输时增加延迟。

三次握手完成后，TCP 会话将处于 ESTABLISHED 状态。

# 状态和计时器
TCP 会话根据数据包和套接字事件在 TCP 状态之间切换。状态包括 LISTEN、SYN-SENT、SYN-RECEIVED、ESTABLISHED、FIN-WAIT-1、FIN-WAIT-2、CLOSE-WAIT、CLOSING、LAST-ACK、TIME-WAIT 和 CLOSED [Postal 80]。性能分析通常侧重于处于 ESTABLISHED 状态的那些，即活动连接。此类连接可能正在传输数据，也可能处于空闲状态，等待下一个事件：数据传输或关闭事件。

已完全关闭的会话将进入 TIME_WAIT2 状态，以便延迟的数据包不会与相同端口上的新连接错误关联。这可能会导致端口耗尽的性能问题，如部分 10.5.7， TCP 分析中所述

某些状态具有与之关联的计时器。TIME_WAIT通常为 2 分钟（某些 kerners（如 Windows 内核）允许对其进行优化）。ESTABLISHED 上也可能有一个“保持活动状态”计时器，设置为较长的持续时间（例如，两小时），以触发探测数据包以检查远程主机是否仍然处于活动状态。

# 重复 ACK 检测
快速重传和快速恢复算法使用重复 ACK 检测来快速检测发送的数据包（或其 ACK）何时丢失。它的工作原理如下：

1. 发送方发送序列号为 10 的数据包。
2. 接收方回复序列号为 11 的 ACK。
3. 发送方发送 11、12 和 13。
4. 数据包 11 被丢弃。
5. 接收方通过发送 11 的 ACK 来回复 12 和 13，这仍然是它所期望的。
6. 发件人收到 11 的重复 ACK。

各种拥塞避免算法也使用重复 ACK 检测。

# 重传
TCP 检测和重新传输丢失数据包的两种常用机制是

■ 基于计时器的重新传输：当时间已过但尚未收到数据包确认边缘时，会发生这些事件。此时间是 TCP 重传超时，根据连接往返时间 （RTT） 动态计算。在 Linux 上，第一次重传至少需要 200 毫秒 （TCP_RTO_MIN），3 并且后续重传会慢得多，遵循使超时加倍的指数退避算法。
■ 快速重传：当重复的 ACK 到达时，TCP 可以假定数据包已被丢弃并立即重新传输

为了进一步提高性能，已经开发了其他机制来避免基于定时器的重传。出现的一个问题是当最后一个传输的数据包丢失时，并且没有后续数据包触发重复的 ACK 检测。（考虑前面的示例，数据包 13上丢失。这可以通过尾部丢失探测 （TLP） 来解决，它在最后一次传输的短暂超时后发送一个额外的数据包（探测），以帮助检测数据包丢失 [Dukkipati 13]。

拥塞控制算法还可能在存在重传的情况下限制吞吐量

# 拥塞控制
已经开发了拥塞控制算法来保持拥塞网络上的性能。某些操作系统（包括基于 Linux 的操作系统）允许在系统调整过程中选择算法。这些算法包括：

■ Reno：三重 ACK 触发器：拥塞窗口减半、慢启动阈值减半、快速重传、快速恢复。
■ Tahoe：三重重复 ACK 触发器：快速重传、慢启动阈值减半、拥塞窗口设置为一个最大分段大小 （MSS） 和慢启动状态。（与 Reno 一起，Tahoe 最初是为 4.3BSD 开发的。
■ CUBIC：使用 cubic 函数（因此得名）来缩放窗口，并使用 “hybrid start” 函数退出慢速启动。CUBIC 往往比 Reno 更激进，并且是 Linux 中的默认值。
■ BBR：BBR 不是基于窗口的，而是使用探测阶段构建网络路径特性（RTT 和带宽）的显式模型。BBR 可以在某些网络路径上提供显著更好的性能，但会损害其他网络路径的性能。BBRv2 目前正在开发中，并承诺修复 v1 的一些缺陷。
■ DCTCP：数据中心 TCP 依赖于配置为在非常浅的队列占用率处发出显式拥塞通知 （ECN） 标记的交换机，以快速增加到可用带宽 （RFC 8257） [Bensley 17]。这使得 DCTCP 不适合在 Internet 上部署，但在适当配置的受控环境中，它可以显著提高性能。

之前未列出的其他算法包括 Vegas、New Reno 和 Hybla。

拥塞控制算法可能会对网络性能产生很大影响。例如，Netflix 云服务使用 BBR，并发现它可以在严重丢包期间将吞吐量提高三倍 [Ather 17]。在分析 TCP 性能时，了解这些算法在不同网络条件下的反应是一项重要的活动。

2020 年发布的 Linux 5.6 增加了对在 BPF 中开发新的拥塞控制算法的支持 [Corbet 20]。这允许它们由最终用户定义并按需加载。

# Nagle
此算法 （RFC 896） [Nagle 84] 通过延迟小数据包的传输以允许更多数据到达并合并，从而减少网络上小数据包的数量。仅当管道中有数据并且已经遇到延迟时，才会延迟数据包。

系统可能会提供一个可调参数或套接字选项来禁用 Nagle，如果其作与延迟的 ACK 冲突，则可能需要这样做（请参见部分 10.8.2， 套接字选项）。

# 延迟确认
此算法 （RFC 1122） [Braden 89] 将 ACK 的发送延迟最多 500 毫秒，以便可以组合多个 ACK。还可以组合其他 TCP 控制消息，从而减少网络上的数据包数量。

与 Nagle 一样，系统可能会提供可调参数来禁用此行为。

# SACK, FACK, 和 RACK
TCP 选择性确认 （SACK） 算法允许接收方通知发送方它收到了不连续的数据块。否则，数据包丢弃最终会导致整个发送窗口重新传输，以保留顺序确认方案。这会损害 TCP 性能，大多数支持 SACK 的现代操作系统都会避免这种情况。

SACK 已通过前向确认 （FACK） 进行了扩展，默认情况下，Linux 中支持转发确认 （FACK）。FACK 跟踪其他状态并更好地调节网络中未完成数据的数量，从而提高整体性能 [Mathis 96]。

SACK 和 FACK 都用于改进丢包恢复。一种较新的算法，Recent ACKnowledgment（RACK;现在称为 RACK-TLP，并结合了 TLP）使用来自 ACK 的时间信息来获得更好的损失检测和恢复，而不是单独的 ACK 序列 [Cheng 20]。对于 FreeBSD，Netflix 基于 RACK、TLP 和其他功能开发了一个新的重构 TCP 堆栈，称为 RACK [Stewart 18]。

# 初始窗口
初始窗口 （IW） 是 TCP 发送方在连接开始时等待发送方确认之前将传输的数据包数。对于短流（如典型的 HTTP 连接），足够大的 IW 可以跨越传输的数据，从而大大缩短完成时间，从而提高性能。但是，较大的 IW 可能会面临拥塞和丢包的风险。当多个流同时启动时，情况尤其复杂。

Linux 默认值（10 个数据包，又名 IW10）在慢速链路上或许多连接启动时可能太高;其他操作系统默认为 2 或 4 个数据包（IW2 或 IW4）

# UDP
用户数据报协议 （UDP） 是一种常用的 Internet 标准，用于通过网络发送消息（称为数据报）[RFC 768] [Postel 80]。在性能方面，UDP 提供：

■ 简单性：简单而小巧的协议标头减少了计算开销和大小。
■ 无状态：降低连接和传输的开销。
■ No retransmits：这些会显著增加 TCP 连接的延迟。

虽然 UDP 简单且通常性能高，但并不可靠，数据可能会丢失或无序接收。这使得它不适用于多种类型的连接。UDP 也没有避免拥塞的功能，因此可能会导致网络拥塞

某些服务（包括 NFS 版本）可以根据需要配置为在 TCP 或 UDP 上运行。其他执行广播或多播数据的 VPN 可能只能使用 UDP。

UDP 的一个主要用途是 DNS。由于 UDP 的简单性、缺乏拥塞控制和 Internet 支持（通常没有防火墙），现在有基于 UDP 构建的新协议，这些协议实现了自己的拥塞控制和其他功能。一个例子是 QUIC。

# QUIC 和 HTTP/3
QUIC 是由 Google 的 Jim Roskind 设计的一种网络协议，作为 TCP 的高性能、低延迟替代方案，针对 HTTP 和 TLS 进行了优化 [Roskind 12]。QUIC 基于 UDP 构建，并在其基础上提供了多项功能，包括：

■ 能够在同一 “连接” 之上多路复用多个应用程序定义的流。
■ 一种类似于 TCP 的可靠顺序流传输，可以选择性地为单个子流关闭。
■ 当客户端根据连接 ID 的加密身份验证更改其网络地址时，连接恢复。
■ 对负载数据（包括 QUIC 标头）进行完全加密。
■ 0-RTT 连接握手，包括加密（适用于之前已通信的对等体）。

QUIC 被 Chrome 网络浏览器大量使用

虽然 QUIC 最初是由 Google 开发的，但 Internet 工程任务组 （IETF） 正在标准化 QUIC 传输本身，以及使用 HTTP over QUIC 的特定配置（后一种组合称为 HTTP/3）。

# #10.4.2 硬件
网络硬件包括接口、控制器、交换机、路由器和防火墙。了解他们的作是有用的，即使他们由其他员工（网络管理员）管理。

# 接口
物理网络接口在连接的网络上发送和接收消息，称为帧。它们管理所涉及的电气、光学或无线信号，包括传输错误的处理。

接口类型基于第 2 层标准，每种类型都提供最大带宽。带宽更高的接口以更高的成本提供更低的数据传输延迟。在设计新服务器时，一个关键的决策通常是如何平衡服务器的价格和所需的网络性能

对于以太网，选项包括铜缆或光纤，最大速度为 1 Gbit/s （1 GbE）、10 GbE、40 GbE、100 GbE、200 GbE 和 400 GbE。许多供应商都制造以太网接口控制器，但您的操作系统可能没有对其中一些控制器的驱动程序支持

接口利用率可以检查为当前吞吐量除以当前协商带宽。大多数接口都有单独的发送和接收通道，当在全双工模式下运行时，必须单独研究每个通道的利用率。

无线接口可能会因信号强度差和干扰而出现性能问题。

# 控制器
物理网络接口通过控制器提供给系统，控制器内置于系统板中或通过扩展卡提供。

控制器由微处理器驱动，并通过 I/O 传输（例如 PCI）连接到系统。其中任何一个都可以成为网络吞吐量或 IOPS 的限制器

例如，双 10 GbE 网络接口卡连接到四通道 PCI express （PCIe） Gen 2 插槽。该卡的最大发送或接收带宽为 2 × 10 GbE = 20 Gbits/s，双向带宽为 40 Gbit/s。该插槽的最大带宽为 4 × 4 Gbits/s = 16 Gbits/s。因此，两个端口上的网络吞吐量将受到 PCIe Gen 2 带宽的限制，并且不可能同时以线速驱动它们（我从实践中知道这一点！）

# 交换机和路由器
交换机在任意两个连接的主机之间提供专用的通信路径，允许在主机对之间进行多次传输而不会产生干扰。这项技术取代了集线器（在此之前，共享物理总线：常用的粗以太网同轴电缆），它与所有主机共享所有数据包。当主机同时传输时，这种共享会导致争用，接口使用“载波侦听多址与冲突检测”（CSMA/CD） 算法将其识别为冲突。此算法将以指数方式回退并重新传输，直到成功，从而在负载下产生性能问题。随着 switch 的使用，这已经过去了，但一些可观察性工具仍然有冲突计数器——即使这些计数器通常只是由于错误（协商或布线错误）而发生的。

路由器在网络之间传递数据包，并使用网络协议和路由表来确定有效的传递路径。在两个城市之间传输数据包可能涉及十几台或更多路由器，以及其他网络硬件。路由器和路由通常配置为动态更新，以便网络可以自动响应网络和路由器的中断时间，并平衡负载。这意味着在给定的时间点，没有人可以确定数据包实际采用的路径。在可能有多个路径的情况下，数据包也有可能无序传送，这可能会导致 TCP 性能问题。

网络上的这种神秘元素通常被归咎于性能不佳：也许来自其他不相关主机的大量网络流量使源和目标之间的路由器饱和？因此，网络管理团队经常需要免除其基础设施的责任。他们可以使用先进的实时监控工具来检查所有涉及的路由器和其他网络组件。

路由器和交换机都包括缓冲区和微处理器，它们本身在负载下可能会成为性能瓶颈。举个极端的例子，我曾经发现，由于 CPU 容量有限，早期的 10 GbE 交换机在所有端口上总共只能驱动不超过 11 Gbits/s 的速度。

请注意，交换机和路由器也经常是发生速率转换的地方（从一个带宽切换到另一个带宽，例如，10 Gbps 链路转换为 1 Gbps 链路）。发生这种情况时，需要一些缓冲以避免过多的 droping，但许多交换机和路由器会过度缓冲（参见 第 10.3.6 节 缓冲 中的 bufferbloat 问题），导致高延迟。更好的队列管理算法可以帮助消除此问题，但并非所有网络设备供应商都支持它们。在源处调整步调还可以通过降低流量的突发性来缓解速率转换问题。


# 防火墙
防火墙通常用于仅允许基于配置的规则集的授权通信，从而提高网络的安全性。它们可能同时作为物理网络设备和内核软件存在。

防火墙可能会成为性能瓶颈，尤其是在配置为有状态时。有状态规则存储每个看到的连接的元数据，防火墙在处理许多连接时可能会遇到过多的内存负载。这可能是由于拒绝服务 （DoS） 攻击试图用连接淹没目标而发生的。出站连接率较高时也可能发生这种情况，因为它们可能需要类似的连接跟踪。

由于防火墙是自定义硬件或软件，因此可用于分析它们的工具取决于每个防火墙产品。请参阅各自的文档

由于其性能、可编程性、易用性和最终成本，使用扩展 BPF 在商用硬件上实施防火墙的情况正在增长。采用 BPF 防火墙和 DDoS 解决方案的公司包括 Facebook [Deepak 18]、Cloudflare [Majkowski 18] 和 Cilium [Cilium 20a]

在性能测试期间，防火墙也可能令人讨厌：在调试问题时执行带宽测试可能涉及修改防火墙规则以允许连接（并与安全团队协调）。

# 其他
您的环境可能包括其他物理网络设备，例如集线器、网桥、中继器和调制解调器。其中任何一个都可能是性能瓶颈和丢包的根源

# #10.4.3 软件
联网软件包括网络堆栈、TCP 和设备驱动程序。本节讨论了与性能相关的主题。

# 网络堆栈
涉及的组件和层取决于所使用的操作系统类型、版本、协议和接口。图 10.7 描述了一个通用模型，显示了软件组件

在现代内核上，堆栈是多线程的，入站数据包可以由多个 CPU 处理

# Linux
Linux 网络堆栈如图 10.8 所示，包括套接字发送/接收缓冲区和数据包队列的位置。

在 Linux 系统上，网络堆栈是核心内核组件，设备驱动程序是附加模块。数据包作为 struct sk_buff （socket buffer） 数据类型通过这些内核组件传递。请注意，IP 层中也可能有排队（未图示）用于数据包重组。

以下部分讨论与性能相关的 Linux 实现详细信息：TCP 连接队列、TCP 缓冲、排队规则、网络设备驱动程序、CPU 扩展和内核旁路。TCP 协议在上一节中进行了介绍。

# TCP 连接队列
入站连接的突增通过使用积压队列来处理。有两个这样的队列，一个用于 TCP 握手完成时的不完整连接（也称为 SYN 积压），另一个用于等待应用程序接受的已建立会话（也称为侦听积压）。这些如图 10.9 所示。

在早期的内核中只使用了一个队列，并且容易受到 SYN 泛洪的影响。SYN 泛洪是一种 DoS 攻击，涉及从伪造的 IP 地址向侦听 TCP 端口发送大量 SYN。这会在 TCP 等待完成握手时填充积压队列，从而阻止真实客户端连接。

对于两个队列，第一个队列可以充当潜在虚假连接的暂存区域，只有在建立连接后，这些连接才会提升到第二个队列。第一个队列可以设置得很长，以吸收 SYN 泛洪，并优化以仅存储所需的最小元数据量

使用 SYN Cookie 会绕过第一个队列，因为它们表明客户端已获得授权。

这些队列的长度可以独立调整（请参见部分 10.8， 调整）。第二个也可以由应用程序设置为 listen（2） 的 backlog 参数。

# TCP 缓冲
通过使用与套接字关联的发送和接收缓冲区，可以提高数据吞吐量。这些如图 10.10 所示

发送和接收缓冲区的大小都是可调的。更大的大小可以提高每个性能的吞吐量，但代价是每个连接花费更多的主内存。如果预计服务器要执行更多的发送或接收，则可以将一个缓冲区设置为大于另一个缓冲区。Linux 内核还将根据连接活动动态增加这些缓冲区的大小，并允许调整它们的最小、默认和最大大小。

# 分段卸载：GSO 和 TSO
网络设备和网络接受最大分段大小 （MSS） 的数据包大小，最大分段大小 （MSS） 可能小至 1500 字节。为了避免发送许多小数据包的网络堆栈开销，Linux 使用通用分段卸载 （GSO） 来发送大小高达 64 KB 的数据包（“超级数据包”），在传送到网络设备之前被拆分为 MSS 大小的段。如果 NIC 和驱动程序支持 TCP 分段卸载 （TSO），则 GSO 将拆分留给设备，从而提高网络堆栈吞吐量。5 GSO [Linux 20i]还有一个通用的接收卸载 （GRO） 补充。GRO 和 GSO 在内核软件中实现，TSO 由 NIC 硬件实现。

# 排队规则
这是一个可选层，用于管理网络数据包的流量分类 （tc）、调度、操作、筛选和整形。Linux 提供了许多排队规则算法 （qdiscs），可以使用 tc（8） 命令进行配置。由于每个都有一个手册页，因此可以使用 man（1） 命令列出它们：

# man -k tc
tc-actions (8)       - independently defined actions in tc
 tc-basic (8)         - basic traffic control filter
 tc-bfifo (8)         - Packet limited First In, First Out queue
 tc-bpf (8)           - BPF programmable classifier and actions for ingress/egress 
queueing disciplines
 tc-cbq (8)           - Class Based Queueing
 tc-cbq-details (8)   - Class Based Queueing
 tc-cbs (8)           - Credit Based Shaper (CBS) Qdisc
 tc-cgroup (8)        - control group based traffic control filter
 tc-choke (8)         - choose and keep scheduler
 tc-codel (8)         - Controlled-Delay Active Queue Management algorithm
 tc-connmark (8)      - netfilter connmark retriever action
 tc-csum (8)          - checksum update action
 tc-drr (8)           - deficit round robin scheduler
 tc-ematch (8)        - extended matches for use with "basic" or "flow" filters
 tc-flow (8)          - flow based traffic control filter
 tc-flower (8)        - flow based traffic control filter
 tc-fq (8)            - Fair Queue traffic policing
 tc-fq_codel (8)      - Fair Queuing (FQ) with Controlled Delay (CoDel)
 [...] 

Linux 内核将 pfifo_fast 设置为默认 qdisc，而 systemd 则不那么保守，并将其设置为 fq_codel 以减少潜在的缓冲区膨胀，但代价是 qdisc 层的复杂性略高。

BPF 可以通过 BPF_PROG_TYPE_ SCHED_CLS 和 BPF_PROG_TYPE_SCHED_ACT 类型的程序来增强该层的功能。这些 BPF 程序可以附加到内核入口和出口点，用于数据包过滤、修改和转发，就像负载平衡器和防火墙使用的那样

# 网络设备驱动程序
网络设备驱动程序通常有一个额外的缓冲区（环形缓冲区），用于在内核内存和 NIC 之间发送和接收数据包。这在图 10.8 中被描绘为驱动程序队列。

在高速网络中，一个越来越常见的性能特性是使用中断合并模式。不是为每个到达的数据包中断内核，而是仅在达到计时器 （轮询） 或一定数量的数据包时发送中断。这会降低内核与 NIC 通信的速率，从而允许进行更大的传输，从而提高吞吐量，但会以一定的延迟为代价。

Linux 内核使用一个新的 API （NAPI） 框架，该框架使用中断缓解技术：对于低数据包速率，使用中断（通过 softirq 调度处理）;对于高数据包速率，将禁用中断，并使用轮询来允许合并 [Corbet 03][Corbet 06b]。这提供了低延迟或高吞吐量，具体取决于工作负载。NAPI 的其他功能包括：

■ 数据包限制，允许在网络适配器中提前丢弃数据包，以防止系统被数据包风暴淹没。
■ 接口调度，其中配额用于限制轮询周期中处理的缓冲区，以确保繁忙网络接口之间的公平性。
■ 支持 SO_BUSY_POLL 套接字选项，其中用户级应用程序可以通过请求套接字上的忙等待（在 CPU 上旋转，直到事件发生）来减少网络接收延迟 [Dumazet 17a]

合并对于改进虚拟机网络尤其重要，AWS EC2 使用的 ena 网络驱动程序使用合并

# NIC 发送和接收
对于发送的数据包，NIC 会收到通知，并且通常使用直接内存访问 （DMA） 从内核内存中读取数据包（帧）以提高效率。NIC 提供用于管理 DMA 数据包的传输描述符;如果 NIC 没有 free 描述符，则网络堆栈将暂停传输以允许 NIC 赶上。

对于收到的数据包，NIC 可以使用 DMA 将数据包放入内核环形缓冲区内存，然后使用中断通知内核（可以忽略该中断以允许合并）。interrupt 触发 softirq 以将数据包传送到网络堆栈以进行进一步处理。

# CPU 扩展
通过让多个 CPU 处理数据包和 TCP/IP 堆栈，可以实现高数据包速率。Linux 支持各种多 CPU 数据包处理方法（参见 Documentation/ networking/scaling.txt）

■ RSS：接收方扩展：适用于支持多个队列并且可以将数据包散列到不同队列的现代 NIC，这些队列又由不同的 CPU 处理，从而中断他们直接。此哈希可能基于 IP 地址和 TCP 端口号，因此来自同一连接的数据包最终由同一 CPU 处理
■ RPS： Receive Packet Steering： 接收数据包转向：RSS 的软件实现，适用于不支持多个队列的 NIC。这涉及一个简短的中断服务程序，用于将入站数据包映射到 CPU 进行处理。类似的哈希可用于根据数据包标头中的字段将数据包映射到 CPU。
■ RFS：Receive Flow Steering：这与 RPS 类似，但与上次在 CPU 上处理套接字的位置具有相关性，以提高 CPU 缓存命中率和内存位置。
■ 加速接收流转向：这将在硬件中为支持此功能的 NIC 实现 RFS。它涉及使用流信息更新 NIC，以便它可以确定要中断的 CPU
■ XPS：传输数据包转向：对于具有多个传输队列的 NIC，这支持由多个 CPU 传输到队列。

如果没有针对网络数据包的 CPU 负载均衡策略，NIC 可能只会中断一个 CPU，这可能会达到 100% 的利用率并成为瓶颈。这可能表现为单个 CPU 上的高 softirq CPU 时间（例如，使用 Linux mpstat（1）：参见第 6 章 CPU， 第 6.6.3 节 mpstat）。对于负载均衡器或代理服务器（例如 nginx），这可能尤其如此，因为它们的预期工作负载是高入站数据包率。

根据缓存一致性等因素将中断映射到 CPU，就像 RFS 所做的那样，可以显著提高网络性能。这也可以通过 irqbalance 进程来完成，该进程将中断请求 （IRQ） 行分配给 CPU

# 内核旁路
图 10.8 显示了通过 TCP/IP 堆栈的最常见路径。应用程序可以使用数据平面开发套件 （DPDK） 等技术绕过内核网络堆栈，以实现更高的数据包速率和性能。这涉及应用程序在用户空间中实现自己的网络协议，并通过 DPDK 库和内核用户空间 I/O （UIO） 或虚拟功能 I/O （VFIO） 驱动程序写入网络驱动程序。通过直接访问 NIC 上的内存，可以避免复制数据包数据的费用。

eXpress 数据路径 （XDP） 技术为网络数据包提供了另一条路径：一个使用扩展 BPF 的程序 mable 快速路径，它集成到现有的内核堆栈中，而不是绕过它 [Høiland-Jørgensen 18]。（DPDK 现在支持用于接收数据包的 XDP，将一些功能移回内核 [DPDK 20]。

使用内核网络堆栈旁路时，使用传统工具和指标的插桩不可用，因为它们使用的计数器和跟踪事件也被绕过。这使得性能分析更加困难。

除了全栈旁路之外，还有一些功能可以避免复制数据的费用：MSG_ZEROCOPY send（2） 标志，以及通过 mmap（2） [Linux 20c][Corbet 18b] 进行零拷贝接收。

# 其他优化
在整个 Linux 网络堆栈中，还有其他算法用于提高性能。图 10.11 显示了 TCP 发送路径的这些（其中许多是从 tcp_write_ xmit（） 内核函数调用的）。

其中一些组件和算法在前面已经描述过（套接字发送缓冲区、TSO、9 拥塞控制、Nagle 和 qdiscs）;其他包括：

■ Pacing：控制何时发送数据包，分散传输（节奏）以避免可能损害性能的突发（这可能有助于避免可能导致排队延迟，甚至导致网络交换机丢弃数据包的 TCP 微突发）。当多个端点同时传输到一个端点时，它还可能有助于解决 incast 问题 [Fritchie 12]）。
■ TCP 小队列 （TSQ）：这控制（减少）网络堆栈排队的数量，以避免包括缓冲区膨胀 [Bufferbloat 20] 在内的问题。
■ 字节队列限制 （BQL）：这些限制会自动调整驱动程序队列的大小，使其足够大以避免匮乏，但也足够小以减少排队数据包的最大延迟，并避免耗尽 NIC TX 描述符 [Hrubý 12]。它的工作原理是在必要时暂停将数据包添加到驱动程序队列中，并在 Linux 3.3 [Siemon 13] 中添加。
■ 最早离开时间 （EDT）：它使用计时轮而不是队列对发送到 NIC 的数据包进行排序。时间戳是根据策略和速率配置在每个数据包上设置的。这是在 Linux 4.20 中添加的，具有类似 BQL 和 TSQ 的功能 [Jacobson 18]。

这些算法通常结合使用以提高性能。TCP 发送的数据包在到达 NIC 之前，可以由任何拥塞控制、TSO、TSQ、起搏和排队规则处理 [Cheng 16]。

# 10.5 方法论
本节介绍网络分析和调整的方法和练习。表 10.2 总结了这些主题。

参见第 2 章 方法论，了解更多策略和其中许多策略的介绍。

这些可以单独遵循或组合使用。我的建议是按以下顺序使用以下策略：性能监控、USE 方法、静态性能调整和工作负载特征。

第 10.6 节 可观测性工具介绍了用于应用这些方法的作系统工具。

# #10.5.1 工具方法
tools 方法是迭代可用工具的过程，检查它们提供的关键指标。它可能会忽略工具提供较差或没有可见性的问题，并且执行起来可能很耗时。

对于网络，工具方法可能涉及检查：

■ nstat/netstat -s：查找高速率的重传和无序数据包。构成“高”重传速率的因素取决于客户端：具有不可靠远程客户端的面向 Internet 的系统应该比客户端位于同一数据中心的内部系统具有更高的重传速率。
■ ip -s link/netstat -i： 检查接口错误计数器，包括 “errors”、“dropped”、“overruns”。ss -tiepm： 检查重要套接字的限制器标志，看看它们的瓶颈是什么，以及其他显示套接字健康状况的统计数据。
■ nicstat/ip -s link：检查发送和接收的字节速率。高吞吐量可能会受到协商的数据链路速度或外部网络限制的限制。它还可能导致系统上的网络用户之间发生争用和延迟。
■ tcplife：记录 TCP 会话，包括进程详细信息、持续时间（生命周期）和吞吐量统计信息。
■ tcptop：实时观看排名靠前的 TCP 会话。
■ tcpdump：虽然就 CPU 和存储成本而言，使用起来可能很昂贵，但短期使用 tcpdump（8） 可能会帮助你识别异常的网络流量或协议头。
■ perf（1）/BCC/bpftrace：检查应用程序和线路之间的选定数据包，包括检查内核状态。

如果发现问题，请检查可用工具中的所有字段以了解更多上下文。有关每个工具的更多信息，请参见部分 10.6， 可观测性工具 。其他方法可以识别更多类型的问题

# #10.5.2 USE方法
USE 方法用于快速识别所有组件中的瓶颈和错误。对于每个网络接口和每个方向（传输 （TX） 和接收 （RX），请检查：

■ 利用率：接口忙于发送或接收帧的时间 
■ 饱和度：由于接口充分利用而导致额外排队、缓冲或阻塞的程度 
■ 错误： 对于接收：校验和错误、帧太短（小于数据链路报头）或太长、冲突（不太可能与交换网络一起）;对于传输：延迟冲突（接线错误）

可以先检查错误，因为它们通常检查速度快且最容易解释

利用率通常不由操作系统或监控工具直接提供（nicstat（1） 是一个例外）。它可以计算为每个方向（RX、TX）的当前吞吐量除以当前协商速度。当前吞吐量应以网络上的每秒字节数来衡量，包括所有协议标头。

对于实施网络带宽限制（资源控制）的环境（如某些云计算环境中的情况），除了物理限制之外，可能还需要根据施加的限制来衡量网络利用率。

网络接口的饱和度很难测量。某些网络缓冲是正常的，因为应用程序发送数据的速度比接口传输数据的速度要快得多。可以测量应用程序线程在网络发送上受阻所花费的时间，该时间应该会随着饱和度的增加而增加。还要检查是否有其他与接口饱和度更密切相关的内核统计数据，例如 Linux“overruns”。请注意，Linux 使用 BQL 来调节 NIC 队列大小，这有助于避免 NIC 饱和。

TCP 级别的重传通常很容易作为统计数据获得，并且可以作为网络饱和的指标。但是，它们是在服务器及其客户端之间的网络中测量的，并且可能是由任何跃点的问题引起的。

USE 方法还可以应用于网络控制器，以及它们与处理器之间的传输。由于这些组件的可观测性工具很少，因此根据网络接口统计信息和拓扑推断指标可能更容易。例如，如果 网络控制器 A 包含端口 A0 和 A1，网络控制器吞吐量可以计算为接口吞吐量 A0 A1 的总和。使用已知的最大吞吐量，可以计算网络控制器的利用率

# #10.5.3 工作负载特征描述
在容量规划、基准测试和模拟工作负载时，描述应用的负载是一项重要的练习。它还可以通过识别可以消除的不必要工作来带来一些最大的性能提升

以下是要测量的最基本特征：
■ 网络接口吞吐量：RX 和 TX，每秒字节数 
■ 网络接口 IOPS：RX 和 TX，每秒帧数 TCP 
■ 连接速率：主动和被动，每秒连接数

术语 主动 和 被动 在第 10.4.1 节 协议的 三次握手 部分中进行了介绍。

这些特征可能会随时间而变化，因为使用模式在一天中会发生变化。部分 10.5.5，性能监控中介绍了随时间推移的监控

下面是一个示例工作负载描述，用于说明如何一起表示这些属性

网络吞吐量因用户而异，并且执行的写入 （TX） 多于读取 （RX）。峰值写入速率为 200 MB/s 和 210,000 个数据包/s，峰值读取速率为 10 MB/s，数据包/s 为 70,000 个数据包/s。入站（被动）TCP 连接速率达到 3000 个连接/秒。

除了在系统范围内描述这些特征外，它们还可以按界面表示。如果可以观察到吞吐量已达到线速，则可以确定接口瓶颈。如果存在网络带宽限制（资源控制），它们可能会在达到线速之前限制网络吞吐量。

# 高级工作负载特征描述/核对表
可能包含其他详细信息来描述工作负载的特征。这些问题已在此处列为供考虑的问题，在彻底研究 CPU 问题时，也可以作为检查表：

■ 平均数据包大小是多少？RX，TX？
■ 每一层的协议细分是什么？
■ 对于传输协议：TCP、UDP（可以包括 QUIC）。
■ 哪些 TCP/UDP 端口处于活动状态？每秒字节数，每秒连接数？
■ 广播和组播数据包的速率是多少？
■ 哪些进程正在积极使用网络？

以下各节回答了其中的一些问题。有关此方法和要测量的特征（谁、为什么、什么、如何）的更高级别摘要，请参阅第 2 章 “方法”

# #10.5.4 延迟分析
可以研究不同的时间（延迟）来帮助理解和表达网络性能。一些是在 Section 10.3.5， Latency 中介绍的，Table 10.3 中提供了更长的列表。尽可能多地测量这些指标，以缩小延迟的真正来源。

延迟可能表现为：

■ 每个间隔的平均值：最好按客户端/服务器对执行，以隔离中间网络中的差异 
■ 完整分布：作为直方图或热图
■ 每次操作延迟：列出每个事件的详细信息，包括源和目标 IP 地址

问题的一个常见来源是 TCP 重新传输导致的延迟异常值。这些可以使用完整分配或按作延迟跟踪来识别，包括通过筛选最小延迟阈值。

可以使用跟踪工具测量延迟，对于某些延迟，还可以使用套接字选项进行测量。在 Linux 上，套接字选项包括 SO_TIMESTAMP 用于传入数据包时间（以及用于纳秒分辨率的 SO_TIMESTAMPNS）和用于每个事件时间戳的 SO_TIMESTAMPING [Linux 20j]。SO_TIMESTAMPING 可以识别传输延迟、网络往返时间和堆栈间延迟;这在分析涉及隧道的复杂数据包延迟 [Hassas Yeganeh 19] 时特别有用。

请注意，某些额外延迟源是暂时性的，仅在系统加载期间发生。要更真实地测量网络延迟，不仅要测量空闲系统，还要测量负载下的系统，这一点很重要。

# #10.5.5 性能监控
性能监控可以识别一段时间内的活动问题和行为模式，包括最终用户的日常模式，以及包括网络备份在内的计划活动。

网络监控的关键指标

■ 吞吐量：每秒接收和传输的网络接口字节数，理想情况下对于每个接口 
■ 连接数：每秒 TCP 连接数，作为网络负载的另一个指示 
■ 错误：包括丢弃的数据包计数器 
■ TCP 重新传输：也可用于记录与网络问题的相关性 
■ TCP 乱序数据包：也可能导致性能问题

对于实施网络带宽限制（资源控制）的环境，如在某些云计算环境中，也可能收集与施加的限制相关的统计信息。

# #10.5.6 数据包嗅探
数据包嗅探（又名数据包捕获）涉及从网络捕获数据包，以便可以逐个数据包检查其协议标头和数据。对于观察性分析，这可能是最后的手段，因为在 CPU 和存储开销方面执行它可能很昂贵。网络内核代码路径通常是周期优化的，因为它们每秒需要处理多达数百万个数据包，并且对任何额外的开销都很敏感。为了减少这种开销，内核可以使用环形缓冲区通过共享内存映射将数据包数据传递给用户级跟踪工具——例如，10 使用 BPF 和 perf（1） 的输出环形缓冲区，以及使用 AF_XDP [Linux 20k]。解决开销的另一种方法是使用带外数据包嗅探器：连接到交换机的“分路”或“镜像”端口的单独服务器。Amazon 和 Google 等公共云提供商将此作为一项服务 [Amazon 19][Google 20b] 提供

数据包嗅探通常涉及将数据包捕获到文件中，然后以不同的方式分析该文件。一种方法是生成一个日志，该日志可以包含每个数据包的以下内容

■ 时间戳 
■ 整个数据包，包括 
    所有协议报头（例如以太网、IP、TCP） 
    部分或全部有效载荷数据 
■ 元数据：数据包数、丢弃数 
■ 接口名称

作为数据包捕获的示例，下面显示了 Linux tcpdump（8） 工具的默认输出：

 # tcpdump -ni eth4
 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
 listening on eth4, link-type EN10MB (Ethernet), capture size 65535 bytes
 01:20:46.769073 IP 10.2.203.2.22 > 10.2.0.2.33771: Flags [P.], seq 
4235343542:4235343734, ack 4053030377, win 132, options [nop,nop,TS val 328647671 ecr 
2313764364], length 192
 01:20:46.769470 IP 10.2.0.2.33771 > 10.2.203.2.22: Flags [.], ack 192, win 501, 
options [nop,nop,TS val 2313764392 ecr 328647671], length 0
 01:20:46.787673 IP 10.2.203.2.22 > 10.2.0.2.33771: Flags [P.], seq 192:560, ack 1, 
win 132, options [nop,nop,TS val 328647672 ecr 2313764392], length 368
 01:20:46.788050 IP 10.2.0.2.33771 > 10.2.203.2.22: Flags [.], ack 560, win 501, 
options [nop,nop,TS val 2313764394 ecr 328647672], length 0
 01:20:46.808491 IP 10.2.203.2.22 > 10.2.0.2.33771: Flags [P.], seq 560:896, ack 1, 
win 132, options [nop,nop,TS val 328647674 ecr 2313764394], length 336
 [...]

此输出包含一行，汇总了每个数据包，包括 IP 地址、TCP 端口和其他 TCP 报头详细信息的详细信息。这可用于调试各种问题，包括消息延迟和数据包丢失。

由于数据包捕获可能是一项 CPU 成本高昂的活动，因此大多数实施都包含在过载时丢弃事件而不是捕获事件的功能。日志中可能包含丢弃的数据包计数。

除了使用环形缓冲区来减少开销外，数据包捕获实现通常还允许用户提供过滤表达式并在内核中执行此过滤。这通过不将不需要的数据包传输到用户级别来减少开销。过滤器表达式通常使用 Berkeley 数据包过滤器 （BPF） 进行优化，它将表达式编译为 BPF 字节码，该字节码可由内核 JIT 编译为机器码。近年来，BPF 已在 Linux 中扩展为通用执行环境，为许多可观测性工具提供支持：请参见第 3 章作系统、第 3.4.4 节 扩展 BPF 和第 15 章 BPF。

# #10.5.7 TCP分析
除了部分 10.5.4， 延迟分析中介绍的内容外，还可以研究其他特定的 TCP 行为，包括：

■ TCP（套接字）发送/接收缓冲区的使用 
■ TCP 积压队列的使用 
■ 由于积压队列已满而导致内核丢弃 
■ 拥塞窗口大小，包括零大小的通告 
■ 在 TCP TIME_WAIT间隔期间收到的 SYN

当服务器使用相同的源 IP 地址和目标 IP 地址频繁连接到同一目标端口上的另一个服务器时，最后一种行为可能会成为可伸缩性问题。每个连接的唯一区别因素是客户端源端口（临时端口），对于 TCP 来说，该端口是 16 位值，并且可能进一步受到操作系统参数（最小值和最大值）的限制。结合 TCP TIME_WAIT间隔（可能是 60 秒），高连接速率（60 秒内超过 65,536 个）可能会遇到新连接冲突。在这种情况下，当该临时端口仍与TIME_WAIT中的先前 TCP 会话相关联时，会发送 SYN，如果新 SYN 被错误地识别为旧连接的一部分（冲突），则可能会被拒绝。为避免此问题，Linux 内核会尝试快速重用或回收连接（通常效果很好）。服务器使用多个 IP 地址是另一种可能的解决方案，延迟时间较短的 SO_LINGER 套接字选项也是如此。

# #10.5.8 静态性能调优
静态性能优化侧重于已配置环境的问题。对于网络性能，请检查静态配置的以下方面：

■ 有多少个网络接口可供使用？目前正在使用中吗？
■ 网络接口的最大速度是多少？
■ 当前协商的网络接口速度是多少？
■ 网络接口是协商为半双工还是全双工？
■ 为网络接口配置了什么MTU？
■ 网络接口是否中继？
■ 设备驱动程序存在哪些可调参数？IP 层？TCP 层？
■ 是否有任何可调参数与默认值不同？
■ 路由是如何配置的？什么是默认网关？
■ 数据路径中网络组件（所有组件，包括交换机和路由器背板）的最大吞吐量是多少？
■ 数据路径的最大 MTU 是多少，是否发生碎片？
■ 数据路径中是否有任何无线连接？他们是否受到干扰？
■ 是否启用了转发？系统是否充当路由器？
■ DNS 是如何配置的？服务器有多远？
■ 网络接口固件的版本或任何其他网络硬件是否存在已知的性能问题（错误）？
■ 网络设备驱动程序是否存在已知的性能问题（错误）？
■ 内核 TCP/IP 堆栈？
■ 存在哪些防火墙？是否存在软件施加的网络吞吐量限制（资源控制）？它们是什么？

这些问题的答案可能会揭示被忽视的配置选择

最后一个问题与云计算环境尤其相关，因为云计算环境可能会对网络吞吐量施加限制。

# #10.5.9 资源控制
操作系统可能会提供控制来限制连接、进程或进程组类型的网络资源。这些控件可以包括以下类型的控件：

■ 网络带宽限制：内核应用的不同协议或应用程序允许的带宽（最大吞吐量）。
■ IP 服务质量 （QoS）：由网络组件（例如路由器）执行的网络流量的优先级排序。这可以通过不同的方式实现： IP 报头包括服务类型 （ToS） 位，包括优先级;此后，这些位已针对较新的 QoS 方案重新定义，包括差分服务（请参见部分 10.4.1 协议，在 IP 标题下）。出于相同的目的，其他协议制定者可能还实施了其他优先级。
■ 数据包延迟：额外的数据包延迟（例如，使用 Linux tc-netem（8）），可用于在测试性能时模拟其他网络

您的网络可能具有可分类为低优先级或高优先级的混合流量。低优先级可能包括备份传输和性能监控流量。高优先级可能是生产服务器和客户端之间的流量。任一资源控制方案都可用于限制低优先级流量，从而为高优先级流量产生更有利的性能。

这些工作如何特定于实现：请参见部分 10.8， 调整。

# #10.5.10 微基准测试
有许多用于网络的基准测试工具。在调查分布式应用程序环境的吞吐量问题时，它们特别有用，可以确认网络至少可以达到预期的网络吞吐量。如果不能，可以通过网络微基准测试工具来测试网络性能，该工具通常比应用程序复杂得多，调试速度也快得多。将网络调整到所需的速度后，注意力可以返回到应用程序上。

可以测试的典型因素包括

■ 方向：发送或接收 
■ 协议：TCP 或 UDP，以及端口
■ 线程数 
■ 缓冲区大小
■ 接口 MTU 大小

更快的网络接口（如 100 Gbits/s）可能需要将多个客户端线程驱动到最大带宽

第 10.7.4 节 iperf 中介绍了一个示例网络微基准测试工具 iperf（1），其他工具在第 10.7 节 实验中列出。

# 10.6 观测工具
本节介绍适用于基于 Linux 的作系统的网络性能可观测性工具。有关使用它们时要遵循的策略，请参阅上一节。

表 10.4 中列出了本节中的工具

这是支持第 10.5 节 方法的一系列工具和功能，从传统工具和统计开始，然后是跟踪工具，最后是数据包捕获工具。一些传统工具可能在其他它们的类 Unix作系统上可用，包括：ifconfig（8）、netstat（8） 和 sar（1）。跟踪工具基于 BPF，并使用 BCC 和 bpftrace 前端（第 15 章）;它们是：Socketio（8）、tcplife（8）、tcptop（8） 和 tcpretrans（8）

首先介绍的统计工具是 ss（8）、ip（8） 和 nstat（8），因为它们来自由网络内核工程师维护的 iproute2 包。此软件包中的工具最有可能支持最新的 Linux 内核功能。net-tools 包中的类似工具，即 ifconfig（8） 和 netstat（8），也被涵盖在内，因为它们被广泛使用，尽管 Linux 内核网络工程师认为这些工具已弃用。

# #10.6.1 ss
ss（8） 是一个 socket 统计工具，它总结了开放的 sockets。默认输出提供有关套接字的高级信息，例如

# ss
 Netid State     Recv-Q  Send-Q    Local Address:Port      Peer Address:Port 
[...]
 tcp   ESTAB     0       0         100.85.142.69:65264    100.82.166.11:6001 
tcp   ESTAB     0       0         100.85.142.69:6028     100.82.16.200:6101 
[...]

此输出是当前状态的快照。第一列显示套接字使用的协议：这些是 TCP。由于此输出列出了所有已建立的连接以及 IP 地址信息，因此它可用于描述当前工作负载的特征，并回答包括打开的客户端连接数、依赖项服务的并发连接数等问题

类似的每个套接字信息可以使用较旧的 netstat（8） 工具。然而，ss（8） 在使用选项时可以显示更多信息。例如，仅显示 TCP 套接字 （-t），其中包含 TCP 内部信息 （-i）、扩展套接字信息 （-e）、进程信息 （-p） 和内存使用情况 （-m）

# ss -tiepm
 State     Recv-Q  Send-Q    Local Address:Port      Peer Address:Port              
ESTAB     0       0         
100.85.142.69:65264     
100.82.166.11:6001   
 users:(("java",pid=4195,fd=10865)) uid:33 ino:2009918 sk:78 <->
         skmem:(r0,rb12582912,t0,tb12582912,f266240,w0,o0,bl0,d0) ts sack bbr ws
 cale:9,9 rto:204 rtt:0.159/0.009 ato:40 mss:1448 pmtu:1500 rcvmss:1448 advmss:14
 48 cwnd:152 bytes_acked:347681 bytes_received:1798733 segs_out:582 segs_in:1397 
data_segs_out:294 data_segs_in:1318 bbr:(bw:328.6Mbps,mrtt:0.149,pacing_gain:2.8
 8672,cwnd_gain:2.88672) send 11074.0Mbps lastsnd:1696 lastrcv:1660 lastack:1660 
pacing_rate 2422.4Mbps delivery_rate 328.6Mbps app_limited busy:16ms rcv_rtt:39.
 822 rcv_space:84867 rcv_ssthresh:3609062 minrtt:0.139
 [...]

以粗体突出显示的是终端节点地址和以下详细信息：

■ “java”，pid=4195：进程名称“java”，PID 4195。
■ fd=10865：文件描述符 10865（用于 PID 4195）。
■ rto：204： TCP 重传超时：204 毫秒。
■ rtt：0.159/0.009： 平均往返时间为 0.159 毫秒，平均偏差为 0.009 毫秒。
■ mss：1448：最大段大小：1448 字节。
■ cwnd：152：拥塞窗口大小：152 × MSS。
■ bytes_acked：347681：成功传输 340 KB。
■ bytes_received：1798733：收到 1.72 MB。
■ bbr：...： BBR 拥塞控制统计数据。
■ pacing_rate 2422.4Mbps：步调速率为 2422.4 Mbps. 
■ app_limited：显示拥塞窗口未充分利用，表明连接受应用程序限制。
■ minrtt：0.139：最小往返时间（以毫秒为单位）。与平均值和平均值偏差（前面列出）进行比较，以了解网络变化和拥塞情况。

此特定连接被标记为应用程序受限 （app_limited），到远程端点的 RTT 较低，传输的总字节数较低。ss（1） 可以打印的 “limited” 标志包括

■ app_limited：应用程序受限。
■ rwnd_limited：Xms：受接收窗口限制。包括以毫秒为单位的时间限制。
■ sndbuf_limited：Xms：受发送缓冲区限制。包括以毫秒为单位的时间限制。

输出中缺少一个详细信息是连接使用年限，这是计算平均吞吐量所必需的。我找到的一个解决方法是在 /proc 中的文件描述符文件上使用更改时间戳：对于此连接，我会在 /proc/4195/fd/10865 上运行 stat（1）

# netlink
ss（8） 从 netlink（7） 接口读取这些扩展的详细信息，该接口通过 family AF_NETLINK 的套接字作，以从内核中获取信息。您可以使用 strace（1） 在实际作中看到这一点（有关 strace（1） 开销的警告，请参见第 5 章 应用程序， 第 5.5.4 节 strace）

# strace -e sendmsg,recvmsg ss -t
 sendmsg(3, {msg_name={sa_family=AF_NETLINK, nl_pid=0, nl_groups=00000000}, 
msg_namelen=12, msg_iov=[{iov_base={{len=72, type=SOCK_DIAG_BY_FAMILY, 
flags=NLM_F_REQUEST|NLM_F_DUMP, seq=123456, pid=0}, {sdiag_family=AF_INET, 
sdiag_protocol=IPPROTO_TCP, idiag_ext=1<<(INET_DIAG_MEMINFO-1)|...
 recvmsg(3, {msg_name={sa_family=AF_NETLINK, nl_pid=0, nl_groups=00000000},...
 [...]

netstat（8） 使用 /proc/net 文件来获取信息

# strace -e openat netstat -an
 [...]
 openat(AT_FDCWD, "/proc/net/tcp", O_RDONLY) = 3
 openat(AT_FDCWD, "/proc/net/tcp6", O_RDONLY) = 3
 [...]

因为 /proc/net 文件是文本，所以我发现它们作为临时报告的来源很方便，只需要 awk（1） 即可进行处理。严肃的监控工具应该使用 netlink（7） 接口，它以二进制格式传递信息并避免文本解析的开销

# #10.6.2 ip
ip（8） 是一个用于管理路由、网络设备、接口和隧道的工具。为了实现可观察性，它可以用来打印以下方面的统计数据：链接、地址、路由等。例如，在接口 （link） 上打印额外的统计信息 （-s）：

# ip -s link
 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT 
group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    RX: bytes  packets  errors  dropped overrun mcast   
    26550075   273178   0       0       0       0       
    TX: bytes  packets  errors  dropped carrier collsns 
    26550075   273178   0       0       0       0       
  2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT 
group default qlen 1000
    link/ether 12:c0:0a:b0:21:b8 brd ff:ff:ff:ff:ff:ff
    RX: bytes  packets  errors  dropped overrun mcast   
    512473039143 568704184 0       0       0       0       
    TX: bytes  packets  errors  dropped carrier collsns 
    573510263433 668110321 0       0       0       0   

在静态性能调整期间，检查所有接口的配置可能很有用，以检查配置错误。输出中还包括错误指标：对于接收 （RX）：接收错误、丢弃和溢出;对于传输 （TX）：传输错误、丢弃、载波错误和冲突。此类错误可能是性能问题的根源，并且可能是由网络硬件故障引起的，具体取决于错误。这些是全局计数器，显示自接口激活以来的所有错误（在网络术语中，它被设置为“UP”）

指定 -s 选项两次 （-s -s） 可提供更多错误类型的统计信息。

虽然 ip（8） 提供了 RX 和 TX 字节计数器，但它没有提供打印某个时间间隔内当前吞吐量的选项。为此，请使用 sar（1） （第 10.6.6 节，sar）。

# 路由表
ip（1） 确实具有对其他网络组件的可观察性。例如，路由对象显示路由表：

# ip route
 default via 100.85.128.1 dev eth0 
default via 100.85.128.1 dev eth0 proto dhcp src 100.85.142.69 metric 100 
100.85.128.0/18 dev eth0 proto kernel scope link src 100.85.142.69 
100.85.128.1 dev eth0 proto dhcp scope link src 100.85.142.69 metric 100 

路由配置错误也可能是性能问题的根源（例如，当管理员添加了特定路由条目，但不再需要这些条目，现在的性能比默认路由差时）。

# 监测
使用 monitoring 子命令 ip monitor 监视 netlink 消息。

# #10.6.3 ifconfig
ifconfig（8） 命令是传统的接口管理工具，也可以列出所有接口的配置。Linux 版本包含带有 output 的统计信息：

# $ ifconfig
 eth0      Link encap:Ethernet  HWaddr 00:21:9b:97:a9:bf  
          inet addr:10.2.0.2  Bcast:10.2.0.255  Mask:255.255.255.0
          inet6 addr: fe80::221:9bff:fe97:a9bf/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
 RX packets:933874764 errors:0 dropped:0 overruns:0 frame:0
 TX packets:1090431029 errors:0 dropped:0 overruns:0 carrier:0
 collisions:0 txqueuelen:1000
 RX bytes:584622361619 (584.6 GB)  TX bytes:537745836640 (537.7 GB)
          Interrupt:36 Memory:d6000000-d6012800 
eth3      Link encap:Ethernet  HWaddr 00:21:9b:97:a9:c5  
[...]

计数器与 ip（8） 命令中描述的计数器相同

在 Linux 上，ifconfig（8） 被视为过时，替换为 ip（8）

# #10.6.4 nstat
nstat（8） 打印内核维护的各种网络度量标准及其 SNMP 名称。例如，使用 -s 可避免重置计数器：

# nstat -s
 #kernel
 IpInReceives                    462657733          0.0
 IpInDelivers                    462657733          0.0
 IpOutRequests                   497050986          0.0
 IpOutDiscards                   42                 0.0
 IpFragOKs                       2298               0.0
 IpFragCreates                   13788              0.0
 IcmpInMsgs                      91                 0.0
 [...]
 TcpActiveOpens                  362997             0.0
 TcpPassiveOpens                 9663983            0.0
 TcpAttemptFails                 12718              0.0
 TcpEstabResets                  14591              0.0
 TcpInSegs                       462181482          0.0
 TcpOutSegs                      938958577          0.0
 TcpRetransSegs                  129212             0.0
 TcpOutRsts                      52362              0.0
 UdpInDatagrams                  476072             0.0
 UdpNoPorts                      88                 0.0
 UdpOutDatagrams                 476197             0.0
 UdpIgnoredMulti                 2                  0.0
 Ip6OutRequests                  29                 0.0
 [...]

关键指标包括： 
■ IpInReceives：入站 IP 数据包。
■ IpOutRequests：出站 IP 数据包。
■ TcpActiveOpens： TCP 活动连接（connect（2） 套接字 syscall）。
■ TcpPassiveOpens： TCP 被动连接（accept（2） 套接字系统调用）。
■ TcpInSegs：TCP 入站分段。
■ TcpOutSegs：TCP 出站段。
■ TcpRetransSegs：TCP 重传段。与 TcpOutSegs 比较重传比率

如果不使用 -s 选项，nstat（8） 的默认行为是重置内核计数器。这很有用，因为您可以第二次运行 nstat（8） 并查看跨该 val 的计数，而不是自启动以来的总数。如果你有一个网络问题，可以用命令重现，那么可以在命令之前和之后运行 nstat（8） 来显示哪些计数器发生了变化。

如果您忘记使用 -s 并错误地重置了计数器，则可以使用 -rs 将它们设置回其摘要，因为 boot values

nstat（8） 也有一个守护进程模式（-d）来收集间隔统计信息，使用时显示在最后一列。

# #10.6.5 netstat
netstat（8） 命令根据使用的选项报告各种类型的网络统计信息。它就像一个具有多种不同功能的多功能工具。这些措施包括：

■（默认）展示连接的套接字 
■ -a：列出所有套接字的信息
■ -s：网络堆栈统计
■ -i：网络接口统计
■ -r：列出路由表

其他选项可以修改输出，包括 -n 不将 IP 地址解析为主机名，以及 -v 提供详细详细信息（如果可用）

下面是一个 netstat（8） 接口统计的例子：

# $ netstat -i
 Kernel Interface table
 Iface    MTU     RX-OK RX-ERR RX-DRP RX-OVR      TX-OK TX-ERR TX-DRP TX-OVR Flg
 eth0    1500 933760207      0      0 0      1090211545      0      0      0 BMRU
 eth3    1500 718900017      0      0 0       587534567      0      0      0 BMRU
 lo     16436 21126497       0      0 0        21126497      0      0      0 LRU
 ppp5    1496     4225       0      0 0            3736      0      0      0 MOPRU
 ppp6    1496     1183       0      0 0            1143      0      0      0 MOPRU
 tun0    1500   695581       0      0 0          692378      0      0      0 MOPRU
 tun1    1462        0       0      0 0               4      0      0      0 PRU

这些列包括网络接口 （Iface）、MTU 以及接收 （RX-） 和传输 （TX-） 的一系列指标：

■ -OK：数据包传输成功 
■ -ERR：数据包错误 
■ -DRP：数据包丢弃 
■ -OVR：数据包超限

丢包和超限是网络接口饱和的指示，可以作为 USE 方法的一部分与错误一起检查。

-c continuous 模式可以与 -i 一起使用，后者每秒打印这些累积计数器。这提供了用于计算数据包速率的数据。

下面是一个 netstat（8） 网络堆栈统计的例子（被截断）：

# $ netstat -s
 Ip:
    Forwarding: 2
 forwarded
    454143446 total packets received
    0 
    0 incoming packets discarded
    454143446 incoming packets delivered
    487760885 requests sent out
    42 outgoing packets dropped
    2260 fragments received ok
    13560 fragments created
 Icmp:
    91 ICMP messages received
 [...]
 Tcp:
    359286 active connection openings
    9463980 passive connection openings

 12527 failed connection attempts
    14323 connection resets received
    13545 connections established
    453673963 segments received
    922299281 segments sent out
    127247 segments retransmitted
    0 bad segments received
    51660 resets sent
 Udp:
    469302 packets received
    88 packets to unknown port received
    0 packet receive errors
    469427 packets sent
    0 receive buffer errors
    0 send buffer errors
    IgnoredMulti: 2
 TcpExt:
    21 resets received for embryonic SYN_RECV sockets
    12252 packets pruned from receive queue because of socket buffer overrun
    201219 TCP sockets finished time wait in fast timer
    11727438 delayed acks sent
    1445 delayed acks further delayed because of locked socket
    Quick ack mode was activated 17624 times
    169257582 packet headers predicted
    76058392 acknowledgments not containing data payload received
    111925821 predicted acknowledgments
    TCPSackRecovery: 1703
    Detected reordering 876 times using SACK
    Detected reordering 19 times using time stamp
    2 congestion windows fully recovered without slow start
    19 congestion windows partially recovered using Hoe heuristic
    TCPDSACKUndo: 164
    88 congestion windows recovered without slow start after partial ack
    TCPLostRetransmit: 901
    TCPSackFailures: 31
    28248 fast retransmits
    709 retransmits in slow start
    TCPTimeouts: 12684
    TCPLossProbes: 73383
    TCPLossProbeRecovery: 132
    TCPSackRecoveryFail: 24
    805315 packets collapsed in receive queue due to low socket buffer
 [...]
    TCPAutoCorking: 13520259
    TCPFromZeroWindowAdv: 257
    TCPToZeroWindowAdv: 257
    TCPWantZeroWindowAdv: 18941
    TCPSynRetrans: 24816
 [...]

输出列出了各种网络统计信息，主要来自 TCP，按其协议分组。幸运的是，其中许多都有很长的描述性名称，因此它们的含义可能很明显。其中许多统计数据以粗体突出显示，以显示可用的与性能相关的信息类型。其中许多需要对 TCP 行为有深入的了解，包括近年来引入的新功能和算法。要查找的一些示例统计数据：

■ 转发数据包的速率与接收的数据包总数较高：检查服务器是否应该转发（路由）数据包。
■ 被动连接打开数：可以对其进行监控，以根据客户端连接来显示负载。
■ 重新传输的段速率与发送的段的比率较高：可能表明网络不可靠。这可能是意料之中的（Internet 客户端）。
■ TCPSynRetrans：显示重新传输的 SYN，这可能是由于远程端点由于负载而从侦听积压中删除 SYN 引起的。
■ 由于套接字缓冲区溢出而从接收队列中删除的数据包：这是网络饱和的迹象，可以通过增加套接字缓冲区来解决，前提是应用程序有足够的系统资源来跟上。

一些统计信息名称包含拼写错误（例如，packetes rejected）。如果其他监控工具已基于相同的输出构建，那么简单地解决这些问题可能会有问题。这些工具应该通过处理 nstat（8） 输出来更好地服务，它使用标准的 SNMP 名称，或者更好的是，直接读取这些统计信息的 /proc 源，即 /proc/net/snmp 和 /proc/net/ netstat。例如：

# $ grep ^Tcp /proc/net/snmp
 Tcp: RtoAlgorithm RtoMin RtoMax MaxConn ActiveOpens PassiveOpens AttemptFails 
EstabResets CurrEstab InSegs OutSegs RetransSegs InErrs OutRsts InCsumErrors
 Tcp: 1 200 120000 -1 102378 126946 11940 19495 24 627115849 325815063 346455 5 24183 
0

这些 /proc/net/snmp 统计信息还包括 SNMP 管理信息库 （MIB）。MIB 文档描述了每个统计数据应该是什么（如果内核已正确实现它）。扩展统计信息位于 /proc/net/netstat 中

interval（以秒为单位）可以与 netstat（8） 一起使用，这样它在每个间隔内连续打印累积计数器。然后，可以对此输出进行后处理以计算每个计数器的速率。

# #10.6.6 sar
系统活动报告器 sar（1） 可用于观察当前活动，并且可以配置为存档和报告历史统计信息。它在第 4 章 可观测性工具中介绍，并在其他章节中适当地提及

Linux 版本通过以下选项提供网络统计信息： 
■-n DEV：网络接口统计 
■-n EDEV：网络接口错误 
■-n IP：IP 数据报统计 
■-n EIP：IP 错误统计 
■-n TCP：TCP 统计 
■-n ETCP：TCP 错误统计 
■-n SOCK：套接字使用情况

提供的统计数据包括表 10.5 中所示的统计数据。

未列出的是 ICMP、NFS 和 SOFT（软件网络处理）组，以及 IPv6 变体：IP6、EIP6、SOCK6 和 UDP6。有关统计信息的完整列表，请参阅手册页，其中还记录了一些等效的 SNMP 名称（例如，irec/s 的 ipInReceives）。许多 sar（1） 统计名称在实践中很容易记住，因为它们包括测量的方向和单位：rx 表示“received”，i 表示“input”，seg 表示“segments”，等等

此示例每秒打印一次 TCP 统计信息：

# $ sar -n TCP 1
 Linux 5.3.0-1010-aws (ip-10-1-239-218)    02/27/20        _x86_64_  (2 CPU)
 07:32:45     active/s passive/s    iseg/s    oseg/s
 07:32:46         0.00     12.00    186.00  28837.00
 07:32:47         0.00     13.00    203.00  33584.00
 07:32:48         0.00     11.00   1999.00  24441.00
 07:32:49         0.00      7.00     92.00   8908.00
 07:32:50         0.00     10.00    114.00  13795.00
 [...]

输出显示被动连接速率（入站）约为每秒 10 个。

检查网络设备 （DEV） 时，网络接口统计信息列 （IFACE） 列出所有接口;然而，通常只有一个是感兴趣的。以下示例使用一点 awk（1） 来过滤输出：

# $ sar -n DEV 1 | awk 'NR == 3 || $2 == "ens5"'
 07:35:41 IFACE  rxpck/s   txpck/s  rxkB/s   txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil
 07:35:42  ens5   134.00  11483.00   10.22  6328.72    0.00    0.00     0.00    0.00
 07:35:43  ens5   170.00  20354.00   13.62  6925.27    0.00    0.00     0.00    0.00
 07:35:44  ens5   185.00  28228.00   14.33  8586.79    0.00    0.00     0.00    0.00
 07:35:45  ens5   180.00  23093.00   14.59  7452.49    0.00    0.00     0.00    0.00
 07:35:46  ens5  1525.00  19594.00  137.48  7044.81    0.00    0.00     0.00    0.00
 07:35:47  ens5   146.00  10282.00   12.05  6876.80    0.00    0.00     0.00    0.00
 [...]

这显示了传输和接收的网络吞吐量以及其他统计信息。

atop（1） 工具还能够存档统计信息。

# #10.6.7 nicstat
nicstat（1）打印网络接口统计信息，包括吞吐量和利用率。它遵循传统资源统计工具 iostat（1） 和 mpstat（1） 的样式。

以下是 Linux 上版本 1.92 的输出

# nicstat -z 1
    Time      Int   rKB/s   wKB/s   rPk/s   wPk/s    rAvs    wAvs %Util    Sat
 01:20:58     eth0    0.07    0.00    0.95    0.02   79.43   64.81  0.00   0.00
 01:20:58     eth4    0.28    0.01    0.20    0.10  1451.3   80.11  0.00   0.00
 01:20:58  vlan123    0.00    0.00    0.00    0.02   42.00   64.81  0.00   0.00
 01:20:58      br0    0.00    0.00    0.00    0.00   42.00   42.07  0.00   0.00
    Time      Int   rKB/s   wKB/s   rPk/s   wPk/s    rAvs    wAvs %Util    Sat
 01:20:59     eth4 42376.0   974.5 28589.4 14002.1  1517.8   71.27  35.5   0.00
    Time      Int   rKB/s   wKB/s   rPk/s   wPk/s    rAvs    wAvs %Util    Sat
 01:21:00     eth0    0.05    0.00    1.00    0.00   56.00    0.00  0.00   0.00
 01:21:00     eth4 41834.7   977.9 28221.5 14058.3  1517.9   71.23  35.1   0.00
    Time      Int   rKB/s   wKB/s   rPk/s   wPk/s    rAvs    wAvs %Util    Sat
 01:21:01     eth4 42017.9   979.0 28345.0 14073.0  1517.9   71.24  35.2   0.00

第一个输出是 summary-since-boot，然后是间隔摘要。区间和矩阵显示 eth4 接口以 35% 的利用率运行（这是从 RX 或 TX 方向报告的此刻最高利用率），读数为 42 MB/s。

这些字段包括接口名称 （Int）、最大利用率 （%Util）、反映接口饱和度统计信息 （Sat） 的值，以及一系列前缀为 r 的统计信息，分别表示 “read” （接收）和 w 表示 “write” （transmit）：

■ KB/s：每秒 KB 数 
■ Pk/s：每秒数据包数 
■ Avs/s：平均数据包大小，字节

此版本支持的选项包括 -z（用于跳过零行（空闲接口）和 -t（用于 TCP 统计信息）。

nicstat（1） 对于 USE 方法特别有用，因为它提供 utilization 和 saturation 值。

# #10.6.8 ethtool
ethtool（8） 可以用 -i 和 -k 选项来检查网络接口的静态配置，也可以用 -S 打印驱动程序统计信息。例如：

# ethtool -S eth0
NIC statistics:
    tx_timeout: 0
    suspend: 0
    resume: 0
    wd_expired: 0
    interface_up: 1
    interface_down: 0
    admin_q_pause: 0
    queue_0_tx_cnt: 100219217
    queue_0_tx_bytes: 84830086234
    queue_0_tx_queue_stop: 0
    queue_0_tx_queue_wakeup: 0
    queue_0_tx_dma_mapping_err: 0
    queue_0_tx_linearize: 0
    queue_0_tx_linearize_failed: 0
    queue_0_tx_napi_comp: 112514572
    queue_0_tx_tx_poll: 112514649
    queue_0_tx_doorbells: 52759561
[...]

这会从内核 ethtool 框架获取统计信息，许多网络设备驱动程序都支持该框架。设备驱动程序可以定义自己的 ethtool 统计信息。

-i 选项显示驱动程序详细信息，-k 显示接口可调参数。例如：

# ethtool -i eth0
 driver: ena
 version: 2.0.3K
 [...]
# ethtool -k eth0
 Features for eth0:
 rx-checksumming: on
 [...]
 tcp-segmentation-offload: off
        tx-tcp-segmentation: off [fixed]
        tx-tcp-ecn-segmentation: off [fixed]
        tx-tcp-mangleid-segmentation: off [fixed]
        tx-tcp6-segmentation: off [fixed]
 udp-fragmentation-offload: off
 generic-segmentation-offload: on
 generic-receive-offload: on
 large-receive-offload: off [fixed]
 rx-vlan-offload: off [fixed]
 tx-vlan-offload: off [fixed]
 ntuple-filters: off [fixed]
 receive-hashing: on
 highdma: on
 [...]

此示例是具有 ena 驱动程序的云实例，其中 tcp-segmentation-offload offload 关闭。-K 选项可用于更改这些可调参数

# #10.6.9 tcplife
tcplife（8）是一个 BCC 和 bpftrace 工具，用于跟踪 TCP 会话的生命周期，显示会话的持久性、地址详细信息、吞吐量，并在可能的情况下显示负责的进程 ID 和名称。

下面显示了来自 BCC 的 tcplife（8），在一个 48 CPU 的生产实例上

# tcplife
 PID   COMM       LADDR           LPORT RADDR           RPORT TX_KB RX_KB MS
 4169  java       100.1.111.231   32648 100.2.0.48      6001      0     0 3.99
 4169  java       100.1.111.231   32650 100.2.0.48      6001      0     0 4.10
 4169  java       100.1.111.231   32644 100.2.0.48      6001      0     0 8.41
 4169  java       100.1.111.231   40158 100.2.116.192   6001      7    33 3590.91
 4169  java       100.1.111.231   56940 100.5.177.31    6101      0     0 2.48
 4169  java       100.1.111.231   6001  100.2.176.45    49482     0     0 17.94
 4169  java       100.1.111.231   18926 100.5.102.250   6101      0     0 0.90
 4169  java       100.1.111.231   44530 100.2.31.140    6001      0     0 2.64
 4169  java       100.1.111.231   44406 100.2.8.109     6001     11    28 3982.11
 34781 sshd       100.1.111.231   22    100.2.17.121    41566     5     7 2317.30
 4169  java       100.1.111.231   49726 100.2.9.217     6001     11    28 3938.47
 4169  java       100.1.111.231   58858 100.2.173.248   6001      9    30 2820.51
 [...]

此输出显示一系列短期（少于 20 毫秒）或长期（超过 3 秒）的连接，如 duration 列（MS 表示毫秒）所示。这是一个侦听端口 6001 的应用程序服务器池。此屏幕截图中的大多数会话都显示了与远程应用程序服务器上的端口 6001 的连接，只有一个连接到本地端口 6001。还看到了一个 ssh 会话，它由 sshd 和本地端口 22 拥有，这是一个入站会话。

密件抄送版本的 tcplife（8） 支持的选项包括： 
■ -t： 包括时间列 （HH：MM：SS）
■ -w： 更宽的列 （为了更好地适应 IPv6 地址）
■ -p PID： 只跟踪这个进程
■ -L port[，port[,...]]：只跟踪这些本地端口的会话
■ -D 端口[，PORT[,...]]：只跟踪这些远程端口的会话

此工具的工作原理是跟踪 TCP 套接字状态更改事件，并在状态更改为 TCP_CLOSE 时打印摘要详细信息。这些状态更改事件的频率比数据包低得多，因此这种方法的开销成本比按数据包嗅探器低得多。这使得 tcplife（8） 可以作为 TCP 流记录器在 Netflix 生产服务器上持续运行。

创建 udplife（8） 来跟踪 UDP 会话是 BPF 性能工具一书 [Gregg 19] 中的第 10 章练习;我已经发布了一个初步的解决方案 [Gregg 19d]。

# #10.6.10 tcptop
tcptop（8）是一个密件抄送工具，它显示使用 TCP 的顶级进程。例如，从 36 个 CPU 的生产 Hadoop 实例中：

# tcptop
 09:01:13 loadavg: 33.32 36.11 38.63 26/4021 123015
 PID    COMM       LADDR                RADDR                 RX_KB  TX_KB
 118119 java       100.1.58.46:36246    100.2.52.79:50010     16840      0
 122833 java       100.1.58.46:52426    100.2.6.98:50010          0   3112
 122833 java       100.1.58.46:50010    100.2.50.176:55396     3112      0
 120711 java       100.1.58.46:50010    100.2.7.75:23358       2922      0
 121635 java       100.1.58.46:50010    100.2.5.101:56426      2922      0
 121219 java       100.1.58.46:50010    100.2.62.83:40570      2858      0
 121219 java       100.1.58.46:42324    100.2.4.58:50010          0   2858
 122927 java       100.1.58.46:50010    100.2.2.191:29338      2351      0
 [...]

此输出显示顶部的一个连接在此间隔内接收超过 16 MB 的吞吐量。默认情况下，屏幕每秒更新一次

这是通过跟踪 TCP 发送和接收代码路径，并以 BPF 映射效率汇总数据来实现的。即便如此，这些事件也可能很频繁，并且在高网络吞吐量系统上，开销可能会变得可衡量。

选项包括：
■ -C：不清除屏幕。
■ -p PID：仅测量此过程。

tcptop（8） 也接受可选的 interval 和 count。

# #10.6.11 tcpretrains
tcpretrans（8）是一个密件抄送和 bpftrace 工具，用于跟踪 TCP 重传，显示 IP 地址和端口详细信息以及 TCP 状态。下面显示了 BCC 在生产实例上的 tcpretrans（8）

# tcpretrans
 Tracing retransmits ... Hit Ctrl-C to end
 TIME     PID    IP LADDR:LPORT         T> RADDR:RPORT         STATE
 00:20:11 72475  4  100.1.58.46:35908   R> 100.2.0.167:50010   ESTABLISHED
 00:20:11 72475  4  100.1.58.46:35908   R> 100.2.0.167:50010   ESTABLISHED
 00:20:11 72475  4  100.1.58.46:35908   R> 100.2.0.167:50010   ESTABLISHED
 00:20:12 60695  4  100.1.58.46:52346   R> 100.2.6.189:50010   ESTABLISHED
 00:20:12 60695  4  100.1.58.46:52346   R> 100.2.6.189:50010   ESTABLISHED
 00:20:12 60695  4  100.1.58.46:52346   R> 100.2.6.189:50010   ESTABLISHED
 00:20:12 60695  4  100.1.58.46:52346   R> 100.2.6.189:50010   ESTABLISHED
 00:20:13 60695  6  ::ffff:100.1.58.46:13562 R> ::ffff:100.2.51.209:47356 FIN_WAIT1
 00:20:13 60695  6  ::ffff:100.1.58.46:13562 R> ::ffff:100.2.51.209:47356 FIN_WAIT1
 [...]

此输出显示低重传率，每秒几次（TIME 列），主要用于处于 ESTABLISHED 状态的会话。处于 ESTABLISHED 状态的高速率可能表示存在外部网络问题。SYN_SENT 状态中的高速率可能表示服务器应用程序过载，该服务器应用程序没有足够快地消耗其 SYN 积压。

这是通过跟踪内核中的 TCP 重传事件来实现的。由于这些应该不经常发生，因此开销应该可以忽略不计。将此与历史上使用数据包嗅探器捕获所有数据包，然后进行后处理以查找重新传输来分析重传的方式进行比较，这两个步骤都会消耗大量的 CPU 开销。Packet-capture 只能看到网络上的细节，而 tcpretrans（8） 直接从内核打印 TCP 状态，如果需要，可以增强它以打印更多的内核状态。

BCC 版本的选项包括： 
■ -l：包括尾部丢失探测尝试（为 tcp_send_loss_probe（） 添加 kprobe）
■ -c：每个流的重新传输计数

-c 选项改变了 tcpretrans（8） 的行为，使其打印计数摘要而不是每个事件的详细信息。

# #10.6.12 bpftrace
bpftrace 是一种基于 BPF 的跟踪器，它提供了一种高级编程语言，允许创建强大的单行代码和简短脚本。它非常适合根据来自其他工具的线索进行自定义网络分析。它可以检查内核和应用程序内部的网络事件，包括套接字连接、套接字 I/O、TCP 事件、数据包传输、积压丢弃、TCP 重新传输和其他详细信息。这些功能支持工作负载特征描述和延迟分析。

bpftrace 在第 15 章中进行了解释。本节介绍网络分析的一些示例：单行代码、套接字跟踪和 TCP 跟踪

# 单行
以下单行代码非常有用，并演示了不同的 bpftrace 功能。

通过 PID 和进程名称对套接字 accept（2） 进行计数：
# bpftrace -e 't:syscalls:sys_enter_accept* { @[pid, comm] = count(); }'

按 PID 和进程名称对套接字连接 （2） 进行计数：
# bpftrace -e 't:syscalls:sys_enter_connect { @[pid, comm] = count(); }'

按用户堆栈跟踪对 socket connect（2） 进行计数：
# bpftrace -e 't:syscalls:sys_enter_connect { @[ustack, comm] = count(); }'

按方向、CPU 上的 PID 和进程名称对套接字发送/接收进行计数：
# bpftrace -e 'k:sock_sendmsg,k:sock_recvmsg { @[func, pid, comm] = count(); }'

根据 CPU 上的 PID 和进程名称对套接字发送/接收字节进行计数：
# bpftrace -e 'kr:sock_sendmsg,kr:sock_recvmsg /(int32)retval > 0/ { @[pid, comm] =sum((int32)retval); }'

按 CPU 上的 PID 和进程名称对 TCP 连接进行计数：
# bpftrace -e 'k:tcp_v*_connect { @[pid, comm] = count(); }'

按 CPU 上的 PID 和进程名称对 TCP 接受进行计数：
# bpftrace -e 'k:inet_csk_accept { @[pid, comm] = count(); }'

按 CPU 上的 PID 和进程名称计算 TCP 发送/接收次数：
# bpftrace -e 'k:tcp_sendmsg,k:tcp_recvmsg { @[func, pid, comm] = count(); }'

TCP 发送字节数作为直方图：
# bpftrace -e 'k:tcp_sendmsg { @send_bytes = hist(arg2); }'

TCP 接收字节数作为直方图：
# bpftrace -e 'kr:tcp_recvmsg /retval >= 0/ { @recv_bytes = hist(retval); }'

按类型和远程主机对 TCP 重新传输进行计数（假设为 IPv4）：
# bpftrace -e 't:tcp:tcp_retransmit_* { @[probe, ntop(2, args->saddr)] = count(); }'

计算所有 TCP 函数（给 TCP 增加高开销）：
# bpftrace -e 'k:tcp_* { @[func] = count(); }'

按 CPU 上的 PID 和进程名称对 UDP 发送/接收进行计数：
# bpftrace -e 'k:udp*_sendmsg,k:udp*_recvmsg { @[func, pid, comm] = count(); }'

UDP 发送字节作为直方图：
# bpftrace -e 'k:udp_sendmsg { @send_bytes = hist(arg2); }'

UDP 接收字节数作为直方图：
# bpftrace -e 'kr:udp_recvmsg /retval >= 0/ { @recv_bytes = hist(retval); }'

计数传输内核堆栈跟踪：
# bpftrace -e 't:net:net_dev_xmit { @[kstack] = count(); }'

显示每个设备的接收 CPU 直方图：
# bpftrace -e 't:net:netif_receive_skb { @[str(args->name)] = lhist(cpu, 0, 128, 1); }'

对 ieee80211 层函数进行计数（为数据包增加高开销）：
# bpftrace -e 'k:ieee80211_* { @[func] = count(); }'

计算所有 ixgbevf 设备驱动程序功能（为 ixgbevf 增加高开销）：
# bpftrace -e 'k:ixgbevf_* { @[func] = count(); }'

对所有 iwl 设备驱动程序跟踪点进行计数（给 iwl 增加高开销）：
# bpftrace -e 't:iwlwifi:*,t:iwlwifi_io:* { @[probe] = count(); }'

# 套接字跟踪
在套接字层跟踪网络事件的优点是，负责的进程仍在 CPU 上，这使得识别负责的应用程序和代码路径变得简单明了。例如，计算调用 accept（2） syscall 的应用程序：

 # bpftrace -e 't:syscalls:sys_enter_accept { @[pid, comm] = count(); }'
 Attaching 1 probe...
 ^C
 @[573, sshd]: 2
 @[1948, mysqld]: 41

输出显示，在跟踪期间，mysqld 调用了 accept（2） 41 次，sshd 调用了 accept（2） 2 次。

可以包含堆栈跟踪以显示导致 accept（2） 的代码路径。例如，按用户级堆栈跟踪和进程名称进行计数：

 # bpftrace -e 't:syscalls:sys_enter_accept { @[ustack, comm] = count(); }'
 Attaching 1 probe...
 ^C
 @[
    accept+79
    Mysqld_socket_listener::listen_for_connection_event()+283
    mysqld_main(int, char**)+15577
    __libc_start_main+243
    0x49564100fe8c4b3d
 , mysqld]: 22

此输出显示 mysqld 通过包含 Mysqld_socket_listener：：listen_for_connection_event（） 的代码路径接受连接。通过将 “accept” 改为 “connect”，这个单行代码将识别通向 connect（2） 的代码路径。我使用这样的单行代码来解释神秘的网络连接，显示调用它们的代码路径。

# sock 跟踪点
除了 socket syscalls，还有 socket tracepoint。从 5.3 内核开始：

 # bpftrace -l 't:sock:*'
 tracepoint:sock:sock_rcvqueue_full
 tracepoint:sock:sock_exceed_buf_limit
 tracepoint:sock:inet_sock_set_state

sock：inet_sock_set_state 跟踪点由早期的 tcplife（8） 工具使用。下面是一个示例单行代码，使用它来计算新连接的源和目标 IPv4 地址：

 # bpftrace -e 't:sock:inet_sock_set_state
    /args->newstate == 1 && args->family == 2/ {
    @[ntop(args->saddr), ntop(args->daddr)] = count() }'
 Attaching 1 probe...
 ^C
 @[127.0.0.1, 127.0.0.1]: 2
 @[10.1.239.218, 10.29.225.81]: 18

这个单行代码越来越长，保存为 bpftrace 程序文件 （.bt） 进行编辑和执行会更容易。作为一个文件，它还可以包含适当的内核头文件，以便可以重写过滤器行以使用常量名称而不是硬编码的数字（这是不可靠的），如下所示：

/args->newstate == TCP_ESTABLISHED && args->family == AF_INET/ {

下一个示例是程序文件：socketio.bt。

# socketio.bt
作为一个更复杂的例子，socketio（8） 工具显示了套接字 I/O 以及进程细节、方向、协议和端口。输出示例：

# ./socketio.bt
 Attaching 2 probes...
 ^C
 [...]
 @io[sshd, 21925, read, UNIX, 0]: 40
 @io[sshd, 21925, read, TCP, 37408]: 41
 @io[systemd, 1, write, UNIX, 0]: 51
 @io[systemd, 1, read, UNIX, 0]: 57
 @io[systemd-udevd, 241, write, NETLINK, 0]: 65
 @io[systemd-udevd, 241, read, NETLINK, 0]: 75
 @io[dbus-daemon, 525, write, UNIX, 0]: 98
 @io[systemd-logind, 526, read, UNIX, 0]: 105
 @io[systemd-udevd, 241, read, UNIX, 0]: 127
 @io[snapd, 31927, read, NETLINK, 0]: 150
 @io[dbus-daemon, 525, read, UNIX, 0]: 160
 @io[mysqld, 1948, write, TCP, 55010]: 8147
 @io[mysqld, 1948, read, TCP, 55010]: 24466

这表明大多数套接字 I/O 是由 mysqld 进行的，对 TCP 端口 55010（客户端正在使用的临时端口）进行读取和写入。

socketio（8） 的源代码是：

#!/usr/local/bin/bpftrace
 #include <net/sock.h>
 kprobe:sock_recvmsg
 {
        $sock = (struct socket *)arg0;
        $dport = $sock->sk->__sk_common.skc_dport;
        $dport = ($dport >> 8) | (($dport << 8) & 0xff00);
        @io[comm, pid, "read", $sock->sk->__sk_common.skc_prot->name, $dport] =
            count();
 }
 kprobe:sock_sendmsg
 {
        $sock = (struct socket *)arg0;
        $dport = $sock->sk->__sk_common.skc_dport;
        $dport = ($dport >> 8) | (($dport << 8) & 0xff00);
        @io[comm, pid, "write", $sock->sk->__sk_common.skc_prot->name, $dport] =
            count();
 }

这是从内核结构体（在本例中为 struct socket）获取详细信息的示例，它提供协议名称和目标端口。目标端口是大端，在包含在 @io 映射中之前，该工具将其转换为小端（对于此 x86 处理器）。可以修改此脚本以显示传输的字节数，而不是 I/O 计数。

# TCP追踪
TCP 级别的跟踪提供了对 TCP 协议事件和内部结构以及与套接字无关的事件（例如 TCP 端口扫描）的见解。

# TCP 跟踪点
检测 TCP 内部通常需要使用 kprobes，但也有一些可用的 TCP 跟踪点。从 5.3 内核开始：

# bpftrace -l 't:tcp:*'
 tracepoint:tcp:tcp_retransmit_skb
 tracepoint:tcp:tcp_send_reset
 tracepoint:tcp:tcp_receive_reset
 tracepoint:tcp:tcp_destroy_sock
 tracepoint:tcp:tcp_rcv_space_adjust
 tracepoint:tcp:tcp_retransmit_synack
 tracepoint:tcp:tcp_probe

tcp：tcp_retransmit_skb 跟踪点被早期的 tcpretrans（8） 工具使用。跟踪点因其稳定性而更可取，但是当它们无法解决您的问题时，您可以在内核 TCP 函数上使用 kprobes。数一数：

# bpftrace -e 'k:tcp_* { @[func] = count(); }'
 Attaching 336 probes...
 ^C
 @[tcp_try_keep_open]: 1
 @[tcp_ooo_try_coalesce]: 1
 @[tcp_reset]: 1
 [...]
 @[tcp_push]: 3191
 @[tcp_established_options]: 3584
 @[tcp_wfree]: 4408
 @[tcp_small_queue_check.isra.0]: 4617
 @[tcp_rate_check_app_limited]: 7022
 @[tcp_poll]: 8898
 @[tcp_release_cb]: 18330
 @[tcp_send_mss]: 28168
 @[tcp_sendmsg]: 31450
 @[tcp_sendmsg_locked]: 31949
 @[tcp_write_xmit]: 33276
 @[tcp_tx_timestamp]: 33485

这表明调用频率最高的函数是 tcp_tx_timestamp（），在跟踪时调用了 33,485 次。计数函数可以更详细地识别要跟踪的目标。请注意，由于跟踪的函数的数量和频率，对所有 TCP 调用进行计数可能会增加明显的开销。对于这个特定的任务，我会通过我的 funccount（8） perf-tools 工具使用 Ftrace 函数分析，因为它的开销和初始化时间要短得多。请参见第 14 章 “Ftrace”。

# tcpsynbl.bt
tcpsynbl（8）19 工具是使用 kprobe 检测 TCP 的一个示例。它显示按队列长度细分的 listen（2） 积压队列的长度，以便您可以判断队列离溢出（这会导致 TCP SYN 数据包的丢弃）有多近。输出示例：

 # tcpsynbl.bt
 Attaching 4 probes...
 Tracing SYN backlog size. Ctrl-C to end.
 04:44:31 dropping a SYN.
 04:44:31 dropping a SYN.
 04:44:31 dropping a SYN.
 04:44:31 dropping a SYN.
 04:44:31 dropping a SYN.
 [...]
 ^C
 @backlog[backlog limit]: histogram of backlog size
 @backlog[128]: 
 [0]                  473 |@                                                   |
 [1]                  502 |@                                                   |
 [2, 4)              1001 |@@@                                                 |
 [4, 8)              1996 |@@@@@@                                              |
 [8, 16)             3943 |@@@@@@@@@@@                                         |
 [16, 32)            7718 |@@@@@@@@@@@@@@@@@@@@@@@                             |
 [32, 64)           14460 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@         |
 [64, 128)          17246 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
 [128, 256)          1844 |@@@@@                                               |

运行时，tcpsynbl.bt 会打印时间戳和 SYN 丢弃（如果在跟踪时出现）。终止时（通过键入 Ctrl-C），将为正在使用的每个积压限制打印积压大小的直方图。此输出显示在 4：44：31 处发生了几次 SYN 丢弃，直方图摘要显示限值为 128，并且分布达到该限值 1844 次（128 到 256 存储桶）。此分布显示 SYN 到达时的积压长度。

通过监控积压长度，您可以检查它是否随着时间的推移而增长，从而为您提供 SYN 下降即将到来的早期警告。这是您可以在容量规划中执行的操作

tcpsynbl（8） 的源代码是：

 #!/usr/local/bin/bpftrace
 #include <net/sock.h>
 BEGIN
 {
   printf("Tracing SYN backlog size. Ctrl-C to end.\n");
 }
 kprobe:tcp_v4_syn_recv_sock,
 kprobe:tcp_v6_syn_recv_sock
 {
        $sock = (struct sock *)arg0;
        @backlog[$sock->sk_max_ack_backlog & 0xffffffff] =
            hist($sock->sk_ack_backlog);
        if ($sock->sk_ack_backlog > $sock->sk_max_ack_backlog) {
                time("%H:%M:%S dropping a SYN.\n");
        }
 }
 END
 {
    printf("\n@backlog[backlog limit]: histogram of backlog size\n");
 }

早期打印分布的形状与 hist（） 使用的 log2 刻度有很大关系，其中后面的存储桶跨越更大的范围。您可以使用以下方法将 hist（） 更改为 lhist（）：

lhist($sock->sk_ack_backlog, 0, 1000, 10);

这将打印一个线性直方图，每个存储桶的范围为偶数：在本例中，范围为 0 到 1000，存储桶大小为 10。有关 bpftrace 编程的详细信息，请参见第 15 章 “BPF”。

# 事件源
bpftrace 可以检测更多;表 10.6 显示了用于检测不同网络事件的事件源。

尽可能使用跟踪点，因为它们是稳定的接口

# #10.6.13 tcpdump
在 Linux 上，可以使用 tcpdump（8） 工具捕获和检查网络数据包。这可以在 STDOUT 上打印数据包摘要，也可以将数据包数据写入文件以供以后分析。后者通常更实用：数据包速率可能太高，无法实时跟踪其摘要

将数据包从 eth4 接口转储到 /tmp 中的文件

 # tcpdump -i eth4 -w /tmp/out.tcpdump
 tcpdump: listening on eth4, link-type EN10MB (Ethernet), capture size 65535 bytes
 ^C273893 packets captured
 275752 packets received by filter
 1859 packets dropped by kernel

输出记录了内核丢弃了多少数据包，而不是像数据包速率过高时那样传递给 tcpdump（8）。请注意，您可以使用 -i any 来获取来自所有接口的数据包。

检查转储文件中的数据包：

# tcpdump -nr /tmp/out.tcpdump
 reading from file /tmp/out.tcpdump, link-type EN10MB (Ethernet)
 02:24:46.160754 IP 10.2.124.2.32863 > 10.2.203.2.5001: Flags [.], seq 
 3612664461:3612667357, ack 180214943, win 64436, options [nop,nop,TS val 692339741 
 ecr 346311608], length 2896
 02:24:46.160765 IP 10.2.203.2.5001 > 10.2.124.2.32863: Flags [.], ack 2896, win 
 18184, options [nop,nop,TS val 346311610 ecr 692339740], length 0
 02:24:46.160778 IP 10.2.124.2.32863 > 10.2.203.2.5001: Flags [.], seq 2896:4344, ack 
 1, win 64436, options [nop,nop,TS val 692339741 ecr 346311608], length 1448
 02:24:46.160807 IP 10.2.124.2.32863 > 10.2.203.2.5001: Flags [.], seq 4344:5792, ack 
 1, win 64436, options [nop,nop,TS val 692339741 ecr 346311608], length 1448
 02:24:46.160817 IP 10.2.203.2.5001 > 10.2.124.2.32863: Flags [.], ack 5792, win 
 18184, options [nop,nop,TS val 346311610 ecr 692339741], length 0
 [...]

每行输出显示数据包的时间（具有微秒级分辨率）、其源和目标 IP 地址以及 TCP 标头值。通过研究这些，可以详细了解 TCP 的作，包括高级功能对您的工作负载的适用程度。

-n 选项用于不将 IP 地址解析为主机名。其他选项包括打印详细详细信息（如果可用） （-v）、链接层标头 （-e） 和十六进制地址转储 （-x 或 -X）。例如：

 # tcpdump -enr /tmp/out.tcpdump -vvv -X
 reading from file /tmp/out.tcpdump, link-type EN10MB (Ethernet)
 02:24:46.160754 80:71:1f:ad:50:48 > 84:2b:2b:61:b6:ed, ethertype IPv4 (0x0800), 
length 2962: (tos 0x0, ttl 63, id 46508, offset 0, flags [DF], proto TCP (6), length 
2948)
    10.2.124.2.32863 > 10.2.203.2.5001: Flags [.], cksum 0x667f (incorrect -> 
0xc4da), seq 3612664461:3612667357, ack 180214943, win 64436, options [nop,nop,TS val 
692339741 ecr 346311608], length 289
 6
        0x0000:  4500 0b84 b5ac 4000 3f06 1fbf 0a02 7c02  E.....@.?.....|.
        0x0010:  0a02 cb02 805f 1389 d754 e28d 0abd dc9f  ....._...T......
        0x0020:  8010 fbb4 667f 0000 0101 080a 2944 441d  ....f.......)DD.
        0x0030:  14a4 4bb8 3233 3435 3637 3839 3031 3233  ..K.234567890123
        0x0040:  3435 3637 3839 3031 3233 3435 3637 3839  4567890123456789
 [...]

在性能分析期间，更改时间戳列以显示数据包之间的增量时间 （-ttt） 或自第一个数据包以来经过的时间 （-ttttt） 可能很有用。

还可以提供一个表达式来描述如何过滤包（请参见 pcap-filter（7）以专注于感兴趣的包。这是在内核中执行的，以提高效率（在 Linux 2.0 及更早版本中除外），使用 BPF。

数据包捕获在 CPU 成本和存储方面都非常昂贵。如果可能的话，只在短期内使用 tcpdump（8） 以限制性能成本，并寻找使用基于 BPF 的高效工具的方法，例如 bpftrace。

tshark（1） 是一个类似的命令行数据包捕获工具，它提供更好的过滤和输出选项。它是 Wireshark 的 CLI 版本。

# #10.6.14 Wireshark
虽然 tcpdump（8） 对于随意的调查来说效果很好，但对于更深入的分析，在命令行中使用它可能很耗时。Wireshark 工具（以前的 Ethereal）提供了一个用于数据包捕获和检查的图形界面，也可以从 tcpdump（8） [Wireshark 20] 导入数据包转储文件。有用的功能包括识别网络连接及其相关数据包，以便可以单独研究它们，以及转换数百个协议标头。

图 10.12 显示了 Wireshark 的示例屏幕截图。窗口水平分为三个部分。顶部是一个表，将数据包显示为行，将详细信息显示为列。中间部分显示协议详细信息：在此示例中，TCP 协议已扩展并选择目标端口。底部部分在左侧以十六进制形式显示原始数据包，在右侧以文本形式显示：突出显示 TCP 目标端口的位置。

# #10.6.15 其他工具
表 10.7 列出了本书其他章节和 BPF 性能工具 [Gregg 19] 中包含的网络分析工具。

其他 Linux 网络可观测性工具和源包括

■ strace（1）：跟踪与套接字相关的系统调用并检查使用的选项（注意 strace（1） 开销很高） 
■ lsof（8）：按进程 ID 列出打开的文件，包括套接字详细信息 
■ nfsstat（8）：NFS 服务器和客户端统计信息 
■ ifpps（8）：类似 Top 的网络和系统统计信息
■ iftop（8）：按主机（嗅探器）汇总网络接口吞吐量
■ perf（1）：计算并记录网络跟踪点和内核函数。
■ /proc/net：包含许多网络统计信息文件BPF 
■ BPF iterator:：允许 BPF 程序在 /sys/fs/bpf 中导出自定义统计信息

还有许多网络监控解决方案，要么基于 SNMP 要么运行自己的自定义代理。

# 10.7 实验
网络性能通常使用执行实验的工具进行测试，而不仅仅是观察系统的状态。此类实验工具包括 ping（8）、traceroute（8） 和网络微基准测试，例如 iperf（8）。这些可用于确定主机之间的网络运行状况，从而帮助确定在调试应用程序性能问题时端到端网络吞吐量是否是一个问题。

# #10.7.1 ping
ping（8） 命令通过发送 ICMP 回显请求数据包来测试网络连接。例如：

# ping www.netflix.com
 PING www.netflix.com(2620:108:700f::3423:46a1 (2620:108:700f::3423:46a1)) 56 data 
bytes
 64 bytes from 2620:108:700f::3423:46a1 (2620:108:700f::3423:46a1): icmp_seq=1 ttl=43 
time=32.3 ms
 64 bytes from 2620:108:700f::3423:46a1 (2620:108:700f::3423:46a1): icmp_seq=2 ttl=43 
time=34.3 ms
 64 bytes from 2620:108:700f::3423:46a1 (2620:108:700f::3423:46a1): icmp_seq=3 ttl=43 
time=34.0 ms
 ^C--- www.netflix.com ping statistics --
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
 rtt min/avg/max/mdev = 32.341/33.579/34.389/0.889 ms

输出包括每个数据包的往返时间 （time），并包含显示各种统计信息的摘要

旧版本的 ping（8） 测量了从用户空间开始的往返时间，由于内核执行和调度程序延迟，时间略有增加。较新的内核和 ping（8） 版本使用内核时间戳支持（SIOCGSTAMP 或 SO_TIMESTAMP）来提高报告的 ping 时间的准确性。

路由器可能会以低于应用程序协议的优先级处理所使用的 ICMP 数据包，并且延迟可能会显示出比平时更高的差异。

# #10.7.2 traceroute
traceroute（8） 命令发送一系列测试数据包，以实验性方式确定到主机的当前路由。这是通过将每个数据包的 IP 协议生存时间 （TTL） 增加 1 来实现的，通过发送 ICMP 超时响应消息（如果防火墙不阻止它们），导致主机的网关序列显示自身。

例如，测试从加利福尼亚主机到我的网站的当前路由：

# traceroute www.brendangregg.com
 traceroute to www.brendangregg.com (184.168.188.1), 30 hops max, 60 byte packets
 1  _gateway (10.0.0.1)  3.453 ms  3.379 ms  4.769 ms
 2  196.120.89.153 (196.120.89.153)  19.239 ms  19.217 ms  13.507 ms
 3  be-10006-rur01.sanjose.ca.sfba.comcast.net (162.151.1.145)  19.141 ms  19.102 ms  
19.050 ms
 4  be-231-rar01.santaclara.ca.sfba.comcast.net (162.151.78.249)  19.018 ms  18.987 
ms  18.941 ms
 5  be-299-ar01.santaclara.ca.sfba.comcast.net (68.86.143.93)  21.184 ms  18.849 ms  
21.053 ms
 6  lag-14.ear3.SanJose1.Level3.net (4.68.72.105)  18.717 ms  11.950 ms  16.471 ms
 7  4.69.216.162 (4.69.216.162)  24.905 ms 4.69.216.158 (4.69.216.158)  21.705 ms  
28.043 ms
 8  4.53.228.238 (4.53.228.238)  35.802 ms  37.202 ms  37.137 ms
 9  ae0.ibrsa0107-01.lax1.bb.godaddy.com (148.72.34.5)  24.640 ms  24.610 ms  24.579 
ms
 10  148.72.32.16 (148.72.32.16)  33.747 ms  35.537 ms  33.598 ms
 11  be38.trmc0215-01.ars.mgmt.phx3.gdg (184.168.0.69)  33.646 ms  33.590 ms  35.220 
ms
 12  * * *
 13  * * *
 [...]

每个跃点显示一系列三个 RTT，可用作网络延迟统计信息的粗略来源。与 ping（8） 一样，使用的数据包是低优先级的，并且可能比其他应用程序协议显示更高的延迟。某些测试显示 “*”：未返回 ICMP 超时消息。所有三个测试都显示 “*” 可能是由于跃点根本不返回 ICMP，或者 ICMP 被防火墙阻止。解决方法是改用 TCP 而不是 ICMP，使用 -T 选项（也作为命令 tcptraceroute（1） 提供;更高级的版本是 astraceroute（8），它可以自定义标志）。

所采用的路径也可以作为静态性能调优的一部分进行研究。网络设计为动态的，可对中断做出响应，并且性能可能会随着路径的更改而下降。请注意，在 traceroute（8） 运行期间，路径也可以更改：上一个输出中的跃点 7 首先从 4.69.216.162 返回，然后是 4.69.216.158。如果地址发生更改，则打印该地址;否则，仅打印 RTT 时间用于后续测试。

有关解释 traceroute（8） 的高级详细信息，请参见 [Steenbergen 09]

traceroute（8） 最初由 Van Jacobson 编写。他后来创造了一个名为 pathchar 的神奇工具。

# #10.7.3 pathchar
pathchar 类似于 traceroute（8），但包括跃点之间的带宽。这是通过多次发送一系列大小不一的网络数据包并执行统计分析来确定的。以下是示例输出：

# pathchar 192.168.1.10
 pathchar to 192.168.1.1 (192.168.1.1)
 doing 32 probes at each of 64 to 1500 by 32
 0 localhost
 |    
30 Mb/s,   79 us (562 us)
 1 neptune.test.com (192.168.2.1)
 |    
44 Mb/s,   195 us (1.23 ms)
 2 mars.test.com (192.168.1.1)
 2 hops, rtt 547 us (1.23 ms), bottleneck  30 Mb/s, pipe 7555 bytes 

不幸的是，pathchar 不知何故错过了流行（据我所知，可能是因为源代码没有发布），并且很难运行原始版本（pathchar 网站上最新的 Linux 二进制文件是 1997 年发布的 Linux 2.0.30 [Jacobson 97]）。Bruce A. Mah 的新版本，称为 pchar（8），更容易获得。PathChar 的运行也非常耗时，需要数十分钟，具体取决于跃点的数量，尽管已经提出了减少这一时间的方法 [Downey 99]。

# #10.7.4 iperf
iperf（1） 是一个开源工具，用于测试最大 TCP 和 UDP 吞吐量。它支持多种选项，包括使用多个客户端线程的并行模式，这可能是将网络驱动到极限所必需的。iperf（1） 必须在服务器和客户端上执行。

例如，在服务器上执行 iperf（1）

# $ iperf -s -l 128k
-----------------------------------------------------------
Server listening on TCP port 5001
TCP window size: 85.3 KByte (default)
-----------------------------------------------------------

这将套接字缓冲区大小从默认的 8 KB 增加到 128 KB （-l 128k）。

在客户端上执行了以下内容

# iperf -c 10.2.203.2 -l 128k -P 2 -i 1 -t 60-----------------------------------------------------------
Client connecting to 10.2.203.2, TCP port 5001
 TCP window size: 48.0 KByte (default)-----------------------------------------------------------
[  4] local 10.2.124.2 port 41407 connected with 10.2.203.2 port 5001
 [  3] local 10.2.124.2 port 35830 connected with 10.2.203.2 port 5001
 [ ID] Interval       Transfer     Bandwidth
 [  4]  0.0- 1.0 sec  6.00 MBytes  50.3 Mbits/sec
 [  3]  0.0- 1.0 sec  22.5 MBytes   189 Mbits/sec
 [SUM]  0.0- 1.0 sec  28.5 MBytes   239 Mbits/sec
 [  3]  1.0- 2.0 sec  16.1 MBytes   135 Mbits/sec
 [  4]  1.0- 2.0 sec  12.6 MBytes   106 Mbits/sec
 [SUM]  1.0- 2.0 sec  28.8 MBytes   241 Mbits/sec
 [...]
 [  4]  0.0-60.0 sec   748 MBytes   105 Mbits/sec
 [  3]  0.0-60.0 sec   996 MBytes   139 Mbits/sec
 [SUM]  0.0-60.0 sec  1.70 GBytes   244 Mbits/sec

这使用了以下选项： 
■ -c 主机：连接到主机名或 IP 地址 
■ - l 128k：使用 128 KB 套接字缓冲区 
■ - P 2：使用两个客户端线程以并行模式运行
■ -i 1：每秒打印一次间隔摘要
■ -t 60：测试总持续时间：60 秒

最后一行显示测试期间的平均吞吐量，所有并行线程的总和：244 Mbits/s

可以检查每个区间的摘要以查看随时间变化的差异。--reportstyle C 选项可用于输出 CSV，以便随后可以通过其他工具（如绘图软件）导入 CSV

# #10.7.5 netperf
netperf（1） 是一个高级的微基准测试工具，可以测试请求 / 响应性能 [HP 18]。我使用 netperf（1） 来测量 TCP 往返延迟;以下是一些示例输出：

server$ netserver -D -p 7001 
Starting netserver with host 'IN(6)ADDR_ANY' port '7001' and family AF_UNSPEC
 [...]

client$ netperf -v 100 -H 100.66.63.99 -t TCP_RR -p 7001
 MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 
100.66.63.99 () port 0 AF_INET : demo : first burst 0
 Alignment      Offset         RoundTrip  Trans    Throughput
 Local  Remote  Local  Remote  Latency    Rate     10^6bits/s
 Send   Recv    Send   Recv    usec/Tran  per sec  Outbound   Inbound
    8      0       0      0   98699.102   10.132 0.000     0.000 

这表明 TCP 往返延迟为 98.7 毫秒

# #10.7.6 tc
流量控制工具 tc（8） 允许选择各种排队规则 （qdisc） 来提高或管理性能。对于实验，还有一些 qdiscs 可以限制或扰乱性能，这对于测试和模拟很有用。本节演示了网络仿真器 （netem） qdisc。

首先，以下命令列出了接口 eth0 的当前 qdisc 配置：

# tc qdisc show dev eth0
 qdisc noqueue 0: root refcnt 2

现在将添加 netem qdisc。每个 qdisc 都支持不同的可调参数。在这个例子中，我将使用 netem 的丢包参数，并将丢包设置为 1%：

# tc qdisc add dev eth0 root netem loss 1%
# tc qdisc show dev eth0
 qdisc netem 8001: root refcnt 2 limit 1000 loss 1%

eth0 上的后续网络 I/O 现在将遭受 1% 的数据包丢失

tc（8） 的 -s 选项显示统计信息：

# tc -s qdisc show dev eth0
 qdisc netem 8001: root refcnt 2 limit 1000 loss 1%
 Sent 75926119 bytes 89538 pkt (dropped 917, overlimits 0 requeues 0) 
 backlog 0b 0p requeues 0

此输出显示丢弃的数据包数的计数。

删除 qdisc

# tc qdisc del dev eth0 root
# tc qdisc show dev eth0
 qdisc noqueue 0: root refcnt 2 

有关选项的完整列表，请参阅每个 qdisc 的手册页（对于 netem，手册页为 tc-netem（8））。

# #10.7.7 其他工具
其他值得一提的实验工具
■ pktgen：包含在 Linux 内核 [Linux 20l] 中的数据包生成器。
■ Flent：FLExible Network Tester 启动多个微基准测试并绘制结果图表 [Høiland-Jørgensen 20]。
■ mtr（8）：一个类似 traceroute 的工具，包括 ping 统计信息。
■ tcpreplay（1）：重放以前捕获的网络流量（来自 tcpdump（8））的工具，包括模拟数据包计时。虽然对于常规调试比性能测试更有用，但可能存在仅发生在特定数据包或位模式序列中的性能问题，并且此工具可能能够重现这些问题。

# 10.8 调优
网络可调参数通常已经过调整以提供高性能。网络堆栈通常还设计为动态响应不同的工作负载，从而提供最佳性能。

在尝试可调参数之前，最好先了解网络使用情况。这还可以识别出可以消除的不必要工作，从而获得更大的绩效收益。使用上一节中的工具尝试工作负载特征描述和静态性能调优方法。

可用的可调参数因作系统版本而异。请参阅其文档。以下各节介绍了哪些内容可用以及如何调整它们;应将它们视为根据您的工作负载和环境进行修订的起点。

# #10.8.1 系统范围
在 Linux 上，可以使用 sysctl（8） 命令查看和设置系统范围的可调参数，并将其写入 /etc/sysctl.conf。还可以从 /proc/sys/net 下的 /proc 文件系统中读取和写入它们。

例如，要查看当前可用于 TCP 的内容，可以在 sysctl（8） 中搜索文本 “tcp” 的参数：

# sysctl -a | grep tcp
 net.ipv4.tcp_abort_on_overflow = 0
 net.ipv4.tcp_adv_win_scale = 1
 net.ipv4.tcp_allowed_congestion_control = reno cubic
 net.ipv4.tcp_app_win = 31
 net.ipv4.tcp_autocorking = 1
 net.ipv4.tcp_available_congestion_control = reno cubic
 net.ipv4.tcp_available_ulp =
 net.ipv4.tcp_base_mss = 1024
 net.ipv4.tcp_challenge_ack_limit = 1000
 net.ipv4.tcp_comp_sack_delay_ns = 1000000
 net.ipv4.tcp_comp_sack_nr = 44
 net.ipv4.tcp_congestion_control = cubic
 net.ipv4.tcp_dsack = 1
 [...]

在这个内核 （5.3） 上，有 70 个包含 “tcp” 的内核，在 “net” 下还有更多的内核，包括 IP、以太网、路由和网络接口的参数

其中一些设置可以基于每个插槽进行调整。例如，net.ipv4.tcp_congestion_ control 是系统范围的默认拥塞控制算法，可以使用 TCP_CONGESTION socket 选项为每个套接字设置该算法（请参见部分 10.8.2， 套接字选项）。

# 生产示例
下面显示了 Netflix 如何调整其云实例 [Gregg 19c];它在引导期间应用于启动脚本

# net.core.default_qdisc = fq
# net.core.netdev_max_backlog = 5000
# net.core.rmem_max = 16777216
# net.core.somaxconn = 1024
# net.core.wmem_max = 16777216
# net.ipv4.ip_local_port_range = 10240 65535
# net.ipv4.tcp_abort_on_overflow = 1
# net.ipv4.tcp_congestion_control = bbr
# net.ipv4.tcp_max_syn_backlog = 8192
# net.ipv4.tcp_rmem = 4096 12582912 16777216
# net.ipv4.tcp_slow_start_after_idle = 0
# net.ipv4.tcp_syn_retries = 2
# net.ipv4.tcp_tw_reuse = 1
# net.ipv4.tcp_wmem = 4096 12582912 16777216

这只设置了 14 个可能的可调参数，并且作为时间点示例提供，而不是配方。Netflix 正在考虑在 2020 年更新其中两个（将积压net.core.netdev_max_设置为 1000，将 net.core.somaxconn 设置为 4096），等待非回归测试

以下部分讨论各个可调参数

# 套接字和 TCP 缓冲区
所有协议类型的最大套接字缓冲区大小，包括读取 （rmem_max） 和写入 （wmem_max），都可以使用

# net.core.rmem_max = 16777216 
# net.core.wmem_max = 16777216 

该值以字节为单位。这可能需要设置为 16 MB 或更高，以支持全速 10 GbE 连接。

启用 TCP 接收缓冲区的自动调整

# net.ipv4.tcp_moderate_rcvbuf = 1

设置 TCP 读取和写入缓冲区的自动调整参数：

# net.ipv4.tcp_rmem = 4096 87380 16777216
# net.ipv4.tcp_wmem = 4096 65536 16777216

每个都有三个值：要使用的最小字节数、默认字节数和最大字节数。使用的大小是从默认值自动调整的。要提高 TCP 吞吐量，请尝试增加 maximum 值。增加 minimum 和 default 将为每个连接消耗更多内存，这可能不是必需的。

# TCP 积压
第一个积压队列，用于半开连接

# net.ipv4.tcp_max_syn_backlog = 4096

第二个 backlog 队列，侦听 backlog，用于将连接传递给 accept（2）：

# net.core.somaxconn = 1024

这两者都可能需要从默认值（例如）增加到 4,096 和 1,024 或更高，以更好地处理负载突发。

# 设备积压
增加每个 CPU 的网络设备积压队列的长度：

# net.core.netdev_max_backlog = 10000

对于 10 GbE NIC，这可能需要增加，例如增加到 10,000。

# TCP 拥塞控制
Linux 支持可插拔的拥塞控制算法。列出当前可用的

# sysctl net.ipv4.tcp_available_congestion_control
 net.ipv4.tcp_available_congestion_control = reno cubic

有些可能可用，但当前未加载。例如，添加 htcp

 # modprobe tcp_htcp
 # sysctl net.ipv4.tcp_available_congestion_control
 net.ipv4.tcp_available_congestion_control = reno cubic htcp

可以使用以下方法选择当前算法：

# net.ipv4.tcp_congestion_control = cubic

# TCP 选项
可以设置的其他 TCP 参数包括

# net.ipv4.tcp_sack = 1
# net.ipv4.tcp_fack = 1
# net.ipv4.tcp_tw_reuse = 1
# net.ipv4.tcp_tw_recycle = 0

SACK 和 FACK 扩展可以提高高延迟网络工作的吞吐量性能，但会消耗一些 CPU 负载。

tcp_tw_reuse 可调功能允许在看起来安全的情况下重复使用 TIME_WAIT 会话。这可以允许在两个主机之间（例如 Web 服务器和数据库之间）之间实现更高的连接速率，而不会在会话处于 TIME_WAIT 的情况下达到 16 位临时端口限制。

tcp_tw_recycle 是另一种重复使用 TIME_WAIT 会话的方法，尽管不如 tcp_tw_reuse 安全。

# ECN
显式拥塞通知可以使用

# net.ipv4.tcp_ecn = 1

值为 0 表示禁用 ECN，1 表示允许传入连接并在传出连接上请求 ECN，值为 2 表示允许传入但不在传出时请求 ECN。默认值为 2。

还有 net.ipv4.tcp_ecn_fallback，默认设置为 1 （true），如果内核检测到连接行为异常，它将禁用连接的 ECN。

# 字节队列限制
这可以通过 /sys 进行调整。显示这些限制的控制文件的内容（在此输出中截断的路径将因您的系统和接口而异）：

 # grep . /sys/devices/pci.../net/ens5/queues/tx-0/byte_queue_limits/limit*
 /sys/devices/pci.../net/ens5/queues/tx-0/byte_queue_limits/limit:16654
 /sys/devices/pci.../net/ens5/queues/tx-0/byte_queue_limits/limit_max:1879048192
 /sys/devices/pci.../net/ens5/queues/tx-0/byte_queue_limits/limit_min:0

此接口的限制为 16654 字节，由自动调整设置。要控制此值，请设置 limit_ min 和 limit_max 以限制可接受的范围。

# 资源控制
容器组 （cgroups） 网络优先级 （net_prio） 子系统可用于为进程或进程组的传出网络流量应用优先级。这可用于支持高优先级网络流量（如生产负载），而不是低优先级流量（如备份或监控）。还有网络分类器 （net_cls） cgroup，用于使用类 ID 标记属于 cgroup 的数据包：然后，这些 ID 可以由排队规则用于应用数据包或带宽限制，也可以由 BPF 程序使用。BPF 程序还可以使用其他信息（如 cgroup v2 ID）进行容器感知，并且可以通过将分类、测量和重新标记移动到 tc 出口钩子来提高可扩展性，从而减轻根 qdisc 锁的压力 [Fomichev 20]。

有关资源控制的更多信息，请参见第 11 章 云计算， 部分 11.3.3， 资源控制中的网络 I/O 标题

# Qisc（Queueing Disciplines）
如第 10.4.3 节 软件中所述，如图 10.8 所示，排队规则 （qdisc） 是用于调度、处理、过滤和调整网络数据包的算法。第 10.7.6 节 tc 展示了如何使用 netem qdisc 创建数据包丢失。还有各种 qdisc 可以提高不同工作负载的性能。您可以使用

# man -k tc-

每个 qdisc 都有自己的手册页。Qdiscs 可用于设置数据包速率或带宽策略、设置 IP ECN 标志等。

默认 qdisc 可以使用

# sysctl net.core.default_qdisc
 net.core.default_qdisc = fq_codel

许多 Linux 发行版已经切换到 fq_codel 作为默认发行版，因为它在大多数情况下都能提供良好的性能

# 调优项目
由于有这么多可用的可调参数，处理它们可能会很费力。Tuned Project 根据可选择的配置文件为其中一些可调参数提供自动调优，并支持 Linux 发行版，包括 RHEL、Fedora、Ubuntu 和 CentOS [Tuned Project 20]。安装 tuned 后，可以使用

 # tuned-adm list
 Available profiles:
 [...]- balanced                    - General non-specialized tuned profile
 [...]- network-latency             - Optimize for deterministic performance at the cost of 
increased power consumption, focused on low latency network performance- network-throughput          - Optimize for streaming network throughput, generally 
only necessary on older CPUs or 40G+ networks
 [...]

此输出被截断：完整列表显示 28 个用户档案。要激活 network-latency 配置文件，请执行以下作：

# tuned-adm profile network-latency

要查看此配置文件设置了哪些可调参数，可以从 tuned 源 [Škarvada 20] 读取其配置文件：

# $ more tuned/profiles/network-latency/tuned.conf 
[...]
 [main]
 summary=Optimize for deterministic performance at the cost of increased power 
consumption, focused on low latency network performance
 include=latency-performance
 [vm]
 transparent_hugepages=never
 [sysctl]
 net.core.busy_read=50
 net.core.busy_poll=50
 net.ipv4.tcp_fastopen=3
 kernel.numa_balancing=0
 [bootloader]
 cmdline_network_latency=skew_tick=1

请注意，这有一个 include 指令，该指令也包括 latency-performance 配置文件中的可调参数。

# #10.8.2 套接字选项
套接字可以由应用程序通过 setsockopt（2） 系统调用单独调整。只有当您正在开发或重新编译软件并且可以对源代码进行修改时，才有可能这样做。

setsockopt（2） 允许调整不同的层（例如，套接字、TCP）。表 10.8 显示了 Linux 上的一些优化可能性。

有关可用的套接字选项，请参见 socket（7）、tcp（7）、udp（7） 等的手册页

还有一些 socket I/O syscall 标志会影响性能。例如，Linux 4.14 为 send（2） 系统调用添加了 MSG_ZEROCOPY 标志：它允许在传输过程中使用用户空间缓冲区，以避免将其复制到内核空间 [Linux 20c] 的费用。

# #10.8.3 配置
以下配置选项也可用于调整网络性能： 
■ 以太网巨型帧：如果网络基础设施支持巨型帧，则将默认 MTU 从 1,500 增加到 ~9,000 可以提高网络吞吐量性能。
■ 链路聚合：可以将多个网络接口组合在一起，以便它们作为一个网络接口使用组合带宽。这需要交换机支持和配置才能正常工作。
■ 防火墙配置：例如，出口钩子上的 iptables 或 BPF 程序可用于根据防火墙规则在 IP 标头中设置 IP ToS （DSCP） 级别。这可用于根据端口以及其他使用案例确定流量的优先级。

# 10.9 练习
1. 回答以下有关网络术语的问题： 
■ 带宽和吞吐量有什么区别？
■ 什么是 TCP 连接延迟？
■ 什么是首字节延迟？
■ 什么是往返时间？

2. 回答以下概念性问题： 
■ 描述网络接口利用率和饱和度。
■ 什么是 TCP 侦听积压，如何使用它？
■ 描述中断合并的优缺点

3. 回答以下更深层次的问题： 
■ 对于 TCP 连接，解释网络帧（或数据包）错误如何损害性能。
■ 描述当网络接口工作过载时会发生什么情况，包括对应用程序性能的影响。

4. 为您的作系统制定以下程序： 
■ 网络资源（网络接口和控制器）的 USE 方法清单。包括如何获取每个指标（例如，要执行哪个命令）以及如何解释结果。在安装或使用其他软件产品之前，请尝试使用现有的作系统可观测性工具。

5. 网络资源的工作负载特征清单。包括如何执行这些任务（可能需要使用动态跟踪）： ■ 测量出站（活动）TCP 连接的第一个字节延迟。
■ 测量 TCP 连接延迟。该脚本应该处理非阻塞的 connect（2） 调用。

6. （可选，高级）测量 RX 和 TX 的 TCP/IP 堆栈间延迟。对于 RX，这测量从中断到套接字读取的时间;对于 TX，为从套接字写入设备传输的时间。在负载下测试。是否可以包含其他信息来解释任何延迟异常值的原因？

# 10.10 引用
 [Postel 80] Postel, J., “RFC 768: User Datagram Protocol,” Information Sciences Institute, 
https://tools.ietf.org/html/rfc768, 1980.
 [Postel 81] Postel, J., “RFC 793: Transmission Control Protocol,” Information Sciences Institute, 
https://tools.ietf.org/html/rfc768, 1981.
 [Nagle 84] Nagle, J., “RFC 896: Congestion Control in IP/TCP Internetworks,” https://
 tools.ietf.org/html/rfc896,1984.
 [Saltzer 84] Saltzer, J., Reed, D., and Clark, D., “End-to-End Arguments in System Design,” 
ACM TOCS, November 1984.
 [Braden 89] Braden, R., “RFC 1122: Requirements for Internet Hosts—Communication 
Layers,” https://tools.ietf.org/html/rfc1122, 1989.
 [Jacobson 92] Jacobson, V., et al., “TCP Extensions for High Performance,” Network Working 
Group, https://tools.ietf.org/html/rfc1323, 1992.
 [Stevens 93] Stevens, W. R., TCP/IP Illustrated, Volume 1, Addison-Wesley, 1993.
 [Mathis 96] Mathis, M., and Mahdavi, J., “Forward Acknowledgement: Refining TCP 
Congestion Control,” ACM SIGCOMM, 1996.
 [Jacobson 97] Jacobson, V., “pathchar-a1-linux-2.0.30.tar.gz,” ftp://ftp.ee.lbl.gov/pathchar, 
1997.
 [Nichols 98] Nichols, K., Blake, S., Baker, F., and Black, D., “Definition of the Differentiated 
Services Field (DS Field) in the IPv4 and IPv6 Headers,” Network Working Group, https://
 tools.ietf.org/html/rfc2474, 1998.
 [Downey 99] Downey, A., “Using pathchar to Estimate Internet Link Characteristics,” ACM 
SIGCOMM, October 1999.
 [Ramakrishnan 01] Ramakrishnan, K., Floyd, S., and Black, D., “The Addition of Explicit 
Congestion Notification (ECN) to IP,” Network Working Group, https://tools.ietf.org/html/
 rfc3168, 2001.
 [Corbet 03] Corbet, J., “Driver porting: Network drivers,” LWN.net, https://lwn.net/
 Articles/30107, 2003.
 [Hassan 03] Hassan, M., and R. Jain., High Performance TCP/IP Networking, Prentice Hall, 
2003.
 [Deri 04] Deri, L., “Improving Passive Packet Capture: Beyond Device Polling,” Proceedings of 
SANE, 2004.
 [Corbet 06b] Corbet, J., “Reworking NAPI,” LWN.net, https://lwn.net/Articles/214457, 2006
  [Cook 09] Cook, T., “nicstat - the Solaris and Linux Network Monitoring Tool You Did Not 
Know You Needed,” https://blogs.oracle.com/timc/entry/nicstat_the_solaris_and_linux, 
2009.
 [Steenbergen 09] Steenbergen, R., “A Practical Guide to (Correctly) Troubleshooting with 
Traceroute,” https://archive.nanog.org/meetings/nanog47/presentations/Sunday/RAS_
 Traceroute_N47_Sun.pdf, 2009.
 [Paxson 11] Paxson, V., Allman, M., Chu, J., and Sargent, M., “RFC 6298: Computing TCP's 
Retransmission Timer,” Internet Engineering Task Force (IETF), https://tools.ietf.org/html/
 rfc6298, 2011.
 [Corbet 12] “TCP friends,” LWN.net, https://lwn.net/Articles/511254, 2012.
 [Fritchie 12] Fritchie, S. L., “quoted,” https://web.archive.org/web/20120119110658/http://
 www.snookles.com/slf-blog/2012/01/05/tcp-incast-what-is-it, 2012.
 [Hrubý 12] Hrubý, T., “Byte Queue Limits,” Linux Plumber’s Conference, https://
 blog.linuxplumbersconf.org/2012/wp-content/uploads/2012/08/bql_slide.pdf, 2012.
 [Nichols 12] Nichols, K., and Jacobson, V., “Controlling Queue Delay,” Communications of the 
ACM, July 2012.
 [Roskind 12] Roskind, J., “QUIC: Quick UDP Internet Connections,” https://docs.google.
 com/document/d/1RNHkx_VvKWyWg6Lr8SZ-saqsQx7rFV-ev2jRFUoVD34/edit#, 2012.
 [Dukkipati 13] Dukkipati, N., Cardwell, N., Cheng, Y., and Mathis, M., “Tail Loss Probe 
(TLP): An Algorithm for Fast Recovery of Tail Losses,” TCP Maintenance Working Group, https://
 tools.ietf.org/html/draft-dukkipati-tcpm-tcp-loss-probe-01, 2013.
 [Siemon 13] Siemon, D., “Queueing in the Linux Network Stack,” https://www.coverfire.
 com/articles/queueing-in-the-linux-network-stack, 2013.
 [Cheng 16] Cheng, Y., and Cardwell, N., “Making Linux TCP Fast,” netdev 1.2, https://
 netdevconf.org/1.2/papers/bbr-netdev-1.2.new.new.pdf, 2016.
 [Linux 16] “TCP Offload Engine (TOE),” https://wiki.linuxfoundation.org/networking/toe, 
2016.
 [Ather 17] Ather, A., “BBR TCP congestion control offers higher network utilization and 
throughput during network congestion (packet loss, latencies),” https://twitter.com/
 amernetflix/status/892787364598132736, 2017.
 [Bensley 17] Bensley, S., et al., “Data Center TCP (DCTCP): TCP Congestion Control for Data 
Centers,” Internet Engineering Task Force (IETF), https://tools.ietf.org/html/rfc8257, 2017.
 [Dumazet 17a] Dumazet, E., “Busy Polling: Past, Present, Future,” netdev 2.1, https://
 netdevconf.info/2.1/slides/apr6/dumazet-BUSY-POLLING-Netdev-2.1.pdf, 2017.
 [Dumazet 17b] Dumazet, E., “Re: Something hitting my total number of connections to 
the server,” netdev mailing list, https://lore.kernel.org/netdev/1503423863.2499.39.camel@
 edumazet-glaptop3.roam.corp.google.com, 2017
  [Gallatin 17] Gallatin, D., “Serving 100 Gbps from an Open Connect Appliance,” Netflix 
Technology Blog, https://netflixtechblog.com/serving-100-gbps-from-an-open-connect
appliance-cdb51dda3b99, 2017.
 [Bruijn 18] Bruijn, W., and Dumazet, E., “Optimizing UDP for Content Delivery: GSO, 
Pacing and Zerocopy,” Linux Plumber’s Conference, http://vger.kernel.org/lpc_net2018_talks/
 willemdebruijn-lpc2018-udpgso-paper-DRAFT-1.pdf, 2018.
 [Corbet 18b] Corbet, J., “Zero-copy TCP receive,” LWN.net, https://lwn.net/Articles/752188, 
2018.
 [Corbet 18c] Corbet, J., “Time-based packet transmission,” LWN.net, https://lwn.net/
 Articles/748879, 2018.
 [Deepak 18] Deepak, A., “eBPF / XDP firewall and packet filtering,” Linux Plumber’s 
Conference, http://vger.kernel.org/lpc_net2018_talks/ebpf-firewall-LPC.pdf, 2018.
 [Jacobson 18] Jacobson, V., “Evolving from AFAP: Teaching NICs about Time,” netdev 
0x12, July 2018, https://www.files.netdevconf.org/d/4ee0a09788fe49709855/files/?p=/
 Evolving%20from%20AFAP%20%E2%80%93%20Teaching%20NICs%20about%20time
 .pdf, 2018.
 [Høiland-Jørgensen 18] Høiland-Jørgensen, T., et al., “The eXpress Data Path: Fast 
Programmable Packet Processing in the Operating System Kernel,” Proceedings of the 14th 
International Conference on emerging Networking EXperiments and Technologies, 2018.
 [HP 18] “Netperf,” https://github.com/HewlettPackard/netperf, 2018.
 [Majkowski 18] Majkowski, M., “How to Drop 10 Million Packets per Second,” https://
 blog.cloudflare.com/how-to-drop-10-million-packets, 2018.
 [Stewart 18] Stewart, R., “This commit brings in a new refactored TCP stack called Rack,” 
https://reviews.freebsd.org/rS334804, 2018.
 [Amazon 19] “Announcing Amazon VPC Traffic Mirroring for Amazon EC2 Instances,” 
https://aws.amazon.com/about-aws/whats-new/2019/06/announcing-amazon-vpc
traffic-mirroring-for-amazon-ec2-instances, 2019.
 [Dumazet 19] Dumazet, E., “Re: [LKP] [net] 19f92a030c: apachebench.requests_per_s
 econd -37.9% regression,” netdev mailing list, https://lore.kernel.org/lkml/20191113172102.
 GA23306@1wt.eu, 2019.
 [Gregg 19] Gregg, B., BPF Performance Tools: Linux System and Application Observability, 
Addison-Wesley, 2019.
 [Gregg 19b] Gregg, B., “BPF Theremin, Tetris, and Typewriters,” http://
 www.brendangregg.com/blog/2019-12-22/bpf-theremin.html, 2019.
 [Gregg 19c] Gregg, B., “LISA2019 Linux Systems Performance,” USENIX LISA, http://
 www.brendangregg.com/blog/2020-03-08/lisa2019-linux-systems-performance.html, 2019.
 [Gregg 19d] Gregg, B., “udplife.bt,” https://github.com/brendangregg/bpf-perf-tools-book/
 blob/master/exercises/Ch10_Networking/udplife.bt, 2019
  [Hassas Yeganeh 19] Hassas Yeganeh, S., and Cheng, Y., “TCP SO_TIMESTAMPING with 
OPT_STATS for Performance Analytics,” netdev 0x13, https://netdevconf.info/0x13/session.
 html?talk-tcp-timestamping, 2019.
 [Bufferbloat 20] “Bufferbloat,” https://www.bufferbloat.net, 2020.
 [Cheng 20] Cheng, Y., Cardwell, N., Dukkipati, N., and Jha, P., “RACK-TLP: A Time-Based 
Efficient Loss Detection for TCP,” TCP Maintenance Working Group, https://tools.ietf.org/html/
 draft-ietf-tcpm-rack-09, 2020.
 [Cilium 20a] “API-aware Networking and Security,” https://cilium.io, accessed 2020.
 [Corbet 20] Corbet, J., “Kernel operations structures in BPF,” LWN.net, https://lwn.net/
 Articles/811631, 2020.
 [DPDK 20] “AF_XDP Poll Mode Driver,” DPDK documentation, http://doc.dpdk.org/guides/
 index.html, accessed 2020.
 [Fomichev 20] Fomichev, S., et al., “Replacing HTB with EDT and BPF,” netdev 0x14, 
https://netdevconf.info/0x14/session.html?talk-replacing-HTB-with-EDT-and-BPF, 2020.
 [Google 20b] “Packet Mirroring Overview,” https://cloud.google.com/vpc/docs/packet
mirroring, accessed 2020.
 [Høiland-Jørgensen 20] Høiland-Jørgensen, T., “The FLExible Network Tester,” 
https://flent.org, accessed 2020.
 [Linux 20i] “Segmentation Offloads,” Linux documentation, https://www.kernel.org/doc/
 Documentation/networking/segmentation-offloads.rst, accessed 2020.
 [Linux 20c] “MSG_ZEROCOPY,” Linux documentation, https://www.kernel.org/doc/html/
 latest/networking/msg_zerocopy.html, accessed 2020.
 [Linux 20j] “timestamping.txt,” Linux documentation, https://www.kernel.org/doc/
 Documentation/networking/timestamping.txt, accessed 2020.
 [Linux 20k] “AF_XDP,” Linux documentation, https://www.kernel.org/doc/html/latest/
 networking/af_xdp.html, accessed 2020.
 [Linux 20l] “HOWTO for the Linux Packet Generator,” Linux documentation, 
https://www.kernel.org/doc/html/latest/networking/pktgen.html, accessed 2020.
 [Nosachev 20] Nosachev, D., “How 1500 Bytes Became the MTU of the Internet,” 
https://blog.benjojo.co.uk/post/why-is-ethernet-mtu-1500, 2020.
 [Škarvada 20] Škarvada, J., “network-latency/tuned.conf,” https://github.com/redhat
performance/tuned/blob/master/profiles/network-latency/tuned.conf, last updated 2020.
 [Tuned Project 20] “The Tuned Project,” https://tuned-project.org, accessed 2020.
 [Wireshark 20] “Wireshark,” https://www.wireshark.org, accessed 2020.
