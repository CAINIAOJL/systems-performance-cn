# Chapter 8: 文件系统
对于应用程序来说，文件系统性能通常比磁盘或存储设备更重要，因为它是应用程序与之交互和等待的文件系统。文件系统可以使用缓存、缓冲和异步 I/O 来避免应用程序受到磁盘级（或远程存储系统）延迟的影响。

系统性能分析和监控工具历来关注磁盘性能，使文件系统性能成为一个盲点。本章阐明了文件系统，展示了它们的工作原理以及如何测量它们的延迟和其他详细信息。这通常可以排除文件系统及其底层磁盘设备是性能不佳的根源，从而允许调查转移到其他方面

本章的学习目标是：

■ 了解文件系统模型和概念。
■ 了解文件系统工作负载如何影响性能。
■ 熟悉文件系统缓存。
■ 熟悉文件系统内部和性能功能。
■ 遵循各种方法进行文件系统分析。
■ 测量文件系统延迟以识别模式和异常值。
■ 使用跟踪工具调查文件系统使用情况。
■ 使用微基准测试测试文件系统性能。
■ 了解文件系统可调参数。

本章由六个部分组成，前三个部分为文件系统分析提供基础，后三个部分介绍其在基于 Linux 的系统中的实际应用。这些部分如下：

■ 背景介绍了与文件系统相关的术语和基本模型，说明了文件系统原理和关键的文件系统性能概念。■ 架构 介绍通用和特定的文件系统架构。
■ 方法描述了性能分析方法，包括观察性和实验性。
■ Observability Tools （可观测性工具） 显示基于 Linux 的系统的文件系统可观测性工具，包括静态和动态检测。
■ 实验总结了文件系统基准测试工具。
■ 优化描述了文件系统可调参数。

# 8.1 术语
作为参考，本章中使用的与文件系统相关的术语包括

■ 文件系统：将数据组织为文件和目录，具有基于文件的界面用于访问它们，以及用于控制访问的文件权限。其他内容可能包括设备、套接字和管道的特殊文件类型，以及包括文件访问时间戳在内的元数据。
■ 文件系统缓存：用于缓存文件系统内容的主内存（通常是 DRAM）区域，其中可能包括各种数据和元数据类型的不同缓存。
■ operations：文件系统作是文件系统的请求，包括 read（2）、write（2）、open（2）、close（2）、stat（2）、mkdir（2） 等操作。
■ I/O：输入/输出。文件系统 I/O 可以通过多种方式定义;这里它只表示直接读写（执行 I/O）的作，包括 read（2）、write（2）、stat（2）（读取统计信息）和 mkdir（2）（写入新的目录条目）。I/O 不包括 open（2） 和 close（2）（尽管这些调用会更新元数据并可能导致间接磁盘 I/O）
■ 逻辑 I/O：应用程序向文件系统发出的 I/O。
■ 物理 I/O：由文件系统（或通过原始 I/O）直接向磁盘发出的 I/O。
■ 块大小：也称为记录大小，是文件系统磁盘数据组的大小。请参见部分 8.4.4， 文件系统功能中的块与范围。
■ 吞吐量：应用程序和文件系统之间的当前数据传输速率，以每秒字节数为单位。
■ inode：索引节点 （inode） 是一种数据结构，包含文件系统对象的元数据，包括权限、时间戳和数据指针。
■ VFS：虚拟文件系统，用于抽象和支持不同文件系统类型的内核接口
■ 卷：一种存储实例，比使用整个存储设备提供更大的灵活性。卷可以是设备的一部分，也可以是多个设备。
■ 卷管理器：用于以灵活的方式管理物理存储设备，创建供操作系统使用的虚拟卷的软件。

本章介绍了其他术语。词汇表包括供参考的基本术语，包括 fsck 、 IOPS 、作速率和 POSIX 。另请参阅第 2 章和第 3 章中的术语部分。

# 8.2 Models
以下简单模型说明了文件系统的一些基本原理及其性能。

# #8.2.1 文件系统接口
图 8.1 显示了文件系统的基本模型，就其接口而言

图中还标记了发生逻辑和物理操作的位置。有关这些的更多信息，请参见第 8.3.12 节 逻辑 I/O 与物理 I/O 的比较。

图 8.1 显示了通用对象操作。内核可以实现其他变体：例如，Linux 提供 readv（2）、writev（2）、openat（2） 等。

研究文件系统性能的一种方法是将其视为黑盒，重点关注对象作的延迟。部分 8.5.2， 延迟分析中对此进行了更详细的解释。

# #8.2.2 文件系统缓存
存储在主内存中的通用文件系统缓存如图 8.2 所示，它为读取作提供服务。

读取从缓存 （缓存命中） 或磁盘 （缓存未命中） 返回数据。缓存未命中存储在缓存中，填充缓存（预热）。

文件系统缓存还可以缓冲写入，以便稍后写入（刷新）。执行此作的机制因文件系统类型而异，如部分 8.4 体系结构中所述。

如果需要，内核通常提供一种绕过缓存的方法。请参见部分 8.3.8， 原始和直接 I/O。

# #8.2.3 二级缓存
二级缓存可以是任何内存类型;图 8.3 将其显示为闪存。这种缓存类型最初是我自己在 2007 年为 ZFS 开发的。

# 8.3 概念
以下是一些重要的文件系统性能概念。

# #8.3.1 文件系统延迟
文件系统延迟是文件系统性能的主要指标，以从逻辑文件系统请求到完成的时间来衡量。它包括在文件系统和磁盘 I/O 子系统中花费的时间，以及在磁盘设备（物理 I/O）上等待的时间。应用程序线程通常会在应用程序请求期间阻塞，以便等待文件系统请求完成，因此，文件系统延迟会直接按比例影响应用程序性能

应用程序可能不会受到直接影响的情况包括使用非阻塞 I/O、预取（第 8.3.4 节）以及从异步线程（例如，后台刷新线程）发出 I/O 时。如果应用程序提供其文件系统使用情况的详细指标，则可以从应用程序中识别这些情况。如果没有，一种通用的方法是使用内核跟踪工具，它可以显示导致逻辑文件系统 I/O 的用户级堆栈跟踪。然后可以研究此堆栈跟踪以查看哪些应用程序例程发出了它。

操作系统历来没有使文件系统延迟易于观察，而是提供磁盘设备级统计数据。但在许多情况下，此类统计信息与应用程序性能无关，并且它们也具有误导性。这方面的一个例子是文件系统对写入数据执行后台刷新，这可能表现为高延迟磁盘 I/O 的突发。从磁盘设备级别的统计信息来看，这看起来令人担忧;但是，没有应用程序等待这些完成。有关更多情况，请参见第 8.3.12 节 逻辑 I/O 与物理 I/O。

# #8.3.2 缓存
文件系统通常使用主内存 （RAM） 作为缓存来提高性能。对于应用程序，此过程是透明的：应用程序逻辑 I/O 延迟变得更低（更好），因为它可以从主内存而不是速度慢得多的磁盘设备提供。

随着时间的推移，缓存会增长，而操作系统的可用内存会缩小。这可能会让新用户感到警觉，但这是完全正常的。原则是：如果有备用的主内存，就用它做一些有用的事情。当应用程序需要更多内存时，内核应迅速将其从文件系统缓存中释放出来以供使用。

文件系统使用缓存来提高读取性能，使用缓冲（在缓存中）来提高写入性能。文件系统和 block device 子系统通常使用多种类型的缓存，其中可能包括 Table 8.1 中的那些。

部分 8.4， 体系结构中介绍了特定的高速缓存类型，而第 3 章操作系统则提供了高速缓存的完整列表（包括应用程序级和设备级）。

# #8.3.3 随机VS顺序I/O
根据每个 I/O 的文件偏移量，一系列逻辑文件系统 I/O 可以描述为随机或顺序。对于顺序 I/O，每个 I/O 偏移量都从前一个 I/O 的末尾开始。随机 I/O 之间没有明显的关系，偏移量是随机变化的。随机文件系统工作负载也可以指随机访问许多不同的文件

图 8.4 说明了这些访问模式，显示了一系列有序的 I/O 和示例文件偏移量

由于某些存储设备的性能特征（在第 9 章 磁盘中介绍），文件系统历来尝试通过按顺序和连续地将文件数据放置在磁盘上来减少随机 I/O。术语碎片描述了文件系统在这方面做得很差的情况，导致文件分散在驱动器上，从而使顺序逻辑 I/O 产生随机的物理 I/O。

文件系统可以测量逻辑 I/O 访问模式，以便它们可以识别顺序工作负载，然后使用预取或预读来提高其性能。这对旋转盘很有帮助;闪存驱动器则不然

# #8.3.4 预取
常见的文件系统工作负载涉及按顺序读取大量文件数据，例如，用于文件系统备份。此数据可能太大而无法放入缓存中，或者它可能只读取一次并且不太可能保留在缓存中（取决于缓存驱逐策略）。此类工作负载的性能相对较差，因为它的缓存命中率较低。

预取是解决此问题的常见文件系统功能。它可以根据当前和以前的文件 I/O 偏移量检测顺序读取工作负载，然后在应用程序请求磁盘读取之前预测并发出磁盘读取。这将填充文件系统缓存，因此，如果应用程序确实执行了预期的读取，则会导致缓存命中（所需的数据已在缓存中）。示例场景如下：

1. 应用程序发出一个文件 read（2），将执行传递给内核。
2. 数据未缓存，因此文件系统将读取问题发送到磁盘。
3. 将前一个文件偏移指针与当前位置进行比较，如果它们是连续的，则文件系统会发出额外的读取（预取）。
4. 第一次读取完成，内核将数据和执行传递回应用程序。
5. 所有预取读取都已完成，填充缓存以供将来的应用程序读取。
6. 未来的顺序应用程序读取通过 RAM 中的缓存快速完成

图 8.5 也说明了这种情况，其中应用程序读取偏移量 1 和 2 触发接下来三个偏移量的预取。

当预取检测运行良好时，应用程序的顺序读取性能会显著提高;磁盘会领先于应用程序请求（前提是它们有足够的带宽来执行此作）。当预取检测效果不佳时，会发出应用程序不需要的不必要的 I/O，从而污染高速缓存并消耗磁盘和 I/O 传输资源。文件系统通常允许根据需要调整预取。

# #8.3.5 预读
从历史上看，预取也称为预读。Linux 使用预读术语 readahead（2） 进行系统调用，它允许应用程序显式预热文件系统缓存

# #8.3.6 回写缓存
文件系统通常使用回写缓存来提高写入性能。它的工作原理是在传输到主内存后将写入视为已完成，并在一段时间后异步将它们写入磁盘。将此 “脏” 数据写入磁盘的文件系统过程称为 flushing。示例序列如下：

1. 应用程序发出文件 write（2），将执行传递给内核。
2. 将应用程序地址空间中的数据复制到内核。
3. 内核将 write（2） syscall 视为已完成，并将执行传递回应用程序。
4. 一段时间后，异步内核任务会找到写入的数据并发出磁盘写入问题。

权衡是可靠性。基于 DRAM 的主内存具有易失性，如果发生电源故障，脏数据可能会丢失。数据也可能不完整地写入磁盘，从而留下损坏的磁盘状态。

如果文件系统元数据损坏，文件系统可能无法再加载。这种状态可能只能从系统备份中恢复，从而导致停机时间延长。更糟糕的是，如果损坏影响了应用程序读取和使用的文件内容，则业务可能会处于危险之中。

为了平衡速度和可靠性的需求，文件系统可以默认提供回写缓存，并提供同步写入选项以绕过此行为并直接写入持久性存储设备。

# #8.3.7 同步写入
同步写入仅在完全写入持久性存储（例如磁盘设备）时完成，其中包括写入任何必要的文件系统元数据更改。这些比异步写入（回写缓存）慢得多，因为同步写入会导致磁盘设备 I/O 延迟（并且由于文件系统元数据，可能会有多个 I/O）。某些应用程序（如数据库日志写入器）使用同步写入，这些应用程序不可接受异步写入导致数据损坏的风险。

同步写入有两种形式：单个 I/O （同步写入）和先前写入组 （同步提交）。

# 单个同步写入
当使用标志 O_SYNC 或变体之一 O_DSYNC 和 O_RSYNC 打开文件时，写入 I/O 是同步的（从 Linux 2.6.31 开始，它们由 glibc 映射到 O_SYNC）。某些文件系统具有挂载选项，以强制对所有文件的所有写入 I/O 都是同步的

# 同步提交以前的写入
应用程序可以使用 fsync（2） 系统调用在其代码的检查点同步提交以前的异步写入，而不是同步写入单个 I/O。这可以通过对写入进行分组来提高性能，还可以通过使用写入取消来避免多个元数据更新

还有其他情况将提交以前的写入，例如关闭文件句柄，或者当文件上有太多未提交的缓冲区时。前者通常很明显，因为在解压缩包含许多文件的存档时，尤其是在 NFS 上。

# #8.3.8 原始 I/O 和直接 I/O
这些是应用程序可以使用的其他类型的 I/O（如果内核或文件系统支持）

原始 I/O 直接发送到磁盘偏移量，完全绕过文件系统。它已被一些应用程序（尤其是数据库）使用，这些应用程序可以比文件系统缓存更好地管理和缓存自己的数据。缺点是软件更复杂，并且管理困难：常规文件系统工具集不能用于备份/还原或可观察性。

Direct I/O 允许应用程序使用文件系统，但绕过文件系统缓存，例如，在 Linux 上使用 O_DIRECT open（2） 标志。这类似于同步写入（但没有 O_SYNC 提供的保证），也适用于读取。它不像原始设备 I/O 那样直接，因为文件偏移量到磁盘偏移量的映射仍必须由文件系统代码执行，并且 I/O 也可以调整大小以匹配文件系统用于磁盘布局的大小（其记录大小），否则可能会出错 （EINVAL）。根据文件系统的不同，这不仅可能禁用读取缓存和写入缓冲，还可能禁用预取

# #8.3.9 非阻塞I/O
通常，文件系统 I/O 将立即完成（例如，从缓存中）或等待后完成（例如，对于磁盘设备 I/O）。如果需要等待，应用程序线程将阻塞并离开 CPU，

允许其他线程在等待时执行。虽然被阻止的线程无法执行其他工作，但这通常不是问题，因为多线程应用程序可以创建额外的线程来执行，而某些线程被阻止。

在某些情况下，非阻塞 I/O 是可取的，例如，当避免线程创建的性能或资源开销时。非阻塞 I/O 可以通过对 open（2） 系统调用使用 O_NONBLOCK 或 O_NDELAY 标志来执行，这会导致读取和写入返回 EAGAIN 错误，而不是阻塞，阻塞告诉应用程序稍后再试。（对此的支持取决于文件系统，它可能仅对建议或强制文件锁定遵循非阻塞。

操作系统还可以提供单独的异步 I/O 接口，例如 aio_read（3） 和 aio_ write（3）。Linux 5.1 添加了一个名为 io_uring 的新异步 I/O 接口，提高了易用性、效率和性能 [Axboe 19]。

非阻塞 I/O 也在第 5 章 应用程序， 第 5.2.6 节 非阻塞 I/O 中讨论过

# #8.3.10 内存映射文件
对于某些应用程序和工作负载，可以通过将文件映射到进程地址空间并直接访问内存偏移量来提高文件系统 I/O 性能。这避免了在调用 read（2） 和 write（2） 系统调用来访问文件数据时产生的系统调用执行和上下文切换开销。如果内核支持直接将文件数据缓冲区映射到进程地址空间，它还可以避免数据的双重复制。

内存映射是使用 mmap（2） 系统调用创建的，并使用 munmap（2） 删除的。映射可以使用 madvise（2） 进行调整，如 第 8.8 节 调整 中所述。某些应用程序提供了在其配置中使用 mmap 系统调用（可能称为“mmap 模式”）的选项。例如，Riak 数据库可以将 mmap 用于其内存中的数据存储

我注意到一种倾向，即尝试使用 mmap（2） 来解决文件系统性能问题，而不先分析它们。如果问题是磁盘设备的高 I/O 延迟，那么当高磁盘 I/O 延迟仍然占主导地位时，使用 mmap（2） 避免小的 syscall 开销可能收效甚微

在多处理器系统上使用映射的一个缺点可能是保持每个 CPU MMU 同步的开销，特别是用于删除映射的 CPU 交叉调用（TLB 击降）。根据内核和映射，可以通过延迟 TLB 更新（惰性击降）来最小化这些 [Vahalia 96]。

# #8.3.11 元数据
数据描述文件和目录的内容，而元数据描述有关它们的信息。元数据可以引用可从文件系统接口 （POSIX） 读取的信息，或实现文件系统磁盘布局所需的信息。这些元数据分别称为逻辑元数据和物理元数据。

# 逻辑元数据
逻辑元数据是使用者（应用程序）读取和写入文件系统的信息，可以是：

■ 显式：读取文件统计信息 （stat（2））、创建和删除文件 （creat（2）、unlink（2）） 和目录 （mkdir（2）、rmdir（2））、设置文件属性 （chown（2）、chmod（2））
■ 隐式：文件系统访问时间戳更新、目录修改时间戳更新、已用块位图更新、可用空间统计信息

“元数据繁重”的工作负载通常是指逻辑元数据，例如，对文件进行 stat（2）作以确保它们在缓存后未更改的 Web 服务器，其速率比读取文件数据内容要快得多。

# 物理元数据
物理元数据是指记录所有文件系统信息所需的磁盘布局元数据。使用的元数据类型取决于文件系统，可能包括超级块、inode、数据指针块（主、辅助等）和空闲列表。

逻辑元数据和物理元数据是逻辑 I/O 和物理 I/O 之间存在差异的一个原因

# #8.3.12 逻辑I/OVS物理I/O
尽管这似乎有悖常理，但出于多种原因，应用程序对文件系统请求的 I/O （逻辑 I/O） 可能与磁盘 I/O （物理 I/O） 不匹配。

文件系统的作用远不止将持久存储（磁盘）呈现为基于文件的接口。它们缓存读取、缓冲区写入、将文件映射到地址空间，并创建额外的 I/O 来维护他们需要记录所有内容位置的磁盘上物理布局元数据。与应用程序 I/O 相比，这可能会导致磁盘 I/O 不相关、间接、隐式、膨胀或收缩。

# 不相关的
这是与应用程序无关的磁盘 I/O，可能是由于：

■ 其他应用程序 
■ 其他租户：磁盘 I/O 来自另一个云租户（在某些虚拟化技术下，通过系统工具可见）。
■ 其他内核任务：例如，当内核重建软件 RAID 卷或执行异步文件系统校验和验证时（请参见部分 8.4 体系结构）。
■ 管理任务：如备份

# 间接
这是由应用程序引起的磁盘 I/O，但没有立即对应的应用程序 I/O。这可能是由于：

■ 文件系统预取：添加应用程序可能使用也可能不使用的其他 I/O。
■ 文件系统缓冲：使用回写缓存来延迟和合并写入，以便以后刷新到磁盘。某些系统可能会在写入之前将写入缓冲数十秒，然后表现为不频繁的大突发。

# 含蓄
这是由应用程序事件直接触发的磁盘 I/O，而不是显式文件系统读取和写入，例如：

内存映射加载/存储：对于内存映射 （mmap（2）） 文件，加载和存储指令可能会触发磁盘 I/O 来读取或写入数据。写入可以缓冲并在以后写入。当分析文件系统作（read（2）、write（2））并且无法找到 I/O 的来源（因为它是由指令而不是系统调用触发）时，这可能会令人困惑。

# 缩减
小于应用程序 I/O 的磁盘 I/O，甚至不存在。这可能是由于：
■ 文件系统缓存：满足从主内存而不是磁盘的读取。
■ 文件系统写入取消：相同的字节偏移量在刷新到磁盘一次之前会多次修改。
■ 压缩：将数据量从逻辑 I/O 减少到物理 I/O。
■ 合并：在将顺序 I/O 发送到磁盘之前合并它们（这会减少 I/O 计数，但不会减少总大小）。
■ 内存文件系统：可能永远不会写入磁盘的内容（例如 tmpfs1）。

# 膨胀
大于应用程序 I/O 的磁盘 I/O。这是典型的情况，因为： 
■ 文件系统元数据：添加额外的 I/O。
■ 文件系统记录大小：四舍五入 I/O 大小（膨胀字节）或分段 I/O（膨胀计数）。
■ 文件系统日志：如果使用，则可以将磁盘写入翻倍，一次写入日志，另一次写入最终目标。
■ 卷管理器奇偶校验：读取-修改-写入周期，添加额外的 I/O。
■ RAID 膨胀：将额外的奇偶校验数据或数据写入镜像卷。

# 例子
为了说明这些因素如何同时发生，以下示例描述了 1 字节应用程序可以写入的内容：

1. 应用程序对现有文件执行 1 字节的写入作。
2. 文件系统将位置标识为 128 KB 文件系统记录的一部分，该记录未缓存（但引用它的元数据是缓存）。
3. 文件系统请求从磁盘加载记录。
4. 磁盘设备层将 128 KB 的读取分解为适合设备的较小读取。
5. 磁盘执行多个较小的读取，总计 128 KB。
6. 文件系统现在将记录中的 1 字节替换为新字节。
7. 一段时间后，文件系统或内核请求将 128 KB 的脏记录写回磁盘。
8. 磁盘写入 128 KB 的记录（如果需要，可以分解）。
9. 文件系统写入新的元数据，例如，更新引用（用于写入时复制）或访问时间。
10. 磁盘执行更多写入操作。

因此，虽然应用程序只执行一次 1 字节的写入，但磁盘执行了多次读取（总共 128 KB）和更多写入（超过 128 KB）。

# #8.3.13 操作并不相等
从前面的部分可以清楚地看出，文件系统操作可以根据其类型表现出不同的性能。仅从速率来看，您无法判断 “500 次作/秒” 的工作负载。某些操作可能会以主内存速度从文件系统缓存返回;其他 Cookie 可能会从磁盘返回，并且速度要慢几个数量级。其他决定因素包括作是随机的还是顺序的、读取还是写入、同步写入还是异步写入、它们的 I/O 大小、它们是否包括其他作类型、它们的 CPU 执行成本（以及系统的 CPU 负载程度）以及存储设备特征。

通常的做法是对不同的文件系统作进行微基准测试，以确定这些性能特征。例如，表 8.2 中的结果来自其他空闲的 Intel Xeon 2.4 GHz 多核处理器上的 ZFS 文件系统。

这些测试不涉及存储设备，而是对文件系统软件和 CPU 速度的测试。某些特殊文件系统从不访问持久性存储设备。

这些测试也是单线程的。并行 I/O 性能可能会受到正在使用的文件系统锁的类型和组织的影响。

# #8.3.14 特殊文件系统
文件系统的意图通常是持久存储数据，但在 Linux 上有一些特殊的文件系统类型用于其他目的，包括临时文件 （/tmp）、内核设备路径 （/dev）、系统统计信息 （/proc） 和系统配置 （/sys）。

# #8.3.15 访问时间戳
许多文件系统都支持访问时间戳，它记录每个文件和目录的访问（读取）时间。这会导致每当读取文件时更新文件元数据，从而创建消耗磁盘 I/O 资源的写入工作负载。第 8.8 节 调整介绍了如何关闭这些更新。

一些文件系统通过延迟和分组来优化访问时间戳写入，以减少对活动工作负载的干扰。

# #8.3.16 容量
当文件系统填满时，性能可能会因以下几个原因而下降。首先，在写入新数据时，可能需要更多的 CPU 时间和磁盘 I/O 才能在磁盘上找到空闲块。其次，磁盘上的可用空间区域可能更小且位置更稀疏，由于 I/O 或随机 I/O 较小，性能会降低。

问题有多大取决于文件系统类型、其磁盘布局、其对写入时复制的使用以及它的存储设备。下一节将介绍各种文件系统类型。

# 8.4 架构
本节介绍通用和特定的文件系统架构，从 I/O 堆栈、VFS、文件系统缓存和功能、常见文件系统类型、卷和池开始。在确定要分析和调整的文件系统组件时，此类背景非常有用。有关更深入的内部和其他文件系统主题，请参阅源代码（如果可用）和外部文档。本章末尾列出了其中一些。

# #8.4.1 文件系统I/O堆栈
图 8.6 描述了文件系统 I/O 堆栈的一般模型，重点介绍了文件系统接口。特定的组件、层和 API 取决于所使用的操作系统类型、版本和文件系统。第 3 章 “操作系统”中包括一个更高级别的 I/O 堆栈图，第 9 章 “磁盘”中提供了另一个更详细地介绍磁盘组件的图。

这显示了从应用程序和系统库到 syscall 以及通过内核的 I/O 路径。从系统调用直接到磁盘设备子系统的路径是原始 I/O。通过 VFS 和文件系统的路径是文件系统 I/O，包括直接 I/O，它会跳过文件系统缓存

# #8.4.2 VFS
VFS（虚拟文件系统接口）为不同的文件系统类型提供了一个通用接口。它的位置如图 8.7 所示。

VFS 起源于 SunOS，现已成为文件系统的标准抽象

Linux VFS 接口使用的术语可能有点令人困惑，因为它重复使用术语 inode 和 superblocks 来指代 VFS 对象，这些术语源自 Unix 文件系统磁盘上的数据结构。用于 Linux 磁盘上数据结构的术语通常以它们的文件系统类型为前缀，例如 ext4_inode 和 ext4_super_block。VFS inode 和 VFS 超级块仅在内存中。

VFS 接口还可以用作测量任何文件系统性能的公共位置。可以使用操作系统提供的统计信息或静态或动态检测来执行此作。

# #8.4.3 文件系统缓存
Unix 最初只有缓冲区缓存，以提高块设备访问的性能。如今，Linux 有多种不同的缓存类型。图 8.8 概述了 Linux 上的文件系统缓存，显示了可用于标准文件系统类型的通用缓存

# 缓冲区缓存
Unix 在块设备接口使用缓冲区缓存来缓存磁盘设备块。这是一个单独的、固定大小的缓存，后来添加了页面缓存，在平衡它们之间的不同工作负载时会出现调优问题，以及双重缓存和同步的开销。通过使用页面高速缓存来存储缓冲区高速缓存，这些问题在很大程度上已经得到解决，这是 SunOS 引入的一种称为统一缓冲区高速缓存的方法。

Linux 最初与 Unix 一样使用缓冲区缓存。从 Linux 2.4 开始，缓冲区缓存也存储在页面缓存中（因此图 8.8 中的虚线边框），避免了双重缓存和同步开销。缓冲区缓存功能仍然存在，提高了块设备 I/O 的性能，并且该术语仍然出现在 Linux 可观测性工具中（例如，free（1））。

缓冲区缓存的大小是动态的，可以从 /proc 中观察到。

# 页面缓存
页面高速缓存在 1985 年的虚拟内存重写期间首次引入 SunOS，并添加到 SVR4 Unix [Vahalia 96] 中。它缓存了虚拟内存页，包括映射的文件系统页，从而提高了文件和目录 I/O 的性能。它比缓冲区缓存更高效，缓冲区缓存需要每次查找时从文件偏移量转换为磁盘偏移量。多种文件系统类型可以使用页面缓存，包括原始使用者 UFS 和 NFS。大小是动态的：页面缓存将增长以使用可用内存，并在应用程序需要时再次释放它。

Linux 具有具有相同属性的页面缓存。Linux 页面缓存的大小也是动态的，可以通过可调来设置从页面缓存中移出和交换之间的平衡（swappiness;参见第 7 章 内存）。

文件系统需要的脏 （修改） 内存页由内核线程刷新到磁盘。在 Linux 2.6.32 之前，有一个页面脏刷新 （pdflush） 线程池，根据需要在 2 到 8 个之间。此后，这些线程已被 flusher 线程（名为 flush）所取代，这些线程是按设备创建的，以更好地平衡每个设备的工作负载并提高吞吐量。页面刷新到磁盘的原因如下：

■ 间隔（30 秒）后 
■ sync（2）、fsync（2）、msync（2） 系统调用 
■ 脏页过多（dirty_ratio 和 dirty_bytes 可调参数） 
■ 页面缓存中没有可用页面

如果存在系统内存不足，另一个内核线程，即 page-out 守护进程（kswapd，也称为页面扫描程序）也可能查找并计划将脏页写入磁盘，以便它可以释放内存页以供重用（请参见第 7 章 “内存”）。为了实现可观察性，kswapd 和 flush 线程在操作系统性能工具中显示为内核任务

有关页面扫描仪的更多详细信息，请参阅第 7 章 内存。

# Dentry 缓存
dentry 高速缓存 （Dcache） 记住从目录条目 （struct dentry） 到 VFS inode 的映射，类似于早期的 Unix 目录名称查找高速缓存 （DNLC）。Dcache 提高了路径名查找的性能（例如，通过 open（2））：当遍历路径名时，每个名称查找都可以检查 Dcache 的直接 inode 映射，而不是单步执行直接的内容。Dcache 条目存储在哈希表中，以便快速且可扩展地查找（由父 dentry 和目录条目名称进行哈希处理）。

多年来，性能得到了进一步改进，包括读取-复制-更新遍历 （RCU-walk） 算法 [Corbet 10]。这将尝试在不更新 dentry 引用计数的情况下遍历路径名，这会导致可扩展性问题，因为在多 CPU 系统上，路径名查找率很高。如果遇到不在缓存中的 dentry，RCU-walk 将恢复为较慢的引用计数遍历 （ref-walk），因为在文件系统查找和阻塞期间需要引用计数。对于繁忙的工作负载，预计 dentry 数据可能会被缓存，并且 RCU 遍历方法将成功。

Dcache 还执行负缓存，这会记住对不存在的条目的查找。这可以提高失败查找的性能，通常在搜索共享库时发生。

Dcache 动态增长，当系统需要更多内存时，通过 LRU（最近最少使用）进行收缩。它的大小可以通过 /proc 看到。

# inode 缓存
此缓存包含 VFS inode （struct inode），每个 inode 描述文件系统对象的属性，其中许多属性通过 stat（2） 系统调用返回。文件系统工作负载经常访问这些属性，例如在打开文件时检查权限，或在修改期间更新时间戳。这些 VFS inode 存储在哈希表中，以便快速且可扩展地查找（按 inode 编号和文件系统超级块进行哈希处理），尽管大多数查找将通过 Dentry 缓存完成。

inode 缓存动态增长，至少保存 Dcache 映射的所有 inode。当存在系统内存压力时，inode 缓存将收缩，丢弃没有关联 dentry 的 inode。其大小可通过 /proc/sys/fs/inode* 文件查看。

# #8.4.4 文件系统功能
此处介绍了影响性能的其他关键文件系统功能。

# Block vs. Extent
基于块的文件系统将数据存储在固定大小的块中，由存储在元数据块中的指针引用。对于大型文件，这可能需要许多块指针和元数据块，并且块的放置可能会变得分散，从而导致随机 I/O。一些基于块的文件系统尝试连续放置块以避免这种情况。另一种方法是使用可变块大小，以便随着文件的增长可以使用更大的块大小，这也减少了元数据开销。

基于扩展数据的文件系统为文件 （扩展数据块） 预分配连续空间，并根据需要增加它们。这些扩展数据块的长度是可变的，表示一个或多个连续的块。

这可以提高流式处理性能，并且可以在本地化文件数据时提高随机 I/O 性能。它还提高了元数据性能，因为要跟踪的对象更少，而不会牺牲范围内未使用块的空间

# 日记
文件系统日志（或日志）记录对文件系统的更改，以便在系统崩溃或电源故障时，可以原子地重放更改 — 要么全部成功，要么失败。这允许文件系统快速恢复到一致状态。如果与更改相关的数据和元数据写入不完整，则非日志文件系统可能会在系统崩溃期间损坏。从此类崩溃中恢复需要遍历所有文件系统结构，对于大型 （TB） 文件系统，这可能需要数小时。

日志是同步写入磁盘的，对于某些文件系统，可以将其配置为使用单独的设备。某些日志同时记录数据和元数据，这会消耗更多的存储 I/O 资源，因为所有 I/O 都写入两次。其他 Alpha S 仅写入元数据，并通过采用 Copy-on-write 来保持数据完整性。

有一种文件系统类型仅包含一个日志：日志结构的文件系统，其中所有数据和元数据更新都写入连续的循环日志。这优化了写入性能，因为写入始终是 Sequences，并且可以合并以使用更大的 I/O 大小

# 写入时复制
写入时复制 （COW） 文件系统不会覆盖现有数据块，而是遵循以下步骤：
1. 将数据块写入新位置（新副本）。
2. 更新对新块的引用。
3. 将旧块添加到免费列表中。

这有助于在系统发生故障时保持文件系统的完整性，还可以通过将随机写入转换为顺序写入来提高性能。

当文件系统接近容量时，COW 可能会导致文件的磁盘数据布局被分段，从而降低性能（尤其是对于 HDD）。文件系统碎片整理（如果可用）可能有助于恢复性能。

# Scrubbing
这是一种文件系统功能，可异步读取所有数据块并验证校验和，以尽早检测到故障驱动器，理想情况下，故障仍可因 RAID 而恢复。但是，清理读取 I/O 可能会损害性能，因此应以低优先级或在低工作负载时发出

# 其他功能
其他可能影响性能的文件系统功能包括快照、压缩、内置冗余、重复数据删除、修剪支持等。以下部分介绍了特定文件系统的各种此类功能。

# #8.4.5 文件系统类型
本章的大部分内容描述了可应用于所有文件系统类型的通用特征。以下各节总结了常用文件系统的特定性能功能。它们的分析和调整将在后面的部分中介绍。

# FFS
许多文件系统最终都基于 Berkeley 快速文件系统 （FFS），该文件系统旨在解决原始 Unix 文件系统的问题。一些背景知识可以帮助解释当今文件系统的状态。

最初的 Unix 文件系统磁盘布局由一个 inode 表、512 字节的存储块和分配资源时使用的信息超级块 [Ritchie 74][Lions 77] 组成。inode 表和存储块将磁盘分区分为两个范围，这会导致在它们之间查找时出现性能问题。另一个问题是使用了较小的固定块大小（512 字节），这限制了吞吐量并增加了存储大型文件所需的元数据（指针）数量。一个将其翻倍到 1024 字节的实验，然后遇到的瓶颈由 [McKusick 84] 描述：

尽管吞吐量翻了一番，但旧文件系统仍然只使用了大约 4% 的磁盘带宽。主要问题是，尽管免费列表最初是为了最佳访问而订购的，但随着文件的创建和删除，它很快就变得混乱了。最终，空闲列表变得完全随机，导致文件的块在磁盘上随机分配。这强制在每次阻止访问之前进行搜索。尽管旧文件系统在最初创建时提供高达每秒 175 KB 的传输速率，但由于数据块放置的这种随机化，在适度使用几周后，此速率恶化到每秒 30 KB。

此摘录描述了空闲列表碎片，它会随着文件系统的使用而随着时间的推移而降低性能。

FFS 通过将分区拆分为多个柱面组来提高性能，如图 8.9 所示，每个柱面组都有自己的 inode 数组和数据块。文件 inode 和数据尽可能保存在一个柱面组中，如图 8.9 所示，减少了磁盘查找。其他相关数据也被放置在附近，包括目录及其条目的 inode。inode 的设计与此类似，具有指针和数据块的层次结构，如图 8.10 所示（三重间接块，具有三级指针，此处未显示）[Bach 86]。

数据块大小增加到最小 4 KB，从而提高了吞吐量。这减少了存储文件所需的数据块数量，从而减少了引用数据块所需的间接数据块的数量。所需的间接指针块的数量进一步减少，因为它们也更大。为了提高小文件的空间效率，每个块可以拆分为 1 KB 的片段。

FFS 的另一个性能特性是块交错：将连续文件块放在磁盘上，它们之间有一个或多个块的间距 [Doeppner 10]。这些额外的块为 kernel 和 processor 提供了时间来发出下一次 sequential file read。如果不交错，下一个块可能会在准备好发出读取之前传递（旋转）磁盘头，从而导致在等待完全旋转时出现延迟。

# ext3
Linux 扩展文件系统 （ext） 开发于 1992 年，是 Linux 及其 VFS 的第一个文件系统，基于原始 Unix 文件系统。第二个版本 ext2 （1993） 包含来自 FFS 的多个时间戳和柱面组。第三个版本 ext3 （1999） 包括文件系统增长和日志。

主要性能功能（包括自发布以来添加的功能）包括：

■ 日记：排序模式（仅用于元数据）或日记模式（用于元数据和数据）。日志功能可以提高系统崩溃后的启动性能，无需运行 fsck。它还可以通过合并元数据写入来提高某些写入工作负载的性能。
■ 日志设备：可以使用外部日志设备，这样日志工作负载就不会与读取工作负载竞争。
■ Orlov 块分配器：这将顶级目录分布在柱面组之间，因此子目录和内容更有可能位于同一位置，从而减少随机 I/O。
■ 目录索引：这些索引将哈希 B 树添加到文件系统中，以便更快地查找目录

mke2fs（8） 手册页中记录了可配置的功能。

# ext4
Linux ext4 文件系统于 2008 年发布，扩展了 ext3，增加了新功能和性能改进：扩展、大容量、使用 fallocate（2） 进行预分配、延迟分配、日志校验和、更快的 fsck、多数据块分配器、纳秒时间戳和快照。

主要性能功能（包括自发布以来添加的功能）包括：

■ 盘区：盘区改进了连续放置，减少了随机 I/O 并增加了顺序 I/O 的 I/O 大小。它们在部分 8.4.4， 文件系统功能中介绍。
■ Preallocation：通过 fallocate（2） syscall，这允许应用程序预分配可能是连续的空间，从而提高以后的写入性能。
■ 延迟分配：块分配延迟到刷新到磁盘，允许写入组（通过多块分配器），从而减少碎片。
■ 更快的fsck：标记未分配的块和 inode 条目，从而减少 fsck 时间

某些功能的状态可以通过 /sys 文件系统查看。例如：

 # cd /sys/fs/ext4/features
 # grep . *
 batched_discard:supported
 casefold:supported
 encryption:supported
 lazy_itable_init:supported
 meta_bg_resize:supported
 metadata_csum_seed:supported

 mke2fs（8） 手册页中记录了可配置的功能。某些功能（如范围）也可以应用于 ext3 文件系统

# XFS
XFS 由 Silicon Graphics 于 1993 年为其 IRIX操作系统创建，用于解决以前的 IRIX 文件系统 EFS（基于 FFS）[Sweeney 96] 中的可扩展性限制。XFS 补丁在 2000 年代初期被合并到 Linux 内核中。如今，大多数 Linux 发行版都支持 XFS，并且可以用于根文件系统。例如，Netflix 使用 XFS 作为其Cassandra 数据库实例，因为它对该工作负载具有高性能（并将 ext4 用于根文件系统）。

主要性能功能（包括自发布以来添加的功能）包括：

■ 分配组：分区分为大小相等的分配组 （AG），这些分配组可以并行访问。为了限制争用，每个 AG 的 inode 和可用阻止列表等元数据是独立管理的，而文件和目录可以跨 AG。
■ 盘区：（请参阅 ext4 中的先前描述。）
■ 日志：日志可以提高系统崩溃后的启动性能，无需运行 fsck（8）。它还可以通过合并元数据写入来提高某些写入工作负载的性能。
■ 日志设备：可以使用外部日志设备，以便日志工作负载不会与数据工作负载竞争。
■ 条带化分配：如果文件系统是在条带化 RAID 或 LVM 设备上创建的，则可以提供用于数据和日志的条带化单元，以确保针对底层硬件优化数据分配。
■ 延迟分配：数据块分配延迟到数据刷新到磁盘，从而允许写入进行分组并减少碎片。块是为内存中的文件保留的，以便在发生 flush 时有可用空间。
■ 在线碎片整理：XFS 提供了一个碎片整理实用程序，该实用程序可以在主动使用时对文件系统进行作。虽然 XFS 使用扩展和延迟分配来防止碎片化，但某些工作负载和条件可能会使文件系统碎片化。

mkfs.xfs（8） 手册页中记录了可配置的功能。XFS 的内部性能数据可以通过 /prov/fs/xfs/stat 查看。数据专为高级分析而设计：有关更多信息，请参阅 XFS 网站 [XFS 06][XFS 10]

# ZFS
ZFS 由 Sun Microsystems 开发并于 2005 年发布，它将文件系统与卷管理器相结合，并包含许多企业功能，使其成为文件服务器（文件管理器）的有吸引力的选择。ZFS 作为开放源代码发布，并被多个操作系统使用，但通常作为附加组件使用，因为 ZFS 使用 CDDL 许可证。大多数开发都在 OpenZFS 项目中进行，该项目在 2019 年宣布支持 Linux 作为主要操作系统 [Ahrens 19]。虽然它在 Linux 中的支持和使用越来越多，但由于源代码许可证仍然存在阻力，包括来自 Linus Torvalds [Torvalds 20a]。

ZFS 的主要性能功能（包括自发布以来添加的功能）包括：
■ 池化存储：所有分配的存储设备都放置在一个池中，从中创建文件系统。这允许所有设备并行使用，以实现最大吞吐量和 IOPS。可以使用不同的 RAID 类型：0、1、10、Z（基于 RAID-5）、Z2（双奇偶校验）和 Z3（三奇偶校验）。
■ COW：复制修改后的块，然后按顺序分组和写入它们
■ 日志记录：ZFS 将更改的事务组 （TXG） 作为批处理进行刷新，这些更改作为一个整体成功或失败，以便磁盘上的格式始终保持一致。
■ ARC：自适应替换缓存通过同时使用多种缓存算法来实现高缓存命中率：最近使用 （MRU） 和最常用 （MFU）。主内存根据它们的性能在它们之间进行平衡，这可以通过维护额外的元数据（幻影列表）来了解如果它统治所有主内存，每个元数据的性能会如何。
■ 智能预取：ZFS 根据需要应用不同类型的预取：元数据、znodes（文件内容）和 vdevs（虚拟设备）。
■ 多个预取流：一个文件上的多个流式读取器可能会在文件系统在它们之间查找时创建随机的 I/O 工作负载。ZFS 跟踪各个预取流，允许新流加入它们。
■ 快照：由于 COW 架构，快照几乎可以立即创建，从而将新块的复制推迟到需要时。
■ ZIO 管道：设备 I/O 由阶段管道处理，每个阶段由一个线程池提供服务以提高性能。
■ Compression：支持多种算法，这通常会导致 CPU 开销降低性能。lzjb （Lempel-Ziv Jeff Bonwick） 选项是轻量级的，可以通过减少 I/O 负载（因为它是压缩的）来略微提高存储性能，但会消耗一些 CPU。
■ SLOG：ZFS 单独的意图日志允许将同步写入写入到不同的设备，从而避免与池磁盘的工作负载争用。在系统发生故障时，对 SLOG 的写入是只读的，以便重放。
■ L2ARC：2 级 ARC 是主内存之后的第二级缓存，用于在基于闪存的固态磁盘 （SSD） 上缓存随机读取工作负载。它不缓冲写入工作负载，并且仅包含已驻留在存储池磁盘上的干净数据。它还可以复制 ARC 中的数据，以便系统在发生主内存刷新扰动时可以更快地恢复。
■ 重复数据删除：一种文件系统级功能，可避免记录相同数据的多个副本。此功能对性能有重大影响，无论是好的（减少设备 I/O）还是坏的（当哈希表不再适合主内存时，设备 I/O 会膨胀，可能会很严重）。初始版本仅适用于哈希表应始终适合主内存的工作负载

与其他文件系统相比，ZFS 存在一种会降低性能的行为：默认情况下，ZFS 向存储设备发出 cache flush 命令，以确保在断电的情况下完成写入。这是 ZFS 完整性功能之一;但是，这是有代价的：它可能会导致必须等待高速缓存刷新的 ZFS作出现延迟。

# btrfs
B 树文件系统 （btrfs） 基于写入时复制 B 树。这是一种现代文件系统和卷管理器组合架构，类似于 ZFS，预计最终将提供类似的功能集。当前功能包括池存储、大容量、扩展、COW、卷增长和收缩、子卷、块存储设备添加和删除、快照、克隆、压缩和各种校验和（包括 crc32c、xxhash64、sha256 和 blake2b）。Oracle 于 2007 年开始开发。

主要性能特点包括以下内容：
■ 池存储：存储设备放置在组合卷中，从中创建文件系统。这允许所有设备并行使用，以实现最大吞吐量和 IOPS。可以使用 RAID 0、1 和 10。
■ COW：按顺序对数据进行分组和写入。
■ 在线平衡：可以在存储设备之间移动对象以平衡其工作负载。盘区：改进顺序布局和性能。
■ 快照：由于 COW 架构，快照几乎可以立即创建，从而将新块的复制推迟到需要时。
■ 压缩：支持 zlib 和 LZO。
■ 日志：可以创建每个子卷的日志树来记录同步 COW 工作负载

计划的性能相关功能包括 RAID-5 和 6、对象级 RAID、增量转储和重复数据删除。

# #8.4.6 卷与工具
从历史上看，文件系统构建在单个磁盘或磁盘分区之上。卷和池允许在多个磁盘上构建文件系统，并且可以使用不同的 RAID 策略进行配置（请参见第 9 章 “磁盘”）。

卷将多个磁盘显示为一个虚拟磁盘，文件系统位于该虚拟磁盘上。当构建在整个磁盘（而不是片或分区）上时，卷可以隔离工作负载，从而减少争用问题的性能

卷管理软件包括用于基于 Linux 的系统的 Logical Volume Manager （LVM）。卷或虚拟磁盘也可能由硬件 RAID 控制器提供。

池化存储包括存储池中的多个磁盘，可以从中创建多个文件系统。如图 8.11 所示，其中 volumes 用于比较。池存储比卷存储更灵活，因为无论后备设备如何，文件系统都可以扩展和收缩。现代文件系统（包括 ZFS 和 btrfs）使用此方法，也可以使用 LVM 使用此方法。

池存储可以将所有磁盘设备用于所有文件系统，从而提高性能。工作负载不是隔离的;在某些情况下，考虑到一些灵活性的权衡，可以使用多个池来分离工作负载，因为磁盘设备最初必须放置在一个池中或另一个池中。请注意，池磁盘可能具有不同的类型和大小，而卷可能仅限于卷中的统一磁盘。

使用软件卷管理器或池存储时的其他性能注意事项包括

■ 条带宽度：与工作负载相匹配。
■ 可观察性：虚拟设备的利用率可能令人困惑;检查单独的 Physical Devices。
■ CPU 开销：尤其是在执行 RAID 奇偶校验计算时。对于现代、更快的 CPU 来说，这已经不再是一个问题。（奇偶校验计算也可以卸载到硬件 RAID 控制器。
■ 重建：也称为重新同步，是指将空磁盘添加到 RAID 组（例如，替换故障磁盘）并填充加入组所需的数据。这可能会显著影响性能，因为它会消耗 I/O 资源，并且可能会持续数小时甚至数天。

重建是一个日益严重的问题，因为存储设备的容量增长速度超过其吞吐量，从而增加了重建时间，并使重建过程中发生故障或介质错误的风险更大。如果可能，脱机重建已卸载的驱动器可以缩短重建时间。

# 8.5 方法论
本节介绍文件系统分析和调整的各种方法和练习。表 8.3 总结了这些主题。

参见第 2 章 方法论，了解更多策略和其中许多策略的介绍。

这些可以单独遵循或组合使用。我的建议是按以下顺序使用以下策略开始：延迟分析、性能监控、工作负载特征、微基准测试和静态性能调整。您可能会想出最适合您的环境的不同组合和排序。

第 8.6 节 可观测性工具 显示了用于应用这些方法的操作系统工具

# #8.5.1 硬盘分析
一种常见的故障排除策略是忽略文件系统，而专注于磁盘性能。这假定最差的 I/O 是磁盘 I/O，因此通过仅分析磁盘，您可以方便地将重点放在预期的问题来源上。

对于更简单的文件系统和更小的缓存，这通常是可行的。如今，这种方法变得令人困惑，并错过了所有类别的问题（参见第 8.3.12 节， 逻辑 I/O 与物理 I/O）

# #8.5.2 延迟分析
对于延迟分析，请首先测量文件系统作的延迟。这应该包括所有对象操作，而不仅仅是 I/O（例如，包括 sync（2））

操作延迟 = 时间（操作完成）- 时间（操作请求）

这些时间可以从四层之一进行测量，如表 8.4 所示

选择图层可能取决于工具的可用性。检查以下内容

■ 应用程序文档：某些应用程序已经提供了文件系统延迟指标，或者能够启用其收集。
■ 操作系统工具：操作系统还可以提供指标，理想情况下是每个文件系统或应用程序的单独统计数据。
■ 动态检测：如果您的系统具有动态检测（Linux kprobe 和 uprobe，由各种跟踪器使用），则可以通过自定义跟踪程序检查所有层，而无需重新启动任何内容。

延迟可以表示为每个间隔的平均值、分布（例如，直方图或热图：参见 Section 8.6.18），或者表示为每个作及其延迟的列表。对于缓存命中率较高（超过 99%）的文件系统，每个间隔的平均值可能会以缓存命中延迟为主。当存在孤立的高延迟（异常值）实例时，这可能是不幸的，这些实例需要识别，但很难从平均值中看出。通过检查完整分布或每个作的延迟，可以调查此类异常值，以及不同延迟层的影响，包括文件系统缓存命中和未命中。

找到高延迟后，继续对文件系统进行向下钻取分析以确定来源。

# 交易成本
表示文件系统延迟的另一种方法是在应用程序事务（例如，数据库查询）期间等待文件系统所花费的总时间：

文件系统中的时间百分比 = 100 * 总阻塞文件系统延迟/应用程序事务时间

这允许根据应用程序性能量化文件系统操作的成本，并预测性能改进。该指标可以表示为间隔内所有事务的平均值，也可以表示为单个事务的平均值。

图 8.12 显示了在为事务提供服务的应用程序线程上花费的时间。此事务发出单个文件系统读取;应用程序会阻塞并等待其完成，从而转换为 CPU 之外。在这种情况下，总阻塞时间是单个文件系统读取的时间。如果在事务期间调用了多个阻塞 I/O，则总时间是它们的总和。

举个具体的例子，一个应用程序事务需要 200 毫秒，在此期间，它在多个文件系统 I/O 上总共等待 180 毫秒。应用程序被文件系统阻塞的时间为 90%（100 * 180 毫秒/200 毫秒）。消除文件系统延迟可以将性能提高多达 10 倍。

再举一个例子，如果一个应用程序事务需要 200 毫秒，在此期间，文件系统中只花费了 2 毫秒，那么文件系统（以及整个磁盘 I/O 堆栈）对事务运行时间的贡献仅为 1%。这个结果非常有用，因为它可以将性能调查引导到延迟的真正来源

如果应用程序将 I/O 作为非阻塞发出，则应用程序可以在文件系统响应时继续在 CPU 上执行。在这种情况下，阻塞文件系统延迟仅测量应用程序在 CPU 之外被阻止的时间

# #8.5.3 工作负载特征描述
在容量规划、基准测试和模拟工作负载时，描述应用的负载是一项重要的练习。它还可以通过识别可以消除的不必要工作来带来一些最大的性能提升

以下是描述文件系统工作负载的基本属性：
■ 操作速率和操作类型 
■ 文件 I/O 吞吐量 
■ 文件 I/O 大小 
■ 读/写比率 
■ 同步写入比率 
■ 随机与顺序文件偏移访问

操作速率和吞吐量在部分 8.1术语中定义。同步写入以及随机写入与顺序写入在部分 8.3概念中介绍

这些特征可能每秒钟都不同，尤其是对于每隔一段时间执行的定时应用程序任务。为了更好地描述工作负载，请捕获最大值和平均值。更好的是，检查值随时间推移的完整分布

下面是一个示例工作负载描述，用于说明如何一起表示这些属性：

在金融交易数据库上，文件系统具有随机读取工作负载，平均读取速度为 18000 次读取/秒，平均读取大小为 4 KB。总作速率为 21000 次作/秒，其中包括读取、统计、打开、关闭和大约 200 次同步写入/秒。写入速率是稳定的，而读取速率是变化的，最高可达 39,000 次读取/秒的峰值

这些特征可以用单个文件系统实例来描述，也可以用相同类型系统上的所有实例来描述。

# 高级工作负载特征描述/核对表
可能包含其他详细信息来描述工作负载的特征。这些问题已在此处列为需要考虑的问题，在彻底研究文件系统问题时，也可以作为检查表：

■ 文件系统缓存命中率是多少？失误率？
■ 文件系统缓存容量和当前使用情况是多少？
■ 还存在哪些其他缓存（directory、inode、buffer），它们的统计信息是什么？
■ 过去是否尝试过调整文件系统？是否有任何文件系统参数设置为其默认值以外的值？
■ 哪些应用程序或用户正在使用文件系统？
■ 正在访问哪些文件和目录？已创建并已删除？
■ 是否遇到任何错误？这是由于无效请求还是文件系统出现问题？
■ 为什么会发出文件系统 I/O（用户级调用路径）？
■ 应用程序直接（同步）请求文件系统 I/O 的程度如何？
■ I/O 到达时间的分布情况

其中许多问题可以按应用程序或文件提出。它们中的任何一个也可以随时间进行检查，以查找最大值、最小值和基于时间的变化。另请参阅第 2 章 方法中的第 2.5.10 节 工作负载特征，其中提供了要测量的特征（谁、为什么、什么、如何）的更高级别摘要。

# 性能表征
前面的工作负载特征列表检查应用的工作负载。下面检查结果性能

■ 平均文件系统作延迟是多少？
■ 是否存在任何高延迟异常值？
■ 什么是操作延迟的完整分布？
■ 文件系统或磁盘 I/O 的系统资源控制是否存在并处于活动状态？

前三个问题可以针对每种作类型单独提出。

# 事件跟踪
跟踪工具可用于将所有文件系统作和详细信息记录到日志中，以供以后分析。这可能包括每个 I/O 的操作类型、操作参数、文件路径名、开始和结束时间戳、完成状态以及进程 ID 和名称。虽然这可能是工作负载描述的终极工具，但在实践中，由于文件系统操作的速率，它可能会花费大量的开销，除非经过大量过滤（例如，只在日志中包含慢速 I/O：参见 Section 8.6.14 中的 ext4sslow（8） 工具）。

# #8.5.4 性能监控
性能监控可以识别一段时间内的活动问题和行为模式。文件系统性能的关键指标包括

■ 运行速率 
■ 运行延迟

操作速率是应用的工作负载的最基本特征，延迟是最终性能。正常或不良延迟的值取决于您的工作负载、环境和延迟要求。如果您不确定，可以对已知的好工作负载与坏工作负载进行微观基准测试，以调查延迟（例如，通常会命中文件系统缓存的工作负载与通常会错过的工作负载）。请参见部分 8.7实验。

操作延迟指标可以作为每秒平均值进行监控，并且可以包括其他值，例如最大值和标准偏差。理想情况下，可以检查延迟的完整分布，例如使用直方图或热图，以查找异常值和其他模式。

还可以记录每种作类型（读取、写入、统计、打开、关闭等）的速率和延迟。这样做将通过识别特定作类型的差异，极大地帮助调查工作负载和性能变化。

对于实施基于文件系统的资源控制的系统，可以包含统计信息以显示是否以及何时使用限制

不幸的是，在 Linux 中，通常没有现成的文件系统作统计信息（例外情况包括，对于 NFS，通过 nfsstat（8））

# #8.5.5 静态性能调优
静态性能优化侧重于已配置环境的问题。对于文件系统性能，请检查静态配置的以下方面

■ 挂载并主动使用了多少个文件系统？
■ 文件系统记录大小是多少？
■ 是否启用了访问时间戳？
■ 启用了哪些其他文件系统选项（压缩、加密...）？
■ 文件系统缓存是如何配置的？最大尺寸？其他缓存（directory、inode、buffer）是如何配置的？
■ 二级缓存是否存在且正在使用中？
■ 存在和正在使用多少个存储设备？
■ 存储设备配置是什么？RAID？
■ 使用哪些文件系统类型？
■ 文件系统（或内核）的版本是什么？
■ 是否有应考虑的文件系统错误/补丁？
■ 是否有用于文件系统 I/O 的资源控制？

回答这些问题可以揭示被忽视的配置选择。有时，系统已针对一个工作负载进行了配置，然后又针对另一个工作负载重新调整了用途。此方法将提醒您重新访问这些选择

# #8.5.6 缓存调优
内核和文件系统可以使用许多不同的缓存，包括缓冲区缓存、目录缓存、inode 缓存和文件系统（页面）缓存。部分 8.4体系结构中介绍了各种高速缓存。这些可以检查并经常进行调整，具体取决于可用的可调选项。

# #8.5.7 Wordload 分离
某些类型的工作负载在配置为使用自己的专用文件系统和磁盘设备时性能更好。这种方法被称为使用 “单独的心轴”，因为通过在两个不同的工作负载位置之间搜索来创建 random I/O 对于旋转磁盘尤其不利（参见第 9 章 磁盘）。

例如，数据库的日志文件和数据库文件可能受益于使用单独的文件系统和磁盘。数据库的安装指南通常包含有关其数据存储放置的建议。

# #8.5.8 微基准测试
用于文件系统和磁盘基准测试的基准测试工具（其中有很多）可用于测试给定工作负载的文件系统中不同文件系统类型或设置的性能。可能测试的典型因素包括

■ 操作类型：读取、写入和其他文件系统作的速率 
■ I/O 大小：1 字节，最大 1 MB 或更大 
■ 文件偏移模式：随机或顺序 
■ 随机访问模式：均匀、随机或帕累托分布 
■ 写入类型：异步或同步 （O_SYNC） 
■ 工作集大小：它在文件系统缓存中的适应程度
■ 并发性：并行 I/O 数量或执行 I/O 的线程数 
■ 内存映射：通过 mmap（2） 而不是 read（2）/write（2） 访问文件 
■ 缓存状态：文件系统缓存是“冷”（未填充）还是“暖” 
■ 文件系统可调参数：可能包括压缩、重复数据删除等

常见组合包括随机读取、顺序读取、随机写入和顺序写入。我没有在这个列表中包括直接 I/O，因为它的微基准测试的目的是绕过文件系统并测试磁盘设备性能（参见第 9 章 磁盘）。

对文件系统进行微基准测试时，一个关键因素是工作集大小 （WSS）：访问的数据量。根据基准测试，这可能是正在使用的文件的总大小。除非使用直接 I/O 标志，否则较小的工作集大小可能完全从主内存 （DRAM） 中的文件系统缓存返回。较大的工作集大小可能主要从存储设备 （磁盘） 返回。性能差异可以是多个数量级。针对新挂载的文件系统运行基准测试，然后在填充缓存后再次运行基准测试，并比较两者的结果，这通常是 WSS 的一个很好的例子。（另请参见部分 8.7.3， 高速缓存刷新。）

考虑表 8.5 中不同基准的一般期望，其中包括文件的总大小 （WSS）。

一些文件系统基准测试工具没有明确说明它们正在测试什么，并且可能意味着磁盘基准测试，但使用的总文件大小很小，它完全从缓存返回，因此不测试磁盘。请参见部分 8.3.12， 逻辑 I/O 与物理 I/O 的区别，以了解测试文件系统（逻辑 I/O）和测试磁盘（物理 I/O）之间的区别。

某些磁盘基准测试工具通过使用直接 I/O 通过文件系统运行，以避免缓存和缓冲。文件系统仍然起着次要作用，增加了代码路径开销，并映射了文件和磁盘放置之间的差异。

有关此一般主题的更多信息，请参见第 12 章 基准测试

# 8.6观测工具
本节介绍适用于基于 Linux 的操作系统的文件系统可观测性工具。有关使用这些策略，请参阅上一节。

Table 8.6 中列出了本节中的工具。

这是支持部分 8.5， 方法的一系列工具和功能。它从传统工具开始，然后介绍基于跟踪的工具。一些传统工具可能在它们起源的其他类 Unix操作系统上可用，包括：mount（8）、free（1）、top（1）、vmstat（8） 和 sar（1）。许多跟踪工具都是基于 BPF 的，并使用 BCC 和 bpftrace 前端（第 15 章）;它们是：opensnoop（8）、filetop（8）、cachestat（8）、ext4dist（8） 和 ext4sslow（8）。

有关其功能的完整参考，请参阅每个工具的文档，包括其手册页。

# #8.6.1 mount
Linux 的 mount（1） 命令列出了已挂载的文件系统及其挂载标志：

# $ mount
 /dev/nvme0n1p1 on / type ext4 (rw,relatime,discard)
 devtmpfs on /dev type devtmpfs (rw,relatime,size=986036k,nr_inodes=246509,mode=755)
 sysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)
 proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
 securityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)
 tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev)
 [...]

第一行显示存储在 /dev/nvme0n1p1 上的 ext4 文件系统挂载在 / 上，挂载标志为 rw、relatime 和 discard。relatime 是一个性能改进选项，它仅在修改或更改时间也更新时，或者如果上次更新超过一天时更新访问时间，从而减少 inode 访问时间更新和后续的磁盘 I/O 成本。

# #8.6.2 free
Linux 的 free（1） 命令显示内存和交换统计信息。以下两个命令显示正常和宽 （-w） 输出，均以兆字节 （-m） 表示：

# $ free -m
            total      used      free    shared  buff/cache   available
 Mem:         1950       568       163         0        1218        1187
 Swap:           0         0         0
 $ free -mw
            total      used      free    shared     buffers       cache   available
 Mem:         1950       568       163         0          84        1133        1187
 Swap:           0         0         0

宽输出显示缓冲区缓存大小的 buffers 列和页面缓存大小的 cached 列。默认输出将它们组合为 buff/cache。

有一个重要的列可用（free（1） 的新增功能），它显示无需交换即可为应用程序提供多少内存。它考虑了无法立即回收的内存。

这些字段也可以从 /proc/meminfo 中读取，后者以 KB 为单位提供这些字段。

# #8.6.3 top
top（1） 命令的某些版本包括文件系统缓存详细信息。这些来自 Linux 版本的 top（1） 的行包括 buff/cache 和 free（1） 打印的可用 （avail Mem） 统计信息：

MiB Mem :   1950.0 total,    161.2 free,    570.3 used,   
MiB Swap:      0.0 total,      0.0 free,      0.0 used.   
1218.6 buff/cache
 1185.9 avail Mem 

有关 top（1） 的更多信息，请参见第 6 章 CPU。

# #8.6.4 vmstat
vmstat（1） 命令，如 top（1），也可能包含有关文件系统缓存的详细信息。有关 vmstat（1） 的更多详细信息，请参见第 7 章 内存。

以下程序以 1 的间隔运行 vmstat（1），以每秒提供一次更新：

# $ vmstat 1
 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----
 r  b   swpd   free   
buff  cache
 0  0      0 167644  87032 1161112
 0  0      0 167636  87032 1161152
 [...]

buff 列显示缓冲区缓存大小，cache 显示页面缓存大小，两者均以 KB 为单位。

# #8.6.5 sar
系统活动报告器 sar（1） 提供各种文件系统统计信息，并且可以配置为定期记录这些统计信息。sar（1） 在本书的各个章节中被提及，因为它提供了不同的统计数据，并在第 4.4 节 sar 中介绍。

以 1 秒的间隔执行 sar（1） 以报告当前活动：

 # sar -v 1
 Linux 5.3.0-1009-aws (ip-10-1-239-218)     02/08/20       _x86_64_  (2 CPU)
 21:20:24    dentunusd   file-nr  inode-nr    pty-nr
 21:20:25        27027      1344     52945         2
 21:20:26        27012      1312     52922         2
 21:20:27        26997      1248     52899         2
 [...]

-v 选项提供以下列：

■ dentunusd： 目录条目缓存未使用的计数（可用条目） 
■ file-nr： 正在使用的文件句柄数 
■ inode-nr： 正在使用的 inode 数 
■ pty-nr： 正在使用的伪终端数

还有一个 -r 选项，用于打印缓冲区缓存和页面缓存大小的 kbbuffers 和 kbcached 列（以 KB 为单位）。

# #8.6.6 slabtop
Linux 中的 slabtop（1） 命令打印有关内核 slab 缓存的信息，其中一些用于文件系统缓存：

 # slabtop -o
 Active / Total Objects (% used)    : 604675 / 684235 (88.4%)
 Active / Total Slabs (% used)      : 24040 / 24040 (100.0%)
 Active / Total Caches (% used)     : 99 / 159 (62.3%)
 Active / Total Size (% used)       : 140593.95K / 160692.10K (87.5%)
 Minimum / Average / Maximum Object : 0.01K / 0.23K / 12.00K
  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME                   
165945 149714  90%    0.10K   4255       39     17020K buffer_head            
107898  66011  61%    0.19K   5138       21     20552K dentry                 
 67350  67350 100%    0.13K   2245       30      8980K kernfs_node_cache      
 41472  40551  97%    0.03K    324      128      1296K kmalloc-32             
 35940  31460  87%    1.05K   2396       15     38336K ext4_inode_cache       
 33514  33126  98%    0.58K   2578       13     20624K inode_cache            
 24576  24576 100%    0.01K     48      512       192K kmalloc-8          
[...]

在输出中可以看到一些与文件系统相关的 slab 缓存：dentry、ext4_inode_cache 和 inode_cache。如果没有 -o （once） 输出模式，slabtop（1） 将刷新并更新屏幕

Slabs可能包括：

■ buffer_head：用于缓冲区缓存 ■ dentry：dentry 缓存 
■ inode_cache：inode 缓存 
■ ext3_inode_cache：Ext3 的 inode 缓存
■ ext4_inode_cache：ext4 的 inode 缓存 
■ xfs_inode：XFS 的 inode 缓存 
■ btrfs_inode：btrfs 的 inode 缓存

slabtop（1） 使用 /proc/slabinfo，如果启用了 CONFIG_SLAB，则存在

# #8.6.7 strace
文件系统延迟可以在 syscall 接口使用跟踪工具（包括 strace（1） for Linux））进行测量。但是，当前基于 ptrace（2） 的 strace（1） 实现可能会严重损害性能，并且可能仅在性能开销可接受且无法使用其他分析延迟的方法时才适合使用。有关 strace（1） 的更多信息，请参见第 5 章，第 5.5.4 节 strace。

此示例显示了 ext4 文件系统上的 strace（1） 定时读取：

# $ strace -ttT -p 845
 [...]
 18:41:01.513110 read(9, "\334\260/\224\356k..."..., 65536) = 65536 <0.018225>
 18:41:01.531646 read(9, "\371X\265|\244\317..."..., 65536) = 65536 <0.000056>
 18:41:01.531984 read(9, "\357\311\347\1\241..."..., 65536) = 65536 <0.005760>
 18:41:01.538151 read(9, "*\263\264\204|\370..."..., 65536) = 65536 <0.000033>
 18:41:01.538549 read(9, "\205q\327\304f\370..."..., 65536) = 65536 <0.002033>
 18:41:01.540923 read(9, "\6\2738>zw\321\353..."..., 65536) = 65536 <0.000032>

-tt 选项在左侧打印相对时间戳，-T 在右侧打印系统调用时间。每次读取 （2） 为 64 KB，第一次花费 18 毫秒，然后是 56 μs（可能缓存），然后是 5 毫秒。读取对象为文件描述符 9。要检查这是文件系统（而不是套接字），open（2） 系统调用将在早期的 strace（1） 输出中可见，或者可以使用其他工具，例如 lsof（8）。您还可以在 /proc 文件系统中找到有关 FD 9 的信息：/proc/845/ fd{，info}/9}。

考虑到 strace（1） 的当前开销，测得的延迟可能会因观察者效应而产生偏差。请参阅较新的跟踪工具，包括 ext4sslow（8），它们使用每 CPU 缓冲跟踪和 BPF 来大大减少开销，提供更准确的延迟测量

# #8.6.8 fatrace
fatrace（1） 是一个专门的跟踪器，它使用 Linux 的 fanotify API（文件访问通知）。输出示例：

# fatrace
 sar(25294): O /etc/ld.so.cache
 sar(25294): RO /lib/x86_64-linux-gnu/libc-2.27.so
 sar(25294): C /etc/ld.so.cache
 sar(25294): O /usr/lib/locale/locale-archive
 sar(25294): O /usr/share/zoneinfo/America/Los_Angeles
 sar(25294): RC /usr/share/zoneinfo/America/Los_Angeles
 sar(25294): RO /var/log/sysstat/sa09
 sar(25294): R /var/log/sysstat/sa09
 [...]

每行显示进程名称、PID、事件类型、完整路径和可选状态。事件类型包括打开 （O）、读取 （R）、写入 （W） 和关闭 （C）。fatrace（1） 可用于工作负载特征描述：了解访问的文件，并查找可以消除的不必要工作。

但是，对于繁忙的文件系统工作负载，fatrace（1） 每秒可以生成数万行输出，并且可能会消耗大量 CPU 资源。通过筛选到一种类型的事件，可以在一定程度上缓解这种情况。基于 BPF 的跟踪工具，包括 opensnoop（8） （第 8.6.10 节），也大大减少了开销。

# #8.6.9 LatencyTOP
LatencyTOP 是一种用于报告延迟来源的工具，在系统范围内和每个进程范围内汇总。文件系统延迟由 LatencyTOP 报告。例如：

 Cause                                                Maximum     Percentage
 Reading from file                                 209.6 msec         61.9 %
 synchronous write                                  82.6 msec         24.0 %
 Marking inode dirty                                 7.9 msec          2.2 %
 Waiting for a process to die                        4.6 msec          1.5 %
 Waiting for event (select)                          3.6 msec         10.1 %
 Page fault                                          0.2 msec          0.2 %
 Process gzip (10969)                       Total: 442.4 msec
 Reading from file                                 209.6 msec         70.2 %
 synchronous write                                  82.6 msec         27.2 %
 Marking inode dirty                                 7.9 msec          2.5 %

上半部分是系统范围的摘要，下半部分是单个 gzip（1） 进程的摘要，该进程正在压缩文件。gzip（1） 的大部分延迟是由于 70.2% 的 Reading from file （从文件中读取） 造成的，在写入新的压缩文件时，同步写入的延迟为 27.2%。

LatencyTOP 由 Intel 开发，但已经有一段时间没有更新了，其网站也不再在线。它还需要不常启用的内核选项。7 您可能会发现使用 BPF 跟踪工具来测量文件系统延迟更容易：请参阅第 8.6.13 节到第 8.6.15 节

# #8.6.10 opensnoop
opensnoop（8）是一个 BCC 和 bpftrace 工具，用于跟踪文件打开。它对于发现数据文件、日志文件和配置文件的位置非常有用。它还可以发现由频繁打开引起的性能问题，或帮助解决因文件丢失而导致的问题。以下是一些示例输出，其中 -T 包含时间戳：

 # opensnoop -T
 TIME(s)       PID    COMM               FD ERR PATH
 0.000000000   26447  sshd                5   0 /var/log/btmp
 [...]
 1.961686000   25983  mysqld              4   0 /etc/mysql/my.cnf
 1.961715000   25983  mysqld              5   0 /etc/mysql/conf.d/
 1.961770000   25983  mysqld              5   0 /etc/mysql/conf.d/mysql.cnf
 1.961799000   25983  mysqld              5   0 /etc/mysql/conf.d/mysqldump.cnf
 1.961818000   25983  mysqld              5   0 /etc/mysql/mysql.conf.d/
 1.961843000   25983  mysqld              5   0 /etc/mysql/mysql.conf.d/mysql.cnf
 1.961862000   25983  mysqld              5   0 /etc/mysql/mysql.conf.d/mysqld.cnf
 [...]
 2.438417000   25983  mysqld              4   0 /var/log/mysql/error.log
 [...]
 2.816953000   25983  mysqld             30   0 ./binlog.000024
 2.818827000   25983  mysqld             31   0 ./binlog.index_crash_safe
 2.820621000   25983  mysqld              4   0 ./binlog.index
 [...]

此输出包括 MySQL 数据库的启动，opensnoop（2） 已揭示配置文件、日志文件、数据文件（二进制日志）等

opensnoop（8） 的工作原理是只跟踪 open（2） 变体的系统调用：open（2） 和 openat（2）。预计开销可以忽略不计，因为打开通常不频繁。

BCC 版本的选项包括：

■ -T：包含时间戳列 
■ -x：仅显示失败的 opens
■ -p PID：仅跟踪此进程 
■ -n NAME：仅当进程名称包含 NAME 时，才会显示打开

-x 选项可用于故障排除：专注于应用程序无法打开文件的情况。

# #8.6.11 filetop
filetop（8）是一个密件抄送工具，它就像 top（1） 一样，用于文件，显示最常读取或写入的文件名。输出示例：

 # filetop
 Tracing... Output every 1 secs. Hit Ctrl-C to end
 19:16:22 loadavg: 0.11 0.04 0.01 3/189 23035
 TID    COMM             READS  WRITES R_Kb    W_Kb    T FILE
 23033  mysqld           481    0      7681    0       R sb1.ibd
 23033  mysqld           3      0      48      0       R mysql.ibd
 23032  oltp_read_only.  3      0      20      0       R oltp_common.lua
 23031  oltp_read_only.  3      0      20      0       R oltp_common.lua
 23032  oltp_read_only.  1      0      19      0       R Index.xml
 23032  oltp_read_only.  4      0      16      0       R openssl.cnf
 23035  systemd-udevd    4      0      16      0       R sys_vendor
 [...]

默认情况下，将显示前 20 个文件，按 read bytes 列排序。第一行显示 mysqld 从 sb1.ibd 文件中读取了 481 次，总计 7,681 KB。

此工具用于工作负载特征描述和一般文件系统可观察性。就像你可以使用 top（1） 发现一个意外的 CPU 消耗进程一样，这可能有助于你发现一个意外的 I/O 繁忙文件。

默认情况下，Filetop 也只显示常规文件。-a 选项显示所有文件，包括 TCP 套接字和设备节点:

# filetop -a
 [...]
 TID    COMM             READS  WRITES R_Kb    W_Kb    T FILE
 21701  sshd             1      0      16      0       O ptmx
 23033  mysqld           1      0      16      0       R sbtest1.ibd
 23335  sshd             1      0      8       0       S TCP
 1      systemd          4      0      4       0       R comm
 [...]

输出现在包含文件类型 other （O） 和 socket （S）。在这种情况下，其他类型 ptmx 是 /dev 中的字符特殊文件。

选项包括：

■ -C：不清屏：滚动输出
■ -a：显示所有文件类型
■ -r ROWS：打印这么多行（默认 20）
■ -p PID：跟踪此进程

屏幕每秒刷新一次（如 top（1）），除非使用 -C。我更喜欢使用 -C，以便输出位于终端回滚缓冲区中，以防以后需要引用它。

# #8.6.12 cachestat
cachestat（8）10 是一个密件抄送工具，它显示页面缓存的命中和未命中统计数据。这可用于检查页面缓存的命中率和效率，并在调查系统和应用程序调优以获取有关缓存性能的反馈时运行。输出示例：

# $ cachestat -T 1
 TIME         HITS   MISSES  DIRTIES HITRATIO   BUFFERS_MB  CACHED_MB
 21:00:48      586        0     1870  100.00%          208        775
 21:00:49      125        0     1775  100.00%          208        776
 21:00:50      113        0     1644  100.00%          208        776
 21:00:51       23        0     1389  100.00%          208        776
 21:00:52      134        0     1906  100.00%          208        777
 [...]

此输出显示完全缓存的读取工作负载 （HITRATIO 为 100% 的 HITS） 和更高的写入工作负载 （DIRTIES）。理想情况下，命中率接近 100%，以便应用程序读取不会在磁盘 I/O 上阻塞。

如果您遇到可能影响性能的低命中率，则可以将应用程序的内存大小调整得更小一些，从而为页面缓存留出更多空间。如果配置了交换设备，则还可以调整交换性，以首选从页面缓存中逐出而不是交换

选项包括 -T 打印时间戳

虽然这个工具为页面缓存命中率提供了关键的见解，但它也是一个实验性工具，它使用 kprobes 来跟踪某些内核函数，因此它需要维护才能在不同的内核版本上工作。更好的是，如果添加了跟踪点或 /proc 统计信息，则可以重写此工具以使用它们并变得稳定。它今天的最佳用途可能只是证明这样的工具是可能的。

# #8.6.13 ext4dist(xfs, zfs, btrfs, nfs)
ext4dist（8）是一个 BCC 和 bpftrace 工具，用于检测 ext4 文件系统，并以直方图的形式显示延迟的分布，用于常见作：读取、写入、打开和 fsync。其他文件系统也有版本：xfsdist（8）、zfsdist（8）、btrfsdist（8） 和 nfsdist（8）。输出示例：

 # ext4dist 10 1
 Tracing ext4 operation latency... Hit Ctrl-C to end.
 21:09:46:
 operation = read
     usecs               : count     distribution
         0 -> 1          : 783      |***********************                 |
         2 -> 3          : 88       |**                                      |
         4 -> 7          : 449      |*************                           |
         8 -> 15         : 1306     |****************************************|
        16 -> 31         : 48       |*                                       |
        32 -> 63         : 12       |                                        |
        64 -> 127        : 39       |*                                       |
       128 -> 255        : 11       |                                        |
       256 -> 511        : 158      |****                                    |
       512 -> 1023       : 110      |***                                     |
      1024 -> 2047       : 33       |*                                       |
 operation = write
     usecs               : count     distribution
         0 -> 1          : 1073     |****************************            |
         2 -> 3          : 324      |********                                |
         4 -> 7          : 1378     |************************************    |
         8 -> 15         : 1505     |****************************************|
        16 -> 31         : 183      |****                                    |
        32 -> 63         : 37       |                                        |
        64 -> 127        : 11       |                                        |
       128 -> 255        : 9        |                                        |
 operation = open
     usecs               : count     distribution
         0 -> 1          : 672      |****************************************|
         2 -> 3          : 10       |                                        |
 operation = fsync
     usecs               : count     distribution
       256 -> 511        : 485      |**********                              |
       512 -> 1023       : 308      |******                                  |
      1024 -> 2047       : 1779     |****************************************|
      2048 -> 4095       : 79       |*                                       |
      4096 -> 8191       : 26       |                                        |
      8192 -> 16383      : 4        |                                        |

这使用 10 秒的间隔和计数 1 来显示单个 10 秒的跟踪。它显示了双模读取延迟分布，一种模式在 0 到 15 微秒之间，可能是内存缓存命中，另一种模式在 256 到 2048 微秒之间，可能是磁盘读取。也可以研究其他作的分布。写入速度很快，这可能是由于稍后使用较慢的 fsync作将缓冲刷新到磁盘。

这个工具及其配套的 ext4sslow（8）（下一节）显示了应用程序可能会遇到的延迟。在磁盘级别测量延迟是可能的，如第 9 章所示，但应用程序可能不会直接阻塞磁盘 I/O，这使得这些测量结果更难解释。在可能的情况下，我会先使用 ext4dist（8）/ext4sslow（8） 工具，然后再使用磁盘 I/O 延迟工具。请参阅第 8.3.12 节以了解此工具测量的文件系统的逻辑 I/O 与磁盘的物理 I/O 之间的区别。

选项包括：
■ -m：以毫秒为单位打印输出 
■ -p PID：仅跟踪此过程

此工具的输出可以可视化为延迟热图。有关慢速文件系统 I/O 的更多信息，请运行 ext4slower（8） 及其变体。

# #8.6.14 ext4slower(xfs, zfs, btrfs, nfs)
ext4sslow（8）跟踪常见的 ext4作，并打印那些慢于给定阈值的作的每个事件的详细信息。跟踪的作包括 reads、writes、opens 和 fsync。输出示例：

 # ext4slower
 Tracing ext4 operations slower than 10 ms
 TIME     COMM           PID    T BYTES   OFF_KB   LAT(ms) FILENAME
 21:36:03 mysqld         22935  S 0       0          12.81 sbtest1.ibd
 21:36:15 mysqld         22935  S 0       0          12.13 ib_logfile1
 21:36:15 mysqld         22935  S 0       0          10.46 binlog.000026
 21:36:15 mysqld         22935  S 0       0          13.66 ib_logfile1
 21:36:15 mysqld         22935  S 0       0          11.79 ib_logfile1
 [...]

这些列显示时间 （TIME）、进程名称 （COMM） 和 PID、作类型（T：R 为读取、W 为写入、O 为打开，S 为同步）、偏移量（以 KB 为单位OFF_KB）、作延迟（以毫秒为单位） （LAT（ms）） 和文件名 （FILENAME）。

输出显示许多超过 10 毫秒的同步作 （S），这是 ext4slower（8） 的默认阈值。阈值可以作为参数提供;选择 0 毫秒 onds 将显示所有操作：

 # ext4slower 0
 Tracing ext4 operations
 21:36:50 mysqld         22935  W 917504  2048        0.42 ibdata1
 21:36:50 mysqld         22935  W 1024    14165       0.00 ib_logfile1
 21:36:50 mysqld         22935  W 512     14166       0.00 ib_logfile1
 21:36:50 mysqld         22935  S 0       0           3.21 ib_logfile1
 21:36:50 mysqld         22935  W 1746    21714       0.02 binlog.000026
 21:36:50 mysqld         22935  S 0       0           5.56 ibdata1
 21:36:50 mysqld         22935  W 16384   4640        0.01 undo_001
 21:36:50 mysqld         22935  W 16384   11504       0.01 sbtest1.ibd
 21:36:50 mysqld         22935  W 16384   13248       0.01 sbtest1.ibd
 21:36:50 mysqld         22935  W 16384   11808       0.01 sbtest1.ibd
 21:36:50 mysqld         22935  W 16384   1328        0.01 undo_001
 21:36:50 mysqld         22935  W 16384   6768        0.01 undo_002
 [...]

在输出中可以看到一种模式： mysqld 执行对文件的写入，然后执行稍后的同步作。

跟踪所有作可能会产生大量输出和相关的开销。我只在短时间内（例如 10 秒）执行此作，以了解在其他摘要中不可见的文件系统作模式 （ext4dist（8））。

选项包括 -p PID （仅用于跟踪单个进程）和 -j （用于生成可解析 （CSV） 输出。

# #8.6.15 bpftrace
bpftrace 是一种基于 BPF 的跟踪器，它提供了一种高级编程语言，允许创建强大的单行代码和简短脚本。它非常适合根据来自其他工具的线索进行自定义文件系统分析。

bpftrace 在第 15 章 BPF 中进行了解释。本节介绍文件系统分析的一些示例：单行代码、syscall 跟踪、VFS 跟踪和文件系统内部结构。

# 单行
以下单行代码非常有用，并演示了不同的 bpftrace 功能。

使用 process name 通过 openat（2） 打开的跟踪文件
# bpftrace -e 't:syscalls:sys_enter_openat { printf("%s %s\n", comm,
    str(args->filename)); }'

按 syscall 类型对读取的系统调用进行计数：
# bpftrace -e 'tracepoint:syscalls:sys_enter_*read* { @[probe] = count(); }'

按 syscall 类型对写入 syscall 进行计数： 
# bpftrace -e 'tracepoint:syscalls:sys_enter_*write* { @[probe] = count(); }

显示 read（） syscall 请求大小的分布情况：
# bpftrace -e 'tracepoint:syscalls:sys_enter_read { @ = hist(args->count); }'

显示 read（） syscall 读取字节（和错误）的分布：
# bpftrace -e 'tracepoint:syscalls:sys_exit_read { @ = hist(args->ret); }'

按错误代码计算 read（） syscall 错误：
#  bpftrace -e 't:syscalls:sys_exit_read /args->ret < 0/ { @[- args->ret] = count(); }'

对 VFS 调用进行计数
# bpftrace -e 'kprobe:vfs_* { @[probe] = count(); }'

对 PID 181 的 VFS 调用进行计数：
#  bpftrace -e 'kprobe:vfs_* /pid == 181/ { @[probe] = count(); }'

计数 ext4 个跟踪点：
# bpftrace -e 'tracepoint:ext4:* { @[probe] = count(); }'

对 xfs 跟踪点进行计数：
# bpftrace -e 'tracepoint:xfs:* { @[probe] = count(); }'

按进程名称和用户级堆栈对 ext4 文件读取进行计数：
# bpftrace -e 'kprobe:ext4_file_read_iter { @[ustack, comm] = count(); }'

跟踪 ZFS spa_sync（） 次：
# bpftrace -e 'kprobe:spa_sync { time("%H:%M:%S ZFS spa_sync()\n"); }'

按进程名称和 PID 对 dcache 引用进行计数：
#  bpftrace -e 'kprobe:lookup_fast { @[comm, pid] = count(); }'

# 系统调用追踪
Syscall 是跟踪的一个很好的目标，也是许多跟踪工具的插桩源。但是，某些 syscall 缺少文件系统上下文，因此使用起来很混乱。我将提供一个有效 （openat（2） 跟踪） 和无效 （read（2） 跟踪） 的示例，并提供建议的补救措施。

# openat(2)
跟踪 syscall 的 open（2） 系列会显示已打开的文件。现在 openat（2） 变体更常用。追踪它：

 # bpftrace -e 't:syscalls:sys_enter_openat { printf("%s %s\n", comm,
    str(args->filename)); }'
 Attaching 1 probe...
 sa1 /etc/sysstat/sysstat
 sadc /etc/ld.so.cache
 sadc /lib/x86_64-linux-gnu/libsensors.so.5
 sadc /lib/x86_64-linux-gnu/libc.so.6
 sadc /lib/x86_64-linux-gnu/libm.so.6
 sadc /sys/class/i2c-adapter
 sadc /sys/bus/i2c/devices
 sadc /sys/class/hwmon
 sadc /etc/sensors3.conf
 [...]

此输出捕获了用于存档统计信息的 sar（1） 的执行，以及它正在打开的文件。bpftrace 使用跟踪点中的 filename 参数;所有参数都可以使用 -lv 列出：

 # bpftrace -lv t:syscalls:sys_enter_openat
 tracepoint:syscalls:sys_enter_openat
    int __syscall_nr;
    int dfd;
    const char * filename;
    int flags;
    umode_t mode;

参数是系统调用号、文件描述符、文件名、打开标志和打开模式：足够的信息供单行代码和工具使用，比如 opensnoop（8）

# read(2)
read（2） 应该是了解文件系统读取延迟的有用跟踪目标。但是，请考虑 tracepoint 参数（看看您是否能发现问题）：

 # bpftrace -lv t:syscalls:sys_enter_read
 tracepoint:syscalls:sys_enter_read
    int __syscall_nr;
    unsigned int fd;
    char * buf;
    size_t count;
read（2） 可以针对文件系统、套接字、/proc 和其他目标调用，并且参数不区分它们。为了说明这有多令人困惑，下面按进程名称对 read（2） syscall 进行计数：

# bpftrace -e 't:syscalls:sys_enter_read { @[comm] = count(); }'
 Attaching 1 probe...
 ^C
 @[systemd-journal]: 13
 @[sshd]: 141
 @[java]: 3472

在跟踪时，Java 执行了 3,472 次 read（2） 系统调用，但它们是来自文件系统、套接字还是其他内容？（sshd 读取可能是套接字 I/O。

read（2） 提供的是整数形式的文件描述符 （FD），但它只是一个数字，不显示 FD 类型（并且 bpftrace 以受限内核模式运行：它无法在 /proc 中查找 FD 信息）。至少有四种解决方案：

■ 从 bpftrace 打印 PID 和 FD，然后用 lsof（8） 或 /proc 查找 FD 看看它们是什么。
■ 即将推出的 BPF 帮助程序 get_fd_path（） 可以返回 FD 的路径名。这将有助于区分文件系统读取（具有路径名）与其他类型的读取。
■ 而是从 VFS 进行跟踪，其中有更多数据结构可用。
■ 直接跟踪文件系统函数，这些函数不包括其他 I/O 类型。ext4dist（8） 和 ext4sslow（8） 都使用这种方法。

以下有关 VFS 延迟跟踪的部分显示了基于 VFS 的解决方案

# VFS 追踪
由于虚拟文件系统 （VFS） 抽象了所有文件系统（和其他设备），因此跟踪其调用提供了一个观察所有文件系统的单一点。

# VFS 计数
对 VFS 调用进行计数 提供了正在使用的作类型的高级概述。下面使用 kprobes 对以 “vfs_” 开头的内核函数进行计数：

# bpftrace -e 'kprobe:vfs_* { @[func] = count(); }'
 Attaching 65 probes...
 ^C
 [...]
 @[vfs_statfs]: 36
 @[vfs_readlink]: 164
 @[vfs_write]: 364
 @[vfs_lock_file]: 516
 @[vfs_iter_read]: 2551
 @[vfs_statx]: 3141
 @[vfs_statx_fd]: 4214
 @[vfs_open]: 5271
 @[vfs_read]: 5602
 @[vfs_getattr_nosec]: 7794
 @[vfs_getattr]: 7795

这显示了系统范围内发生的不同作类型。在追踪过程中，有 7,795 vfs_read （）

# VFS 延迟
与 syscall 一样，VFS 读取可以用于文件系统、套接字和其他目标。以下 bpftrace 程序从内核结构（inode 超级块名称）中获取类型，并按类型提供 vfs_read（） 延迟的细分（以微秒为单位）：

 # vfsreadlat.bt
 Tracing vfs_read() by type... Hit Ctrl-C to end.
 ^C
 [...] 
@us[sockfs]: 
[0]                  141 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
 [1]                   91 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                   |
 [2, 4)                57 |@@@@@@@@@@@@@@@@@@@@@                               |
 [4, 8)                53 |@@@@@@@@@@@@@@@@@@@                                 |
 [8, 16)               86 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                     |
 [16, 32)               2 |                                                    |
 [...]
 @us[proc]: 
[0]                  242 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
 [1]                   41 |@@@@@@@@                                            |
 [2, 4)                40 |@@@@@@@@                                            |
 [4, 8)                61 |@@@@@@@@@@@@@                                       |
 [8, 16)               44 |@@@@@@@@@                                           |
 [16, 32)              40 |@@@@@@@@                                            |
 [32, 64)               6 |@                                                   |
 [64, 128)              3 |                                                    |
  @us[ext4]: 
 [0]                  653 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@         |
 [1]                  447 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                      |
 [2, 4)                70 |@@@@                                                |
 [4, 8)               774 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|
 [8, 16)              417 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@                        |
 [16, 32)              25 |@                                                   |
 [32, 64)               7 |                                                    |
 [64, 128)            170 |@@@@@@@@@@@                                         |
 [128, 256)            55 |@@@                                                 |
 [256, 512)            59 |@@@                                                 |
 [512, 1K)            118 |@@@@@@@                                             |
 [1K, 2K)               3 |@@                                                  |

输出（截断）还包括以下各项的延迟直方图：sysfs、devpts、pipefs、devtmpfs、tmpfs 和 anon_inodefs

源代码为：

#!/usr/local/bin/bpftrace
 #include <linux/fs.h>
 BEGIN
 {
 }
        printf("Tracing vfs_read() by type... Hit Ctrl-C to end.\n");
 kprobe:vfs_read
 {
        @file[tid] = ((struct file *)arg0)->f_inode->i_sb->s_type->name;
        @ts[tid] = nsecs;
 }
 kretprobe:vfs_read
 /@ts[tid]/
 {
        @us[str(@file[tid])] = hist((nsecs - @ts[tid]) / 1000);
        delete(@file[tid]); delete(@ts[tid]);
 }
 END
  {
    clear(@file); clear(@ts);
 }

您可以扩展此工具以包含其他作，例如 vfs_readv（）、vfs_write（）、vfs_ writev（） 等。要理解此代码，请从第 15.2.4 节 编程开始，其中解释了 timing vfs_read（） 的基础知识。

请注意，此延迟可能会也可能不会直接影响应用程序性能，如部分 8.3.1 文件系统延迟中所述。这取决于延迟是在应用程序请求期间遇到，还是在异步后台任务期间发生。要回答这个问题，您可以将用户堆栈跟踪 （ustack） 作为额外的直方图键包含在内，这可能会揭示 vfs_read（） 调用是否发生在应用程序请求期间。

# 文件系统内部
如果需要，您可以开发自定义工具来显示文件系统内部的行为。首先尝试跟踪点（如果可用）。在 ext4 中列出它们：

 # bpftrace -l 'tracepoint:ext4:*'
 tracepoint:ext4:ext4_other_inode_update_time
 tracepoint:ext4:ext4_free_inode
 tracepoint:ext4:ext4_request_inode
 tracepoint:ext4:ext4_allocate_inode
 tracepoint:ext4:ext4_evict_inode
 tracepoint:ext4:ext4_drop_inode
 [...]

每个参数都有可以使用 -lv 列出的参数。如果跟踪点不足（或不适用于您的文件系统类型），请考虑将动态插桩与 kprobes 结合使用。列出 ext4 的 kprobe 目标

# bpftrace -lv 'kprobe:ext4_*'
 kprobe:ext4_has_free_clusters
 kprobe:ext4_validate_block_bitmap
 kprobe:ext4_get_group_number
 kprobe:ext4_get_group_no_and_offset
 kprobe:ext4_get_group_desc
 kprobe:ext4_wait_block_bitmap
 [...]

在这个内核版本 （5.3） 中，有 105 个 ext4 跟踪点和 538 个可能的 ext4 kprobe。

# #8.6.17 其他工具
表 8.7 中列出了本书其他章节和 BPF 性能工具 [Gregg 19] 中包含的文件系统可观测性工具。

其他与 Linux 文件系统相关的工具包括：
■ df（1）：报告文件系统使用情况和容量统计数据 
■ inotify：用于监控文件系统事件的 Linux 框架

除了操作系统提供的性能工具（例如 ZFS）之外，某些文件系统类型还具有自己特定的性能工具。

# ZFS
ZFS 附带了 zpool（1M），它有一个 iostat 子选项，用于观察 ZFS 池统计信息。它报告池作速率 （读取和写入） 和吞吐量。

一个流行的附加组件是 arcstat.pl 工具，它报告 ARC 和 L2ARC 的大小以及命中率和未命中率。例如：

# $ arcstat 1
    time  read  miss  miss%  dmis  dm%  pmis  pm%  mmis  mm%  arcsz     c
 04:45:47     0     0      0     0    0     0    0     0    0    14G   14G
 04:45:49   15K    10      0    10    0     0    0     1    0    14G   14G
 04:45:50   23K    81      0    81    0     0    0     1    0    14G   14G
 04:45:51   65K    25      0    25    0     0    0     4    0    14G   14G
 [...]

统计数据是按间隔计算的，它们是：
■ read,miss：ARC 访问总数，misses
■ miss%，dm%，pm%，mm%：ARC 未命中百分比总数，需求，预取，元数据 
■ dmis，mmis：需求未命中，预取，元数据 
■ arcsz，c：ARC 大小，ARC 目标大小 

arcstat.pl 是一个从 kstat 读取统计信息的 Perl 程序。

# #8.6.18 可视化
应用于文件系统的负载可以随时间绘制为折线图，以帮助识别基于时间的使用模式。为读取、写入和其他文件系统作绘制单独的图形可能很有用。

文件系统延迟的分布预计是双峰的：一种模式是低延迟的，用于文件系统缓存命中，另一种模式是高延迟的，用于缓存未命中（存储设备 I/O）。因此，将分布表示为单个值（如平均值、众数或中位数）具有误导性。

解决此问题的一种方法是使用显示完整分布的可视化效果，例如热图。热图在第 2 章 方法论， 第 2.10.3 节 热图中介绍。图 8.13 显示了一个示例文件系统延迟热图：它在 x 轴上显示了时间的流逝，在 y 轴上显示了 I/O 延迟 [Gregg 09a]。

此热图显示了启用 L2ARC 设备对 NFSv3 延迟的差异。L2ARC 设备是主内存之后的辅助 ZFS 高速缓存，通常使用闪存（部分 8.3.2， 高速缓存中提到过）。图 8.13 中的系统具有 128 GB 的主内存 （DRAM） 和 600 GB 的 L2ARC（读取优化型 SSD）。热图的左半部分显示没有 L2ARC 设备（L2ARC 已禁用），右半部分显示 L2ARC 设备的延迟。

对于左半部分，文件系统延迟为低或高，由间隙分隔。低延迟是底部的蓝线，大约 0 毫秒，这可能是主内存缓存命中。高延迟从大约 3 毫秒开始，一直延伸到顶部，显示为 “云”，这可能是旋转磁盘延迟。这种双峰延迟分布在由旋转磁盘支持时是文件系统延迟的典型特征。

对于右半部分，启用了 L2ARC，延迟现在通常低于 3 毫秒，并且较高的磁盘延迟更少。您可以看到 L2ARC 的延迟如何填补热图左侧有间隙的范围，从而降低整体文件系统延迟。

# 8.7 实验
本节介绍用于主动测试文件系统性能的工具。有关要遵循的建议策略，请参见第 8.5.8 节 Micro-Benchmarking。 

当使用这些工具时，最好让 iostat（1） 持续运行，以确认工作负载按预期到达磁盘，这可能意味着根本不运行。例如，当测试应该很容易适应文件系统缓存的工作集大小时，读取工作负载的预期结果是 100% 的缓存命中，因此 iostat（1） 不应显示大量的磁盘 I/O。iostat（1） 在第 9 章 磁盘中介绍。

# #8.7.1 临时
dd（1） 命令 （device-to-device copy） 可用于对顺序文件系统性能执行临时测试。以下命令写入并读取名为 file1 且 I/O 大小为 1 MB 的 1 GB 文件：

write: dd if=/dev/zero of=file1 bs=1024k count=1k 
read: dd if=file1 of=/dev/null bs=1024k 

Linux 版本的 dd（1） 会在完成时打印统计信息。例如

# $ dd if=/dev/zero of=file1 bs=1024k count=1k
 1024+0 records in
 1024+0 records out
 1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.76729 s, 1.4 GB/s

这显示文件系统写入吞吐量为 1.4 GB/s（正在使用回写缓存，因此这只会弄脏内存，稍后将刷新到磁盘，具体取决于 vm.dirty_* 可调设置：请参见第 7 章 内存， 第 7.6.1 节 可调参数）。

# #8.7.2 基准测试工具
有许多可用的文件系统基准测试工具，包括 Bonnie、Bonnie、iozone、tiobench、SysBench、fio 和 FileBench。这里按照复杂性增加的顺序讨论了一些。另请参阅第 12 章 基准测试。我个人的建议是使用 fio。

# Bonnie, Bonnie++
Bonnie 工具是一个简单的 C 程序，用于从单个线程在单个文件上测试多个工作负载。它最初是由蒂姆·布雷 （Tim Bray） 于 1989 年写的 [Bray 90]。用法很简单，不需要参数（将使用默认值）：

 $ ./Bonnie
 File './Bonnie.9598', size: 104857600
 [...]
              -------Sequential Output-------- ---Sequential Input-- --Random-
              -Per Char- --Block--- -Rewrite-- -Per Char- --Block--- --Seeks--
Machine    MB K/sec %CPU K/sec %CPU K/sec %CPU K/sec %CPU K/sec %CPU  /sec %CPU
          100 123396 100.0 1258402 100.0 996583 100.0 126781 100.0 2187052 100.0 
164190.1 299.0

输出包括每次测试期间的 CPU 时间，100% 表示 Bonnie 从未在磁盘 I/O 上阻塞，而是始终从缓存命中并保持在 CPU 上。原因是目标文件大小为 100 MB，它完全缓存在此系统上。您可以使用 -s size 更改文件大小

有一个名为 Bonnie-64 的 64 位版本，它允许测试更大的文件。还有一个 C 语言重写的 Bonnie 由 Russell Coker [Coker 01] 编写

不幸的是，像 Bonnie 这样的文件系统基准测试工具可能会产生误导，除非您清楚地了解正在测试的内容。第一个结果，即 putc（3） 测试，可以根据系统库实现而变化，然后系统库将成为测试的目标，而不是文件系统。请参见第 12 章 基准测试， 部分 12.3.2 主动基准测试中的示例。

# fio
Jens Axboe 的 Flexible IO Tester （fio） 是一个可定制的文件系统基准测试工具，具有许多高级功能 [Axboe 20]。导致我使用它而不是其他基准测试工具的两个是：

■ 非均匀随机分布，可以更准确地模拟真实世界的访问模式（例如，-random_distribution=pareto：0.9）■ 迟百分位数的报告，包括 99.00、99.50、99.90、99.95、99.99

下面是一个示例输出，显示了一个随机读取工作负载，具有 8 KB 的 I/O 大小、5 GB 的工作集大小和非统一访问模式 （pareto：0.9）

 # fio --runtime=60 --time_based --clocksource=clock_gettime --name=randread -
numjobs=1 --rw=randread --random_distribution=pareto:0.9 --bs=8k --size=5g -
filename=fio.tmp
 randread: (g=0): rw=randread, bs=8K-8K/8K-8K/8K-8K, ioengine=sync, iodepth=1
 fio-2.0.13-97-gdd8d
 Starting 1 process
 Jobs: 1 (f=1): [r] [100.0% done] [3208K/0K/0K /s] [401 /0 /0  iops] [eta 00m:00s]
 randread: (groupid=0, jobs=1): err= 0: pid=2864: Tue Feb  5 00:13:17 2013
  read : io=247408KB, bw=4122.2KB/s, iops=515 , runt= 60007msec
    clat (usec): min=3 , max=67928 , avg=1933.15, stdev=4383.30
     lat (usec): min=4 , max=67929 , avg=1934.40, stdev=4383.31
    clat percentiles (usec):
     |  1.00th=[    5],  5.00th=[    5], 10.00th=[    5], 20.00th=[    6],
     | 30.00th=[    6], 40.00th=[    6], 50.00th=[    7], 60.00th=[  620],
     | 70.00th=[  692], 80.00th=[ 1688], 90.00th=[ 7648], 95.00th=[10304],
     | 99.00th=[19584], 99.50th=[24960], 99.90th=[39680], 99.95th=[51456],
     | 99.99th=[63744]
    bw (KB/s)  : min= 1663, max=71232, per=99.87%, avg=4116.58, stdev=6504.45
    lat (usec) : 4=0.01%, 10=55.62%, 20=1.27%, 50=0.28%, 100=0.13%
    lat (usec) : 500=0.01%, 750=15.21%, 1000=4.15%
    lat (msec) : 2=3.72%, 4=2.57%, 10=11.50%, 20=4.57%, 50=0.92%
    lat (msec) : 100=0.05%
  cpu          : usr=0.18%, sys=1.39%, ctx=13260, majf=0, minf=42
  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=30926/w=0/d=0, short=r=0/w=0/d=0

延迟百分位数 （clat） 显示非常低的延迟，直到第 50 个百分位数，根据延迟（5 到 7 微秒），我假设这是缓存命中。其余百分位数显示缓存未命中的影响，包括队列的尾部;在这种情况下，第 99.99 个 Percen 磁贴显示 63 毫秒的延迟。

虽然这些百分位数缺乏真正理解什么是多模式分布的信息，但它们确实关注了最有趣的部分：较慢模式的尾部（磁盘 I/O）。

对于类似但更简单的工具，您可以尝试 SysBench（第 6 章第 6.8.2 节 SysBench 中使用 SysBench 进行 CPU 分析的示例）。另一方面，如果您想要更多控制权，请尝试 FileBench。

# FileBench
FileBench 是一种可编程的文件系统基准测试工具，可以通过使用其工作负载模型语言描述应用程序工作负载来模拟应用程序工作负载。这允许模拟具有不同行为的线程，并指定同步线程行为。它附带了各种配置，称为个性，包括用于模拟 Oracle 数据库 I/O 模型的配置。不幸的是，FileBench 不是一个易于学习和使用的工具，可能只有那些全职处理文件系统的人才会感兴趣。

# #8.7.3 缓存刷新
Linux 提供了一种从文件系统缓存中刷新（删除条目）的方法，这对于从一致的 “cold” 缓存状态对性能进行基准测试很有用，就像在系统引导后一样。这种机制在内核源文档 （Documentation/sysctl/vm.txt） 中非常简单地描述为

 To free pagecache:
        echo 1 > /proc/sys/vm/drop_caches
 To free reclaimable slab objects (includes dentries and inodes):
        echo 2 > /proc/sys/vm/drop_caches
 To free slab objects and pagecache:
        echo 3 > /proc/sys/vm/drop_caches

在其他基准测试运行之前释放所有内容 （3） 特别有用，这样系统就可以以一致的状态（冷缓存）开始，从而有助于提供一致的基准测试结果

# 8.8 调优
部分 8.5， 方法中已经介绍了许多调优方法，包括高速缓存调优和工作负载特征。后者可以通过识别和消除不必要的工作来获得最高的 tuning Wins。本节包括特定的优化参数 （tunables）。

调整的细节（可用选项和设置选项）取决于运行系统版本、文件系统类型和预期的工作负载。以下部分提供了可能可用的内容以及可能需要调整它们的原因的示例。我将介绍应用程序调用和两个示例文件系统类型：ext4 和 ZFS。有关页面高速缓存的调整，请参见第 7 章 “内存”。

# #8.8.1 应用程序调用
第 8.3.7 节 同步写入 提到了如何通过使用 fsync（2） 来刷新一个逻辑写入组来提高同步写入工作负载的性能，而不是在使用 O_DSYNC/open（2） 标志时单独刷新O_RSYNC。

其他可以提高性能的调用包括 posix_fadvise（） 和 madvise（2），它们为缓存资格提供提示

# posix_fadvise()
此库调用（fadvise64（2） syscall 的包装器）在文件的某个区域上运行，并具有函数原型：

 int posix_fadvise(int fd, off_t offset, off_t len, int advice);

建议可能如表 8.8 所示

内核可以使用此信息来提高性能，帮助它决定何时最好预取数据，何时最好缓存数据。这可以提高较高优先级数据的缓存命中率，如应用程序所建议的那样。有关通知参数的完整列表，请参阅系统上的手册页

# madvise()
此系统调用对内存映射进行作，其概要为：

 int madvise(void *addr, size_t length, int advice);

建议可能如表 8.9 所示。

与 posix_fadvise（） 一样，内核可以使用此信息来提高性能，包括做出更好的缓存决策。

# #8.8.2 ext4
在 Linux 上，可以通过以下四种方式之一优化 ext2、ext3 和 ext4 文件系统：

■ 挂载选项
■ tune2fs（8） 命令
■ /sys/fs/ext4 属性文件
■ e2fsck（8） 命令

# mount and tune2fs
挂载选项可以在挂载时设置，可以用 mount（8） 命令手动设置，也可以在引导时在 /boot/grub/menu.lst 和 /etc/fstab 中设置。可用选项位于 mount（8） 的手册页中。一些示例选项：

# man mount
 [...]
 FILESYSTEM-INDEPENDENT MOUNT OPTIONS
 [...]
       atime  Do not use the noatime feature, so the inode access time is con
              trolled  by  kernel  defaults.  See also the descriptions of the
              relatime and strictatime mount options.
       noatime
              Do not update inode access times on this  filesystem  (e.g.  for
              faster access on the news spool to speed up news servers).  This
 [...]
       relatime
              Update  inode  access  times  relative to modify or change time.
              Access time is only updated if the previous access time was ear
              lier  than  the  current  modify  or  change  time.  (Similar to
              noatime, but it doesn't break mutt or  other  applications  that
              need  to know if a file has been read since the last time it was
              modified.)
              Since Linux 2.6.30, the kernel defaults to the behavior provided
              by   this   option  (unless  noatime  was  specified),  and  the
              strictatime option is required to obtain traditional  semantics.
              In  addition, since Linux 2.6.30, the file's last access time is
              always updated if it is more than 1 day old.
 [...]

noatime 选项历来用于通过避免访问时间戳更新及其关联的磁盘 I/O 来提高性能。如此输出中所述，relatime 现在是默认值，这也会减少这些更新。

mount（8） 手册页涵盖了通用挂载选项和特定于文件系统的挂载选项;但是，对于 ext4，特定于文件系统的挂载选项有自己的手册页 ext4（5）：

 # man ext4
 [...]
 Mount options for ext4
 [...]
       The  options  journal_dev, journal_path, norecovery, noload, data, com
       mit, orlov, oldalloc, [no]user_xattr, [no]acl, bsddf,  minixdf,  debug,
       errors,  data_err,  grpid,  bsdgroups, nogrpid, sysvgroups, resgid, re
       suid, sb, quota, noquota, nouid32, grpquota, usrquota,  usrjquota,  gr
       pjquota, and jqfmt are backwardly compatible with ext3 or ext2.
       journal_checksum | nojournal_checksum
              The  journal_checksum option enables checksumming of the journal
              transactions.  This will allow the recovery code in  e2fsck  and
 [...]

可以使用 tune2fs -l device 和 mount （无选项） 查看当前的挂载设置。tune2fs（8） 可以设置或清除各种挂载选项，如它自己的手册页所述。

提高性能的常用挂载选项是 noatime：它避免了文件访问时间戳更新，如果文件系统用户不需要，这将减少后端 I/O。

# /sys/fs Property Files
一些额外的可调参数可以通过 /sys 文件系统实时设置。对于 ext4

 # cd /sys/fs/ext4/nvme0n1p1
 # ls
 delayed_allocation_blocks  last_error_time        msg_ratelimit_burst
 err_ratelimit_burst        lifetime_write_kbytes  msg_ratelimit_interval_ms
 err_ratelimit_interval_ms  max_writeback_mb_bump  reserved_clusters
 errors_count               mb_group_prealloc      session_write_kbytes
 extent_max_zeroout_kb      mb_max_to_scan         trigger_fs_error
 first_error_time           mb_min_to_scan         warning_ratelimit_burst
 inode_goal                 mb_order2_req          warning_ratelimit_interval_ms
 inode_readahead_blks       mb_stats
 journal_task               mb_stream_req
 # cat inode_readahead_blks
 32

此输出显示 ext4 将最多预读 32 个 inode 表块。并非所有这些文件都是可调参数：有些文件仅供参考。它们记录在 Linux 源代码的 Documentation/admin-guide/ext4.rst [Linux 20h] 下，其中还记录了挂载选项

# e2fsck
最后，e2fsck（8） 命令可用于重新索引 ext4 文件系统中的目录，这可能有助于提高性能。例如：

 e2fsck -D -f /dev/hdX

e2fsck（8） 的其他选项与检查和修复文件系统有关。

# #8.8.3 ZFS
ZFS 支持每个文件系统使用大量可调参数（称为属性），其中较小的参数数量可以在系统范围内设置。可以使用 zfs（1） 命令列出这些选项。例如：

 # zfs get all zones/var
 NAME       PROPERTY              VALUE                  SOURCE
 [...]
 zones/var  recordsize            128K                   default
 zones/var  mountpoint            legacy                 local
 zones/var  sharenfs              off                    default
 zones/var  checksum              on                     default
 zones/var  compression           off                    inherited from zones
 zones/var  atime                 off                    inherited from zones
 [...]

（截断的）输出包括属性名称、当前值和源的列。源显示了它的设置方式：它是从更高级别的 ZFS 数据集继承的，是默认值，还是为该文件系统在本地设置。

也可以使用 zfs（1M） 命令设置这些参数，并在其手册页中进行了介绍。表 8.10 中列出了与性能相关的关键参数。

要优化的最重要的参数通常是 record size，以匹配应用程序 I/O。它通常默认为 128 KB，这对于小型随机 I/O 来说可能效率低下。请注意，这不适用于小于记录大小的文件，这些文件使用等于其文件长度的动态记录大小进行保存。如果不需要这些时间戳，禁用 atime 也可以提高性能

ZFS 还提供了系统范围的可调参数，包括用于调整事务组 （TXG） 同步时间 （zfs_txg_synctime_ms、 zfs_txg_timeout） 以及元板切换到空间而不是时间优化分配 （metaslab_df_free_pct） 的阈值。将 TXG 调优为更小可以通过减少争用和与其他 I/O 的排队来提高性能

与其他内核可调参数一样，请查看其文档以获取完整列表、描述和警告。

# 8.9 练习
1. 回答以下有关文件系统术语的问题：
■ 逻辑 I/O 和物理 I/O 有什么区别？
■ 随机 I/O 和顺序 I/O 有什么区别？
■ 什么是直接 I/O？
■ 什么是非阻塞 I/O？
■ 工作集的大小是多少？

2. 回答以下概念性问题： 
■ VFS 的作用是什么？
■ 描述文件系统延迟，特别是可以从何处测量延迟。
■ 预取 （pre-pre-ahead） 的目的是什么？
■ 直接 I/O 的目的是什么？

3. 回答以下更深层次的问题：
■ 描述使用 fsync（2） 优于 O_SYNC 的优势。
■ 描述 mmap（2） 相对于 read（2）/write（2） 的优缺点。
■ 描述逻辑 I/O 在变为物理 I/O 时膨胀的原因。
■ 描述逻辑 I/O 在变为物理 I/O 时收缩的原因。
■ 说明文件系统写时复制如何提高性能。

4. 为您的操作系统制定以下程序：
■ 文件系统缓存调整清单。这应该列出存在的文件系统缓存、如何检查其当前大小和使用情况以及命中率。
■ 文件系统作的工作负载特征检查表。包括如何获取每个详细信息，并首先尝试使用现有的操作系统可观测性工具。

5. 执行以下任务：
■ 选择一个应用程序，并测量文件系统作和延迟。包括：q 文件系统作延迟的全分布，而不仅仅是平均值。问：每个应用程序线程在文件系统作中花费的每一秒部分。
■ 使用微基准测试工具，以实验方式确定文件系统缓存的大小。解释您在使用该工具时的选择。此外，还显示当工作集不再缓存时的性能下降（使用任何指标）。

6. （可选，高级）开发一个可观测性工具，为同步与异步文件系统写入提供指标。这应该包括它们的速率和延迟，并且能够识别发出它们的进程 ID，使其适合于工作负载特征描述。

7. （可选，高级）开发一个工具，为间接和膨胀的文件系统 I/O 提供统计信息：应用程序不直接发出的额外字节和 I/O。该工具应将此额外的 I/O 分解为不同的类型，以解释其原因。

# 8.10 引用
 [Ritchie 74] Ritchie, D. M., and Thompson, K., “The UNIX Time-Sharing System,” 
Communications of the ACM 17, no. 7, pp. 365–75, July 1974
 [Lions 77] Lions, J., A Commentary on the Sixth Edition UNIX Operating System, University of 
New South Wales, 1977
  [McKusick 84] McKusick, M. K., Joy, W. N., Leffler, S. J., and Fabry, R. S., “A Fast File System 
for UNIX.” ACM Transactions on Computer Systems (TOCS) 2, no. 3, August 1984.
 [Bach 86] Bach, M. J., The Design of the UNIX Operating System, Prentice Hall, 1986.
 [Bray 90] Bray, T., “Bonnie,” http://www.textuality.com/bonnie, 1990.
 [Sweeney 96] Sweeney, A., “Scalability in the XFS File System,” USENIX Annual Technical 
Conference, https://www.cs.princeton.edu/courses/archive/fall09/cos518/papers/xfs.pdf, 1996.
 [Vahalia 96] Vahalia, U., UNIX Internals: The New Frontiers, Prentice Hall, 1996.
 [Coker 01] Coker, R., “bonnie++,” https://www.coker.com.au/bonnie++, 2001.
 [XFS 06] “XFS User Guide,” https://xfs.org/docs/xfsdocs-xml-dev/XFS_User_Guide/tmp/
 en-US/html/index.html, 2006.
 [Gregg 09a] Gregg, B., “L2ARC Screenshots,” http://www.brendangregg.com/blog/
 2009-01-30/l2arc-screenshots.html, 2009.
 [Corbet 10] Corbet, J., “Dcache scalability and RCU-walk,” LWN.net, http://lwn.net/
 Articles/419811, 2010.
 [Doeppner 10] Doeppner, T., Operating Systems in Depth: Design and Programming, Wiley, 2010.
 [XFS 10] “Runtime Stats,” https://xfs.org/index.php/Runtime_Stats, 2010.
 [Oracle 12] “ZFS Storage Pool Maintenance and Monitoring Practices,” Oracle Solaris 
Administration: ZFS File Systems, https://docs.oracle.com/cd/E36784_01/html/E36835/
 storage-9.html, 2012.
 [Ahrens 19] Ahrens, M., “State of OpenZFS,” OpenZFS Developer Summit 2019, https://
 drive.google.com/file/d/197jS8_MWtfdW2LyvIFnH58uUasHuNszz/view, 2019.
 [Axboe 19] Axboe, J., “Efficient IO with io_uring,” https://kernel.dk/io_uring.pdf, 2019.
 [Gregg 19] Gregg, B., BPF Performance Tools: Linux System and Application Observability, 
Addison-Wesley, 2019.
 [Axboe 20] Axboe, J., “Flexible I/O Tester,” https://github.com/axboe/fio, last updated 2020.
 [Linux 20h] “ext4 General Information,” Linux documentation, https://www.kernel.org/doc/
 html/latest/admin-guide/ext4.html, accessed 2020.
 [Torvalds 20a] Torvalds, L., “Re: Do not blame anyone. Please give polite, constructive 
 criticism,” https://www.realworldtech.com/forum/?threadid=189711&curpostid=189841, 
2020
