# Chapter 6: CPUS
CPU 驱动所有软件，通常是系统性能分析的第一个目标。本章介绍 CPU 硬件和软件，并说明如何详细检查 CPU 使用率以寻找性能改进。

在高级别上，可以监视系统范围的 CPU 利用率，并可以检查进程或线程的使用情况。在较低级别，可以分析和研究应用程序和内核中的代码路径，以及中断的 CPU 使用率。在最低级别，可以分析 CPU 指令执行和循环行为。还可以调查其他行为，包括任务等待 CPU 启动时的调度程序延迟，这会降低性能。

本章的学习目标是：
■ 熟悉 CPU 硬件内部结构。
■ 熟悉 CPU 调度程序内部结构。
■ 遵循不同的 CPU 分析方法。
■ 解释负载平均值和 PSI。
■ 描述系统范围和每个 CPU 的利用率。
■ 识别并量化调度程序延迟问题。
■ 执行 CPU 周期分析以识别效率低下的情况。
■ 使用分析器和 CPU 火焰图调查 CPU 使用率。
■ 识别软硬 IRQ CPU 使用者。
■ 解释 CPU 火焰图和其他 CPU 可视化效果。
■ 了解 CPU 可调参数。

本章分为六个部分。前三个参数为 CPU 分析提供了基础，后三个参数显示了它在基于 Linux 系统中的实际应用。这些部分是：

■ 背景介绍了与 CPU 相关的术语、CPU 的基本模型和关键的 CPU 性能概念。
■ 架构 引入了处理器和内核调度程序架构。
■ 方法描述了性能分析方法，包括观察性和实验性。
■ 可观测性工具介绍了基于 Linux 的系统上的 CPU 性能分析工具，包括分析、跟踪和可视化。
■ 实验总结了 CPU 基准测试工具。
■ 优化包括可调参数的示例

涵盖了内存 I/O 对 CPU 性能的影响，包括内存上停滞的 CPU 周期和 CPU 缓存的性能。第 7 章 内存 继续讨论内存 I/O，包括 MMU、NUMA/UMA、系统互连和内存总线。

# 6.1 术语
作为参考，本章中使用的 CPU 相关术语包括以下内容：

■ 处理器：插入系统或处理器板上的插槽的物理芯片，包含一个或多个作为内核或硬件线程实现的 CPU。
■ 核心：多核处理器上的独立 CPU 实例。使用内核是一种扩展处理器的方法，称为芯片级多处理 （CMP）。
■ 硬件线程：一种 CPU 架构，支持在单个内核上以 paral lel 方式执行多个线程（包括 Intel 的超线程技术），其中每个线程都是一个独立的 CPU 实例。这种扩展方法称为同步多线程 （SMT）。
■ CPU 指令：来自其指令集的单个 CPU作。有关于算术运算、内存 I/O 和控制逻辑的指令。
■ 逻辑 CPU：也称为虚拟处理器，操作系统 CPU 实例（调度的 CPU 实体）。这可以由处理器作为硬件线程实现（在这种情况下，它也可以称为虚拟内核）、内核或单核处理器。
■ Scheduler：分配线程在 CPU 上运行的内核子系统。
■ Run queue：等待 CPU 提供服务的可运行线程队列。现代内核可能会使用其他数据结构（例如，红黑树）来存储可运行的线程，但我们仍然经常使用术语 run queue

本章介绍了其他术语。词汇表包括供参考的基本术语，包括 CPU、CPU 周期和堆栈。另请参阅第 2 章和第 3 章中的术语部分。

# 6.2 模型
以下简单模型说明了 CPU 和 CPU 性能的一些基本原理。第 6.4 节 体系结构 进行了更深入的挖掘，并包括特定于实现的细节

# #6.2.1 CPU 架构
图 6.1 显示了一个 CPU 架构示例，对于总共具有 4 个内核和 8 个硬件线程的单个处理器。图中显示了物理架构，以及操作系统如何看待它。

每个硬件线程都可以作为逻辑 CPU 进行寻址，因此此处理器显示为 8 个 CPU。操作系统可能具有一些额外的拓扑知识，以改进其调度决策，例如哪些 CPU 位于同一内核上以及如何共享 CPU 缓存。

# #6.2.2 CPU 内存缓存
处理器提供各种硬件高速缓存，以提高内存 I/O 性能。图 6.2 显示了缓存大小的关系，缓存大小越靠近 CPU，缓存大小就越小、越快（一种权衡）。

存在的高速缓存，以及它们是在处理器 （集成） 上还是在处理器外部，都取决于处理器类型。早期的处理器提供的集成缓存级别较少。

# #6.2.3 CPU 运行队列
图 6.3 显示了一个 CPU 运行队列，它由内核调度程序管理。

图中所示的线程状态 ready to run 和 on-CPU 在第 3 章 “操作系统”中的图 3.8 中进行了介绍。

已排队并准备运行的软件线程数是指示 CPU 饱和的重要性能指标。在这个图中（此时此刻），有四个线程，还有一个额外的线程在 CPU 上运行。等待 CPU 运行队列所花费的时间有时称为 run-queue 延迟或 dispatcher-queue 延迟。在本书中，经常使用术语 scheduler latency，因为它适用于所有 scheduler，包括那些不使用队列的 scheduler（参见 Section 6.4.2， Software中对 CFS 的讨论）。

对于多处理器系统，内核通常为每个 CPU 提供一个运行队列，旨在将线程保持在同一运行队列上。这意味着线程更有可能在 CPU 缓存缓存其数据的同一 CPU 上继续运行。这些缓存被描述为具有缓存暖度，这种使线程保持在同一 CPU 上运行的策略称为 CPU 关联。在 NUMA 系统上，每 CPU 运行队列还提高了内存局部性。这通过使线程运行在同一个内存节点上来提高性能（如第 7 章 内存中所述），并避免了队列作的线程同步（互斥锁）成本，如果运行队列是全局的并在所有 CPU 之间共享，这将损害可伸缩性。

# 6.3 概念
以下是有关 CPU 性能的一些重要概念，首先是处理器内部的摘要：CPU 时钟速率和如何执行指令。这是以后性能分析的背景，特别是用于理解每个周期的指令数 （IPC） 指标。

# #6.3.1 时钟速率
clock 是驱动所有处理器 logic的数字信号。每个 CPU 指令可能需要一个或多个 clock 周期（称为 CPU cycles）来执行。CPU 以特定的时钟速率执行;例如，一个 4 GHz CPU 每秒执行 40 亿个时钟周期。

一些处理器能够改变其时钟速率，增加它以提高性能或降低它以降低功耗。该速率可以根据操作系统的要求改变，也可以由处理器本身动态改变。例如，内核空闲线程可以请求 CPU 降低节流以节省功耗。

时钟速率通常作为处理器的主要功能进行销售，但这可能会有点误导。即使系统中的 CPU 似乎被充分利用（瓶颈），更快的时钟速率也可能不会提高性能——这取决于那些快速的 CPU 周期实际做什么。如果它们在等待内存访问时大部分是停顿周期，那么更快地执行它们实际上不会增加 CPU 指令速率或工作负载吞吐量

# #6.3.2 指令
CPU 执行从其指令集中选择的指令。指令包括以下步骤，每个步骤都由 CPU 的一个组件（称为功能单元）处理：

1. 指令获取 
2. 指令解码 
3. 执行 
4. 内存访问 
5. 寄存器回写

最后两个步骤是可选的，具体取决于说明。许多指令仅在 registers 上运行，不需要 memory access 步骤。

这些步骤中的每一个都至少需要一个 clock cycle 才能执行。内存访问通常是最慢的，因为可能需要数十个 clock cycles 才能读取或写入主内存，在此期间 instruction 执行已停止（这些停止的周期称为 stall cycles）。这就是为什么 CPU 高速缓存很重要的原因，如部分 6.4.1， 硬件中所述：它可以显著减少内存访问所需的周期数

# #6.3.3 指令管道
指令流水线是一种 CPU 架构，可以通过同时执行不同指令的不同组件来并行执行多条指令。它类似于工厂装配线，其中生产阶段可以并行执行，从而提高吞吐量。

考虑前面列出的指令步骤。如果每个都需要一个 clock cycle，则需要五个 cycles 才能完成 instruction。在此指令的每个步骤中，只有一个功能单元处于活动状态，四个功能单元处于空闲状态。通过使用流水线，多个功能单元可以同时处于活动状态，处理流水线中的不同指令。理想情况下， processor 可以在每个 clock cycle中完成一条指令。

指令流水线可能涉及将指令分解为多个简单的步骤以并行执行。（根据处理器的不同，这些步骤可能会变成称为微作 （uOps） 的简单操作，由称为后端的处理器区域执行。这种处理器的前端负责获取指令和分支预测。

# 分支预测
现代处理器可以执行流水线的乱序执行，其中后面的指令可以完成，而前面的指令会停滞，从而提高指令吞吐量。但是，条件分支指令会带来问题。分支指令将执行跳转到不同的指令，而条件分支则基于测试进行执行。对于条件分支，处理器不知道后面的指令会是什么。作为优化，处理器通常会实现分支预测，他们将猜测测试的结果并开始处理结果指令。如果后来证明猜测是错误的，则必须丢弃 INSTRUCTION PIPELINE 中的进度，从而损害性能。为了提高正确猜测的机会，程序员可以在代码中放置提示（例如，Linux 内核源代码中的 likely（） 和 unlikely（） 宏）。

# #6.3.4 指令宽度
但我们还可以走得更快。可以包含多个相同类型的 functional unit，以便每个 clock cycle有更多的 instructions 可以向前推进。这种 CPU 架构称为超标量，通常与流水线一起使用，以实现高指令吞吐量。

指令宽度 描述要并行处理的目标指令数。现代处理器是 3 宽或 4 宽的，这意味着它们每个周期最多可以完成 3 或 4 条指令。其工作原理取决于处理器，因为每个阶段的功能单元数量可能不同。

# #6.3.5 指令大小
另一个指令特性是指令大小：对于某些处理器架构，它是可变的：例如，被归类为复杂指令集计算机 （CISC） 的 x86 最多允许 15 字节的指令。ARM 是一种精简指令集计算机 （RISC），具有用于 AArch32/A32 的 4 字节指令和 4 字节对齐指令，以及用于 ARM Thumb 的 2 字节或 4 字节指令。

# #6.3.6 SMT
同步多线程利用超标量架构和硬件多线程支持（由处理器提供）来提高并行性。它允许 CPU 内核运行多个线程，在指令期间有效地在它们之间进行调度，例如，当一条指令在内存 I/O 上停顿时。内核将这些硬件线程呈现为虚拟 CPU，并像往常一样在这些线程上调度线程和进程。这在 第 6.2.1 节 CPU 体系结构 中介绍和说明。

一个示例实现是 Intel 的 Hyper-Threading Technology，其中每个内核通常有两个硬件线程。另一个例子是 POWER8，它的每个内核有 8 个硬件线程。

每个硬件线程的性能与单独的 CPU 内核不同，具体取决于工作负载。为避免性能问题，内核可能会将 CPU 负载分散到内核之间，以便每个内核上只有一个硬件线程繁忙，从而避免硬件线程争用。停顿周期较重（低 IPC）的工作负载也可能比指令较多（高 IPC）的工作负载具有更好的性能，因为停顿周期减少了内核争用。

# #6.3.7 IPC, CPI
每个周期的指令数 （IPC） 是一个重要的高级指标，用于描述 CPU 如何花费其时钟周期以及了解 CPU 利用率的性质。该指标也可以表示为每条指令的周期数 （CPI），与 IPC 相反。IPC 更常被 Linux 社区和 Linux perf（1） 分析器使用，而 CPI 更常被 Intel 和其他地方使用。

低 IPC 表示 CPU 经常停滞，通常用于内存访问。高 IPC 表明 CPU 通常不会停顿并且具有高指令吞吐量。这些指标表明性能优化工作可能最好花在何处。

例如，可以通过安装更快的内存 （DRAM）、改进内存局部性（软件配置）或减少内存 I/O 量来改善内存密集型工作负载。安装具有更高时钟速率的 CPU 可能无法将性能提高到预期的程度，因为 CPU 可能需要等待相同的时间才能完成内存 I/O。Put 不同，更快的 CPU 可能意味着更多的停顿周期，但每秒完成指令的速率相同。

IPC 的高值或低值取决于处理器和处理器功能，可以通过运行已知工作负载通过实验确定。例如，您可能会发现低 IPC 工作负载使用 0.2 或更低的 IPC 运行，而高 IPC 工作负载使用超过 1.0 的 IPC 运行（这可能是由于前面描述的指令流水线和宽度造成的）。在 Netflix，云工作负载的 IPC 从 0.2（被认为慢）到 1.5（被认为良好）不等。以 CPI 表示，此范围为 5.0 到 0.66。

应该注意的是，IPC 显示的是指令处理的效率，而不是结构本身的效率。考虑一个软件更改，它添加了一个低效的软件循环，该循环主要在 CPU 寄存器上运行（无停顿周期）：这样的更改可能会导致更高的整体 IPC，但也会导致更高的 CPU 使用率和利用率

# #6.3.8 利用率
CPU 利用率是通过 CPU 实例在时间间隔内忙于执行工作的时间来衡量的，以百分比表示。它可以衡量为 CPU 未运行内核空闲线程，而是运行用户级应用程序线程或其他内核线程或处理中断的时间。

高 CPU 利用率不一定是问题，而是系统正在工作的迹象。有些人还认为这是一个投资回报率 （ROI） 指标：高利用率的系统被认为具有良好的 ROI，而闲置的系统被认为被浪费了。与其他资源类型（磁盘）不同，在高利用率下，性能不会急剧下降，因为内核支持优先级、抢占和分时。这些共同使内核能够了解什么具有更高的优先级，并确保它首先运行。

CPU 利用率的度量跨越符合条件的活动的所有 clock cycles，包括内存停顿周期。这可能会产生误导：CPU 可能利用率很高，因为它经常停滞在等待内存 I/O 时，而不仅仅是执行指令，如上一节所述。Netflix 云就是这种情况，其中 CPU 利用率主要是内存停顿周期 [Gregg 17b]。

CPU 利用率通常分为单独的内核时间和用户时间指标

# #6.3.9 用户时间/内核时间
执行用户级软件所花费的 CPU 时间称为用户时间，内核级软件称为内核时间。内核时间包括系统调用、内核线程和中断期间的时间。在整个系统中测量时，用户时间/内核时间比率表示执行的工作负载类型。

计算密集型应用程序可能几乎将所有时间都花在执行用户级代码上，并且用户/内核比率接近 99/1。示例包括图像处理、机器学习、基因组学和数据分析。

I/O 密集型应用程序具有较高的系统调用速率，这些调用执行内核代码来执行 I/O。例如，执行网络 I/O 的 Web 服务器可能具有大约 70/30 的用户/内核比率。

这些数字取决于许多因素，包括这些数字是为了表示预期的比率类型。

# #6.3.10 饱和
利用率为 100% 的 CPU 已饱和，线程在等待在 CPU 上运行时将遇到计划程序延迟，从而降低整体性能。此延迟是在 CPU 运行队列或用于管理线程的其他结构上等待所花费的时间。

另一种形式的 CPU 饱和涉及 CPU 资源控制，这可能在多租户云计算环境中实施。虽然 CPU 可能未得到 100% 的利用，但已达到施加的限制，可运行的线程必须等待轮到它们。这对系统用户的可见程度取决于所使用的虚拟化类型;请参见第 11 章 “云计算”。

与其他资源类型相比，在饱和状态下运行的 CPU 问题较小，因为更高优先级的工作可以抢占当前线程

# #6.3.11 抢占
第 3 章 “操作系统”中介绍的抢占允许优先级较高的线程抢占当前正在运行的线程，并改为开始自己的执行。这消除了更高优先级工作的运行队列延迟，从而提高了其性能

# #6.3.12 优先级倒置
当较低优先级的线程持有资源并阻止较高优先级的线程运行时，会发生优先级倒置。这会降低优先级较高的工作的性能，因为它被阻止等待。

这可以使用 priority 继承方案来解决。下面是一个工作原理示例（基于实际情况）
1. 线程 A 执行监控，优先级较低。它获取 production 数据库的地址空间锁，以检查内存使用情况。
2. 线程 B（执行系统日志压缩的例行任务）开始运行。
3. 没有足够的 CPU 来运行两者。线程 B 抢占 A 并运行。
4. 线程 C 来自生产数据库，具有高优先级，并且一直在休眠等待 I/O。此 I/O 现在完成，将线程 C 重新置于 runnable 状态。
5. 线程 C 抢占 B 并运行，但随后阻塞了线程 A 持有的地址空间锁。线程 C 离开了 CPU
6. 调度程序选择下一个优先级最高的线程来运行：B. 
7. 线程 B 运行时，高优先级线程 C 实际上被低优先级线程 B 阻塞。这就是优先级倒置。
8. 优先级继承为线程 A 提供线程 C 的高优先级，抢占线程 B，直到它释放锁。线程 C 现在可以运行。

从 2.6.18 开始，Linux 提供了一个支持优先级继承的用户级互斥锁，用于实时工作负载 [Corbet 06a]

# #6.3.13 多进程，多线程
大多数处理器都提供某种形式的多个 CPU。要使应用程序使用它们，它需要单独的执行线程，以便它可以并行运行。例如，对于 64 个 CPU 的系统，这可能意味着如果应用程序可以并行使用所有 CPU 或处理 64 倍的负载，则应用程序的执行速度可以提高 64 倍。应用程序能够随着 CPU 数量的增加而有效扩展的程度是衡量可扩展性的指标。

跨 CPU 扩展应用程序的两种技术是多进程和多线程，如图 6.4 所示。（请注意，这是软件多线程处理，而不是前面提到的基于硬件的 SMT。

在 Linux 上，可以使用多进程模型和多线程模型，并且两者都由任务实现

多进程和多线程之间的区别如表 6.1 所示

尽管 multithreading 具有表中所示的所有优点，但通常认为多线程处理更优越，但开发人员的实现要复杂得多。多线程编程在 [Stevens 13] 中介绍

无论使用哪种技术，都必须创建足够的进程或线程来跨越所需数量的 CPU，为了获得最佳性能，可能是所有可用的 CPU。当线程同步和减少内存局部性 （NUMA） 的成本超过在更多 CPU 上运行的好处时，某些应用程序在较少的 CPU 上运行时可能会表现得更好。

第 5 章 应用程序， 第 5.2.5 节 并发和并行性中也讨论了并行体系结构，其中还总结了协程。

# #6.3.14 字长
处理器是围绕最大字长 — 32 位或 64 位 — 即整数大小和寄存器大小而设计的。根据处理器的不同，字长也常用于地址空间大小和数据路径宽度（有时称为位宽）。

更大的尺寸可能意味着更好的性能，尽管它并不像听起来那么简单。较大的大小可能会导致某些数据类型中未使用的位的内存开销。当指针大小 （字大小） 增加时，数据占用空间也会增加，这可能需要更多的内存 I/O。对于 x86 64 位体系结构，这些开销通过寄存器的增加和更高效的寄存器调用约定来补偿，因此 64 位应用程序可能比其 32 位版本更快。

处理器和操作系统可以支持多种字长，并且可以同时运行为不同字长编译的应用程序。如果软件已针对较小的字长进行编译，则它可能会成功执行，但性能相对较差

# #6.3.15 编译器优化
通过编译器选项（包括设置字长）和优化，可以显著改善应用程序的 CPU 运行时间。编译器也经常更新，以利用最新的 CPU 指令集并实施其他优化。有时，只需使用较新的编译器即可显著提高应用程序性能。

第 5 章 应用程序， 第 5.3.1 节 编译语言中更详细地介绍了此主题。

# 6.4 架构
本节介绍硬件和软件的 CPU 架构和实现。第 6.2 节 模型 中介绍了简单的 CPU 模型，上一节中介绍了通用概念。

在这里，我总结了这些主题作为性能分析的背景。有关更多详细信息，请参阅 ven dor 处理器手册和有关操作系统内部结构的文档。本章末尾列出了一些内容。

# #6.4.1 硬件
CPU 硬件包括处理器及其子系统，以及用于多处理器系统的 CPU 互连。

# 处理器
通用双核处理器的组件如图 6.5 所示

控制单元是 CPU 的核心，执行指令获取、解码、管理执行和存储结果。

此示例处理器描述了一个共享的浮点单元和（可选的）共享的 Level 3 缓存。处理器中的实际组件将根据其类型和型号而有所不同。可能存在的其他与性能相关的组件包括：

■ P-cache： 预取缓存（每个 CPU 内核） 
■ W-cache： 写入缓存（每个 CPU 内核） 
■ 时钟：CPU 时钟的信号发生器（或外部提供） 
■ 时间戳计数器：用于高分辨率时间，由时钟递增 
■ Microcode ROM：快速将指令转换为电路信号 
■ 温度传感器：用于热监控 
■ 网络接口：如果片上存在（用于高性能）

某些处理器类型使用温度传感器作为独立内核动态超频的输入（包括 Intel Turbo Boost 技术），在内核保持其温度包络的同时提高时钟速率。可能的 clock rates 可以由 P-state 定义。

# P 状态和 C 状态
Intel 处理器使用的高级配置和电源接口 （ACPI） 标准定义了处理器性能状态 （P 状态） 和处理器电源状态 （C 状态） [ACPI 17]。

P 状态通过改变 CPU 频率在正常执行期间提供不同级别的性能：P0 是最高频率（对于某些 Intel CPU，这是最高的“turbo boost”级别），P1...N 是低频状态。这些状态可以通过硬件（例如，基于处理器温度）或软件（例如，内核省电模式）来控制。当前工作频率和可用状态可以使用特定于模型的寄存器 （MSR） 来观察（例如，使用 Section 6.6.10 中的 showboost（8） 工具，showboost）。

C 状态在执行停止时提供不同的空闲状态，从而节省电量。C 状态如表 6.2 所示：C0 用于正常作，C1 及以上用于空闲状态：数字越高，状态越深。

处理器制造商可以定义 C3 之外的其他状态。一些 Intel 处理器定义了 C10 之前的附加级别，其中更多的处理器功能被关闭，包括高速缓存内容。

# CPU 缓存
各种硬件缓存通常包含在处理器中（称为片上、片上、嵌入式或集成）或与处理器（外部）一起。这些通过使用更快的内存类型来缓存读取和缓冲写入，从而提高内存性能。通用处理器的高速缓存访问级别如图 6.6 所示。

它们包括：
■ 1 级指令缓存 （I$）
■ 1 级数据缓存 （D$） 
■ 转换后备缓冲区 （TLB） 
■ 2 级缓存 （E$） 
■ 3 级缓存（可选）

E$ 中的 E 最初代表外部缓存，但随着 2 级缓存的集成，它后来被巧妙地称为嵌入式缓存。现在使用 “Level” 术语而不是 “E$” 风格的符号，这避免了这种混淆。

通常希望引用主内存之前的最后一个缓存，它可能是也可能不是 3 级。Intel 为此使用术语最后一级高速缓存 （LLC），也称为最长延迟高速缓存。

每个处理器上可用的缓存取决于其类型和型号。随着时间的推移，这些缓存的数量和大小一直在增加。表 6.3 对此进行了说明，其中列出了自 1978 年以来的英特尔处理器测试，包括高速缓存的进步 [英特尔 19a][英特尔 20a]

对于多核和多线程处理器，某些高速缓存可能在内核和线程之间共享。对于表 6.3 中的示例，自 Intel Xeon 7460 （2008） 以来的所有处理器都有多个 1 级和 2 级缓存，通常每个内核一个（表中的大小是指每个内核的缓存，而不是总大小）。

除了 CPU 缓存的数量和大小不断增加之外，还有一种趋势是在片上提供这些缓存，这样可以访问延迟可以最小化，而不是将它们提供给处理器外部。

# 延迟
多级缓存用于提供大小和延迟的最佳配置。Level 1 cache 的访问时间通常是几个 CPU clock cycles，而较大的 Level 2 cache 大约有十几个 clock cycles。主内存访问可能需要大约 60 ns（4 GHz 处理器大约需要 240 个周期），并且 MMU 的地址转换也会增加延迟。

处理器的 CPU 缓存延迟特性可以使用微基准测试 [Ruggiero 08] 通过实验确定。图 6.7 显示了这样做的结果，绘制了使用 LMbench [McVoy 12] 在增加的内存范围内测试的 Intel Xeon E5620 2.4 GHz 的内存访问延迟。

两个轴都是对数的。图表中的步骤显示了何时超过了缓存级别，并且访问延迟成为下一个（较慢的）缓存级别的结果。

# 关联性
Associativity 是一种高速缓存特征，用于描述在高速缓存中查找新条目的约束。类型包括：

■ 完全关联：缓存可以在任何位置找到新条目。例如，最近最少使用的 （LRU） 算法可用于整个缓存中的逐出。
■ Direct mapped：每个条目在缓存中只有一个有效位置，例如，内存地址的哈希值，使用地址位的子集在缓存中形成地址。
■ 设置关联性：通过映射（例如，哈希）来识别缓存的子集，在该子集中可以执行另一种算法（例如，LRU）。它是根据子集大小来描述的;例如，四向集合关联将地址映射到四个可能的位置，然后从这四个位置中选择最佳位置（例如，最近最少使用的位置）。

CPU 缓存通常使用设置关联性作为完全关联（执行成本高昂）和直接映射（命中率较差）之间的平衡。

# 缓存行
CPU 缓存的另一个特征是它们的缓存行大小。这是作为一个单元存储和传输的字节范围，从而提高内存吞吐量。x86 处理器的典型高速缓存行大小为 64 字节。编译器在优化性能时会考虑这一点。程序员有时也会这样做;请参见第 5 章 “应用程序”中的“哈希表”，部分 5.2.5 并发和并行性。

# 缓存一致性
内存可以同时缓存在不同处理器上的多个 CPU 缓存中。当一个 CPU 修改内存时，所有缓存都需要知道它们的缓存副本现在已过时，并且应该被丢弃，以便任何将来的读取都将检索新修改的副本。此过程称为缓存一致性，可确保 CPU 始终访问正确的内存状态。

缓存一致性的影响之一是 LLC 访问惩罚。以下示例作为粗略指南提供（这些来自 [Levinthal 09]）：

■ LLC 命中，线路未共享：~40 个 CPU 周期 
■ LLC 命中，另一个内核共享线路：~65 个 CPU 周期 
■ LLC 命中，另一个内核中的线路修改：~75 个 CPU 周期

高速缓存一致性是设计可扩展多处理器系统的最大挑战之一，因为内存可以快速修改。

# MMU
内存管理单元 （MMU） 负责虚拟到物理的地址转换

图 6.8 中显示了一个通用的 MMU 以及 CPU 缓存类型。该 MMU 使用片上转换后备缓冲区 （TLB） 来缓存地址转换。缓存未命中由主内存 （DRAM） 中的转换表（称为页表）满足，这些页表由 MMU（硬件）直接读取并由内核维护。

这些因素取决于处理器。一些（较旧的）处理器使用内核软件遍历页表来处理 TLB 未命中，然后使用请求的映射填充 TLB。此类软件可以维护自己的更大的翻译内存缓存，称为翻译存储缓冲区 （TSB）。较新的处理器可以处理硬件中的 TLB 未命中，从而大大降低其成本。

# 互连
对于多处理器体系结构，处理器使用共享系统总线或专用互连进行连接。这与系统的内存架构、统一内存访问 （UMA） 或 NUMA 有关，如第 7 章 内存 中所述。

早期 Intel 处理器使用的共享系统总线（称为前端总线）如图 6.9 中的四处理器示例所示。

当处理器数量增加时，由于争夺共享总线资源，使用系统总线会出现可伸缩性问题。现代服务器通常是多处理器 NUMA，并使用 CPU 互连。

互连模块可以连接处理器以外的组件，例如 I/O 控制器。示例互连包括 Intel 的 Quick Path Interconnect （QPI）、Intel 的 Ultra Path Interconnect （UPI）、AMD 的 HyperTransport （HT）、ARM 的 CoreLink Interconnects（有三种不同的类型）和 IBM 的 Coherent Accelerator Processor Interface （CAPI）。图 6.10 显示了一个用于四处理器系统的英特尔 QPI 架构示例

处理器之间的专用连接允许无竞争访问，并且还允许比共享系统总线更高的带宽。表 6.4 [Intel 09][Mulnix 17] 显示了 Intel FSB 和 QPI 的一些速度示例

为了解释传输速率与带宽的关系，我将解释 QPI 示例，该示例适用于 3.2 GHz 时钟。QPI 是双泵的，在时钟的上升沿和下降沿执行数据传输。这将使传输速率翻倍（3.2 GHz × 2 = 6.4 GT/s）。25.6 GB/s 的最终带宽用于发送和接收方向（6.4 GT/s × 2 个方向的 2 字节×宽度 = 25.6 GB/s）。

QPI 的一个有趣细节是，它的缓存一致性模式可以在 BIOS 中进行调整，选项包括优化内存带宽的 Home Snoop、优化内存延迟的 Early Snoop 和提高可扩展性的 Directory Snoop（它涉及跟踪共享的内容）。UPI 正在取代 QPI，仅支持目录窥探

除了外部互连之外，处理器还具有用于核心通信的内部互连。

互连通常设计为高带宽，因此它们不会成为系统瓶颈。如果这样做，性能将下降，因为 CPU 指令在涉及互连的作（例如远程内存 I/O）时遇到停顿周期。一个关键指标是 IPC 的下降。CPU 指令、周期、IPC、停顿周期和内存 I/O 可以使用 CPU 性能计数器进行分析。

# 硬件计数器 （PMC）
性能监控计数器 （PMC） 在第 4 章 可观测性工具， 第 4.3.9 节 硬件计数器 （PMC） 中总结为可观测性统计的来源。本节更详细地介绍它们的 CPU 实现，并提供其他示例。

PMC 是在硬件中实现的处理器寄存器，可以对其进行编程以计算低级 CPU 活动。它们通常包括以下各项的计数器：

■ CPU 周期：包括停顿周期和停顿周期的类型 
■ CPU 指令：停用（执行） 
■ 1、2、3 级缓存访问：命中、未命中
■ 浮点单元： 运算 
■ 内存 I/O： 读、写、停顿周期 
■ 资源 I/O： 读、写、停顿周期

每个 CPU 都有少量的 registers，通常在 2 到 8 之间，可以对其进行编程以记录此类事件。可用的选项取决于处理器类型和型号，并记录在处理器手册中

作为一个相对简单的示例，Intel P6 系列处理器通过四个特定于模型的寄存器 （MSR） 提供性能计数器。两个 MSR 是计数器，并且是只读的。其他两个 MSR 称为事件选择 MSR，用于对计数器进行编程，并且是读写的。性能计数器是 40 位寄存器，事件选择 MSR 是 32 位。事件选择 MSR 的格式如图 6.11 所示。

计数器由事件 select 和 UMASK 标识。事件选择标识要计数的事件类型，UMASK 标识子类型或子类型组。可以设置 OS 和 USR 位，以便仅在内核模式 （OS） 或用户模式 （USR） 下根据处理器保护环递增计数器。可以将 CMASK 设置为在计数器递增之前必须达到的事件阈值。

Intel 处理器手册（第 3B 卷 [Intel 19b]）列出了几十个事件，这些事件可以通过其 event-select 和 UMASK 值进行计数。表 6.5 中选定的示例提供了可以观察的不同目标（处理器功能单元）的概念，包括手册中的描述。您需要参考您当前的处理器手册，看看您实际上拥有什么。

还有很多很多计数器，特别是对于较新的处理器

另一个需要注意的处理器细节是它提供了多少个 hardware counter registers。例如，Intel Skylake 微架构为每个硬件线程提供 3 个固定计数器，为每个内核提供 8 个额外的可编程计数器（“通用”）。这些是读取时的 48 位计数器。

有关 PMC 的更多示例，请参阅 Intel 架构集第 4.3.9 节中的表 4.4。Section 4.3.9 还为 AMD 和 ARM 处理器供应商提供了 PMC 参考。

# GPUs
图形处理单元 （GPU） 旨在支持图形显示，现在正用于其他工作负载，包括人工智能、机器学习、分析、图像处理和加密货币挖掘。对于服务器和云实例，GPU 是一种类似处理器的资源，可以执行工作负载的一部分，称为计算内核，适用于高度并行的数据处理，例如矩阵转换。Nvidia 使用其计算统一设备架构 （CUDA） 的通用 GPU 已得到广泛采用。CUDA 提供用于使用 Nvidia GPU 的 API 和软件库。

一个处理器 （CPU） 可能包含十几个内核，而一个 GPU 可能包含数百或数千个称为流处理器 （SP） 的较小内核，5每个内核都可以执行一个线程。由于 GPU 工作负载是高度并行的，因此可以并行执行的线程被分组到线程块中，它们可以在线程块中相互协作。这些线程块可以由称为流式多处理器 （SM） 的 SP 组执行，这些 SP 还提供其他资源，包括内存缓存。表 6.6 进一步比较了处理器 （CPU） 和 GPU [附录 19]。

必须使用自定义工具来实现 GPU 可观测性。可能的 GPU 性能指标包括每个周期的指令数、缓存命中率和内存总线利用率。

# 其他加速器
除了 GPU 之外，请注意，可能存在其他加速器，用于将 CPU 工作卸载到更快的特定于应用程序的集成电路。其中包括现场可编程门阵列 （FPGA）和张量处理单元 （TPU）。如果使用，则应与 CPU 一起分析它们的使用情况和性能，尽管它们通常需要自定义工具

GPU 和 FPGA 用于提高加密货币挖矿的性能。

# #6.4.2 软件
支持 CPU 的内核软件包括调度程序、调度类和空闲线程

# Scheduler
内核 CPU 调度器的关键功能如图 6.12 所示。

这些功能是：
■ 分时：可运行线程之间的多任务处理，首先执行优先级最高的线程。
■ Preemption：对于已变为可运行高优先级的线程，计划程序可以抢占当前正在运行的线程，以便可以立即开始执行较高优先级的线程。
■ 负载平衡：将可运行的线程移动到空闲或不太繁忙的 CPU 的运行队列中。

图 6.12 显示了 run queues，这是 scheduling 最初的实现方式。术语和心智模型仍然用于描述等待任务。但是，Linux CFS 调度器实际上使用未来任务执行的红/黑树。

在 Linux 中，时间共享由系统计时器中断通过调用 scheduler_tick（） 来驱动，该中断调用调度程序类函数来管理优先级和 CPU 时间单位（称为时间片）的过期时间。当线程变为可运行时，将触发抢占，并调用调度程序类 check_preempt_curr（） 函数。线程切换由 __schedule（） 管理，它通过 pick_next_task（） 选择优先级最高的线程进行运行。负载平衡由 load_balance（） 函数执行

Linux 计划程序还使用逻辑来避免在预期成本超过收益时进行迁移，宁愿让繁忙的线程在相同的 CPU 上运行，而 CPU 缓存仍应处于热状态（CPU 关联性）。在 Linux 源代码中，请参阅 idle_balance（） 和 task_hot（） 函数。

请注意，所有这些函数名称都可能更改;有关更多详细信息，请参阅 Linux 源代码，包括 Documentation 目录中的 docu mentation。

# Scheduling Classes
调度类管理可运行线程的行为，特别是它们的优先级、它们的 CPU 时间是否是时间切片的，以及这些时间片的持续时间（也称为时间量）。还可以通过调度策略进行其他控制，这些控制可以在调度类中选择，并且可以控制相同优先级的线程之间的调度。图 6.13 描述了它们在 Linux 中的表现以及线程优先级范围

用户级线程的优先级受用户定义的 nice 值影响，可以设置该值以降低不重要工作的优先级（以便对其他系统用户友好）。在 Linux 中，nice 值设置线程的静态优先级，该优先级与调度程序计算的动态优先级是分开的。

对于 Linux 内核，调度类为：
■ RT：为实时工作负载提供固定和高优先级。内核支持用户级和内核级抢占，允许以低延迟分派 RT 任务。优先级范围为 0–99 （MAX_RT_PRIO-1）。
■ O（1）：O（1） 调度器是在 Linux 2.6 中引入的，作为用户进程的默认分时调度器。该名称来自 O（1） 的算法复杂度（有关大 O 表示法的摘要，请参见第 5 章 “应用程序”）。之前的调度程序包含迭代所有任务的例程，使其成为 O（n），这成为一个可伸缩性问题。O（1） 调度程序动态提高了 I/O 密集型工作负载的优先级，而不是 CPU 密集型工作负载，以减少交互式工作负载和 I/O 工作负载的延迟。
■ CFS：完全公平的调度已添加到 Linux 2.6.23 内核中，作为用户进程的默认分时调度程序。计划程序管理以任务 CPU 时间为键的红黑树上的任务，而不是传统的运行队列。这使得可以轻松找到低 CPU 消耗器并优先执行 CPU 密集型工作负载，从而提高交互式工作负载和 I/O 密集型工作负载的性能。
■ Idle：以尽可能低的优先级运行线程。
■ Deadline：添加到 Linux 3.14 中，使用三个参数应用最早截止时间优先 （EDF） 调度：运行时间、周期和截止时间。任务应在每个周期微秒内接收运行时微秒的 CPU 时间，并在截止时间内执行此作。

要选择调度类，用户级进程使用 sched_setscheduler（2） syscall 或 chrt（1） 工具选择映射到类的调度策略。

调度程序策略为：
■ RR： SCHED_RR 是循环调度。一旦线程用完了其时间量程，它就会移动到该优先级的运行队列的末尾，从而允许具有相同优先级的其他线程运行。使用 RT 调度类。
■ FIFO：SCHED_FIFO先进先出调度，它继续运行运行队列开头的线程，直到它自愿离开，或者直到优先级更高的线程到达。线程将继续运行，即使具有相同优先级的其他线程位于运行队列中也是如此。使用 RT 类。
■ NORMAL：SCHED_NORMAL（以前称为 SCHED_OTHER）是分时调度，是用户进程的默认设置。调度器根据调度类动态调整先验性。对于 O（1），时间片持续时间是根据静态优先级设置的：优先级较高的工作会延长持续时间。对于 CFS，时间片是动态的。使用 CFS 调度类。
■ BATCH：SCHED_BATCH 类似于 SCHED_NORMAL，但预期线程将受 CPU 限制，不应计划该线程以中断其他受 I/O 限制的交互式工作。使用 CFS 调度类。
■ IDLE：SCHED_IDLE 使用 Idle 调度类。
■ DEADLINE：SCHED_DEADLINE 使用 Deadline 调度类。

随着时间的推移，可能会添加其他类和策略。已经研究了超线程感知 [Bulpin 05] 和温度感知 [Otto 06] 的调度算法，它们通过考虑额外的处理器因素来优化性能。

当没有要运行的线程时，将作为占位符执行特殊空闲任务（也称为空闲线程），直到另一个线程可运行为止。

# 空闲线程
在第 3 章中介绍，当没有其他可运行的线程并且具有尽可能低的优先级时，内核 “idle” 线程（或空闲任务）在 CPU 上运行。它通常被编程为通知处理器 CPU 执行可以停止（停止指令）或节流以节省功耗。CPU 将在下一次硬件中断时唤醒

# NUMA 分组
通过使内核能够识别 NUMA，可以显著提高 NUMA 系统上的性能，以便它可以做出更好的调度和内存放置决策。这可以自动检测和创建本地化的 CPU 和内存资源组，并将它们组织成 topol ogy 以反映 NUMA 架构。此拓扑允许估计任何内存访问的成本。

在 Linux 系统上，这些称为调度域，它们位于以根域开头的拓扑中。

系统管理员可以执行手动分组形式，方法是将进程绑定为仅在一个或多个 CPU 上运行，或者为要运行的进程创建一组独占 CPU。请参见部分 6.5.10， CPU 绑定

# 处理器资源感知
内核也可以理解 CPU 资源拓扑，以便它可以为电源管理、硬件缓存使用和负载平衡做出更好的调度决策。

# 6.5 方法论
本节介绍用于 CPU 分析和调整的各种方法和练习。表 6.7 总结了这些主题。

参见第 2 章 方法，了解更多方法和其中许多方法的介绍。你不需要用它们全部;将此视为食谱的说明书，可以单独遵循或组合使用。

我的建议是按以下顺序使用以下内容：性能监控、USE 方法、分析、微基准测试和静态性能调优。

第 6.6 节 可观测性工具以及后面的章节介绍了用于应用这些方法的操作系统工具。

# #6.5.1 工具方法
tools 方法是迭代可用工具的过程，检查它们提供的关键指标。虽然这是一种简单的方法，但它可能会忽略工具提供较差或没有可见性的问题，并且执行起来可能很耗时

对于 CPU，工具方法可能涉及检查以下内容 （Linux）：

■ uptime/top：检查负载平均值，查看负载是随时间推移而增加还是减少。使用以下工具时，请记住这一点，因为在分析过程中负载可能会发生变化。
■ vmstat：以 1 秒的间隔运行 vmstat（1） 并检查系统范围的 CPU 利用率 （“us” “sy”）。利用率接近 100% 会增加计划程序延迟的可能性。
■ mpstat：检查每个 CPU 的统计信息，并检查单个热（繁忙）CPU，从而确定可能的线程可扩展性问题。top：查看哪些进程和用户是 CPU 消耗量最大的进程和用户。
■ pidstat：将排名靠前的 CPU 使用者分为用户时间和系统时间。
■ perf/profile：分析用户时间或内核时间的 CPU 使用情况堆栈跟踪，以确定使用 CPU 的原因。perf：将 IPC 作为基于周期的低效率指标进行衡量。
■ showboost/turboboost：检查当前的 CPU 时钟速率，以防它们异常低。
■ dmesg：检查 CPU 温度停顿消息 （“cpu clock throttled”）。

如果发现问题，请检查可用工具中的所有字段以了解更多上下文。有关每个工具的更多信息，请参见部分 6.6， 可观测性工具

# #6.5.2 USE 方法
USE 方法可用于在性能调查的早期识别所有组件的瓶颈和错误，然后再尝试更深入、更耗时的策略

对于每个 CPU，请检查：
■ 利用率：CPU 繁忙的时间（不在空闲线程中） 
■ 饱和度：可运行线程排队等待轮到其开启的程度 
■ CPU 错误：CPU 错误，包括可纠正的错误

您可以先检查错误，因为它们通常检查速度快且最容易解释。某些处理器和操作系统会检测到可纠正错误（纠错码、ECC）的增加，并在不可纠正的错误导致 CPU 故障之前将 CPU 脱机作为预防措施。检查这些错误可能是检查所有 CPU 是否仍处于联机状态的问题。

利用率通常很容易从操作系统工具中获得，以繁忙百分比的形式。应按 CPU 检查此指标，以检查可扩展性问题。通过使用分析和周期分析，可以了解高 CPU 和内核利用率

对于实施 CPU 限制或配额（资源控制;例如 Linux 任务集和 cgroups）的环境，就像云计算环境中一样，除了物理限制之外，还应根据施加的限制来衡量 CPU 利用率。您的系统可能会在物理 CPU 达到 100% 利用率之前就耗尽其 CPU 配额，从而比预期更早地遇到饱和。

饱和度指标通常在系统范围内提供，包括作为负载平均值的一部分。此指标量化 CPU 过载的程度，或者 CPU 配额（如果存在）用完的程度。

您可以按照类似的过程来检查 GPU 和其他加速器（如果正在使用）的运行状况，具体取决于可用的指标

# #6.5.3 工作负载特征描述
描述应用的负载在容量规划、基准测试和模拟工作负载中非常重要。它还可以通过识别可以消除的不必要工作来带来一些最大的性能提升。

描述 CPU 工作负载的基本属性是：
■ CPU 负载平均值（利用率饱和） 
■ 用户时间与系统时间的比率 
■ 系统调用速率 
■ 自愿上下文切换速率 
■ 中断速率

目的是描述应用的负载，而不是交付的性能。某些操作系统（例如 Solaris）上的负载平均值仅显示 CPU 需求，这使它们成为 CPU 工作负载特征的主要指标。但是，在 Linux 上，负载平均值包括其他负载类型。请参阅第 6.6.1 节 uptime 中的示例和进一步说明

速率指标有点难以解释，因为它们既反映了应用的负载，也在某种程度上反映了交付的性能，这可能会限制其速率

用户时间与系统时间的比率显示应用的负载类型，如前面的部分 6.3.9 用户时间/内核时间中所述。高用户时间率是由于应用程序花费时间造成的执行自己的计算。高系统时间表示在内核中花费的时间，这可以通过 syscall 和 interrupt rate 进一步理解。与 CPU 绑定的工作负载相比，I/O 密集型工作负载具有更高的系统时间、系统调用和更高的自愿上下文切换，因为线程会阻塞等待 I/O。

下面是一个示例工作负载描述，旨在展示如何同时表示这些属性：

在平均 48 个 CPU 的应用程序服务器上，白天的平均负载在 30 到 40 之间变化。用户/系统比率为 95/5，因为这是 CPU 密集型工作负载。大约有 325 K 个系统调用/秒，大约有 80 K 个自愿上下文切换/秒

由于遇到不同的负载，这些特性会随着时间的推移而变化。

# 高级工作负载特征描述/核对表
可能包含其他详细信息来描述工作负载的特征。这些问题在此处列出为需要考虑的问题，在彻底研究 CPU 问题时，也可以作为检查表：

■ 系统范围内的 CPU 利用率是多少？每个 CPU？每个核心？
■ CPU 负载的并行度如何？它是单线程的吗？有多少个线程？
■ 哪些应用程序或用户正在使用 CPU？多少？
■ 哪些内核线程正在使用 CPU？多少？
■ 中断的 CPU 使用率是多少？
■ CPU 互连利用率是多少？
■ 为什么要使用 CPU（用户级和内核级调用路径）？
■ 会遇到哪些类型的失速循环？

参见第 2 章，方法论，了解该方法和要衡量的特征（谁、为什么、什么、如何）的更高层次的总结。以下各节扩展了此列表中的最后两个问题：如何使用 profiling 分析 and how can analyze call paths and use cycle analysis of stall cycles .

# #6.5.4 分析
Profiling 构建了要研究的目标的图片。CPU 性能分析可以通过不同的方式执行，通常为：

■ 基于计时器的采样：收集当前正在运行的函数或堆栈跟踪的基于计时器的样本。使用的典型速率是每个 CPU 99 赫兹（每秒样本数）。这提供了 CPU 使用情况的粗略视图，为大问题和小问题提供了足够的详细信息。99 用于避免在 100 赫兹时可能发生的锁步采样，这将产生偏斜的配置文件。如果需要，可以降低计时器速率并扩大时间跨度，直到开销可以忽略不计并适合生产使用。

■ 函数跟踪：检测所有或部分函数调用以测量其持续时间。这提供了一个精细的视图，但对于生产使用来说，开销可能令人望而却步，通常为 10% 或更多，因为函数跟踪会为每个函数调用添加插桩。

生产中使用的大多数分析器以及本书中的分析器都使用基于计时器的采样。如图 6.14 所示，其中应用程序调用函数 A（），函数 A（） 调用函数 B（），依此类推，同时收集堆栈跟踪样本。有关堆栈跟踪以及如何读取堆栈跟踪的说明，请参见第 3 章操作系统， 第 3.2.7 节 堆栈。

图 6.14 显示了如何仅在进程在 CPU 上时收集样本：两个样本显示函数 A（） 在 CPU 上，两个样本显示函数 B（） 在 CPU 上由 A（） 调用。未对系统调用期间的 CPU 关闭时间进行采样。此外，采样完全错过了短寿命函数 C（）

内核通常为进程维护两个堆栈跟踪：用户级堆栈和内核上下文中的内核堆栈（例如，syscalls）。对于完整的 CPU 配置文件，性能分析器必须在可用时记录两个堆栈。

除了对堆栈跟踪进行采样外，分析器还可以只记录指令指针，它显示 on-CPU 函数和指令偏移量。有时，这足以解决问题，而不会产生收集堆栈跟踪的额外开销。

# 样本处理
如第 5 章所述，Netflix 的典型 CPU 配置文件在 32 个 CPU（大约）上以 49 赫兹的频率收集用户和内核堆栈跟踪，持续 30 秒：这总共产生了 47,040 个样本，并带来了两个挑战：

1. 存储 I/O：分析器通常将样本写入配置文件，然后可以以不同的方式读取和检查该文件。但是，将如此多的样本写入文件系统可能会生成存储 I/O，从而影响生产应用程序的性能。这基于 BPF 的 profile（8） 工具通过汇总内核内存中的样本并仅发出摘要来解决存储 I/O 问题。不使用中间配置文件。

2. 理解：逐个阅读 47,040 条多行堆栈轨迹是不切实际的：必须使用摘要和可视化来理解配置文件。常用的堆栈跟踪可视化是火焰图，前面的章节（1 和 5）中显示了一些示例;本章还有更多例子

图 6.15 显示了从 perf（1） 和 profile生成 CPU 火焰图以解决理解问题的整体步骤。它还展示了如何解决存储 I/O 问题：profile（8） 不使用中间文件，从而节省了开销。使用的确切命令列在第 6.6.13 节 perf 中。

虽然基于 BPF 的方法开销较低，但 perf（1） 方法保存了原始样本（带有时间戳），可以使用不同的工具重新处理这些样本，包括 FlameScope（第 6.7.4 节）。

# 配置文件解释
收集、汇总或可视化 CPU 配置文件后，您的下一个任务是了解它并搜索性能问题。图 6.16 中显示了 CPU 火焰图的摘录，阅读此可视化的说明在第 6.7.3 节 火焰图中。您如何总结这些概况？

我在 CPU 火焰图中查找性能胜利的方法如下

1. 自上而下（从叶到根）寻找大的“平台”。这些结果表明，在许多样本中，单个函数都在 CPU 上，并且可以快速获胜。在图 6.16 中，右侧有两个平台，分别是 unmap_page_range（） 和 page_remove_rmap（），都与内存页有关。也许一个快速的胜利是将应用程序切换为使用大页面。
2. 自下而上地了解代码层次结构。在此示例中，bash（1） shell 调用 execve（2） 系统调用，后者最终调用页面函数。也许更大的好处是以某种方式避免使用 execve（2），例如使用 bash 内置进程而不是外部进程，或者切换到另一种语言。
3. 更仔细地从上到下查看分散但常见的 CPU 使用率。也许有很多小帧与同一个问题相关，比如锁争用。反转火焰图的合并顺序，使它们从叶合并到根并成为冰柱图，这有助于揭示这些情况。

第 5 章 应用程序， 第 5.4.1 节 CPU 性能分析中提供了解释 CPU 火焰图的另一个示例。

# 更多信息
部分 6.6， 可观察性工具中提供了用于 CPU 分析和火焰图的命令。另请参阅 Section 5.4.1 关于应用程序的 CPU 分析，以及 Section 5.6， Gotchas，其中描述了缺少堆栈跟踪和符号的常见分析问题。

对于特定 CPU 资源（如高速缓存和互连）的使用，性能分析可以使用基于 PMC 的事件触发器，而不是定时间隔。这将在下一节中介绍。

# #6.5.5 周期分析
您可以使用性能监控计数器 （PMC） 来了解周期级别的 CPU 利用率。这可能表明周期停滞在第 1 级、第 2 级或第 3 级高速缓存未命中、内存或资源 I/O 上，或者花费在浮点运算或其他活动上。此信息可能显示通过调整编译器选项或更改代码可以实现的性能提升

通过测量 IPC（CPI 的倒数）开始周期分析。如果 IPC 较低，请继续研究停顿周期的类型。如果 IPC 较高，请在代码中寻找减少执行指令的方法。“高”或“低”IPC 的值取决于您的处理器：low 可能小于 0.2，high 可能大于 1。您可以通过执行已知的内存 I/O 密集型或指令密集型工作负载来了解这些值，并测量每个工作负载的结果 IPC。

除了测量计数器值外，还可以将 PMC 配置为在给定值溢出时中断内核。例如，在每 10,000 次 3 级缓存未命中时，内核可能会中断以收集堆栈回溯。随着时间的推移，内核会构建导致 Level 3 缓存未命中的代码路径的配置文件，而不会产生测量每个 sin gle 未命中高昂的开销。这通常由集成开发人员环境 （IDE） 软件使用，以使用导致内存 I/O 和停顿周期的位置对代码进行注释。

如第 4 章 可观测性工具， 第 4.3.9 节 PMC 质询中所述，溢出采样可能会因滑移和无序执行而错过记录正确的指令。在 Intel 上，解决方案是 PEBS，Linux perf（1） 工具支持它。

周期分析是一项高级活动，使用命令行工具可能需要几天时间才能完成，如部分 6.6， 可观测性工具中所述。您还应该期望花一些时间阅读 CPU 供应商的处理器手册。性能分析器（如 Intel vTune [Intel 20b] 和 AMD uprof [AMD 20]）可以节省时间，因为它们经过编程以查找您感兴趣的 PMC。

# #6.5.6 性能监控
性能监控可以识别一段时间内的活动问题和行为模式。CPU 的关键指标包括：

■ 利用率：繁忙
■ 饱和度百分比：运行队列长度或计划程序延迟

应按每个 CPU 监控利用率，以识别线程可伸缩性问题。对于实施 CPU 限制或配额（资源控制）的环境，例如云计算环境，还应记录与这些限制的 CPU 使用率比较。

选择正确的时间间隔进行测量和存档是监控 CPU 使用率的一项挑战。一些监控工具使用 5 分钟的间隔，这可能会隐藏 CPU 利用率的较短突增的存在。最好使用每秒测量，但您应该注意，即使在一秒钟内也可能出现突发。这些可以从饱和度中识别出来，并使用 FlameScope（第 6.7.4 节）进行检查，该节是为亚秒级分析而创建的。

# #6.5.7 静态性能调优
静态性能优化侧重于已配置环境的问题。对于 CPU 性能，请检查静态配置的以下方面

■ 有多少个 CPU 可供使用？它们是核心吗？硬件线程？
■ GPU 或其他加速器是否可用且正在使用中？
■ CPU 架构是单处理器还是多处理器？
■ CPU 缓存的大小是多少？它们是否共享？
■ CPU 时钟速度是多少？它是动态的（例如，Intel Turbo Boost 和 SpeedStep）？这些动态功能在 BIOS 中是否已启用？
■ BIOS 中启用或禁用了哪些其他与 CPU 相关的功能？例如，turboboost、总线设置、省电设置？
■ 此处理器型号是否存在性能问题（错误）？它们是否列在处理器勘误表中？
■ 什么是微码版本？它是否包括针对安全漏洞（例如 Spectre/Meltdown）的影响性能的缓解措施？
■ 此 BIOS 固件版本是否存在性能问题（错误）？
■ 是否存在软件施加的 CPU 使用限制（资源控制）？它们是什么？

这些问题的答案可能会揭示以前被忽视的配置选择。

最后一个问题对于云计算环境尤其如此，因为 CPU 使用率通常受到限制。

# #6.5.8 优先级调整
Unix 一直提供一个 nice（2） 系统调用来调整进程优先级，它设置了一个 nice ness 值。正 nice 值会导致较低的进程优先级 （nicer），而负值（只能由超级用户 （root）7 设置）会导致更高的优先级。nice（1） 命令可用于启动具有 nice 值的程序，后来（在 BSD 中）添加了 renice（1M） 命令来调整已运行进程的 nice 值。Unix 第 4 版的手册页提供了以下示例 [TUHS 73]：

对于希望执行长时间运行的程序而没有 Administration 的 flak 的用户，建议使用值 16。

nice 值现在仍然可用于调整进程优先级。当存在 CPU 争用时，这最有效，这会导致高优先级工作的调度程序延迟。您的任务是确定低优先级工作，其中可能包括监视代理和计划备份，这些工作可以修改为以一个好的值开始。还可以执行分析以检查优化是否有效，以及高优先级工作的调度程序延迟是否保持较低。

除了 nice 之外，操操作系统还可以为进程优先级提供更高级的控制，例如更改调度程序类和调度程序策略以及可调参数。Linux 包括一个实时调度类，该类可以允许进程抢占所有其他工作。虽然这可以消除计划程序延迟（其他实时进程和中断除外），但请确保您了解后果。如果实时应用程序遇到多个线程进入无限循环的错误，则可能导致所有 CPU 无法进行所有其他工作，包括手动修复问题所需的管理 shell。

# #6.5.9 资源控制
操作系统可以提供精细的控制，用于将 CPU 周期分配给进程或进程组。这些可能包括 CPU 利用率和份额的固定限制，以实现更灵活的方法，允许根据份额值消耗空闲的 CPU 周期。这些工作原理是特定于实现的，在 Section 6.9， 调优 中进行了讨论。

# #6.5.10 CPU绑定
调整 CPU 性能的另一种方法涉及将进程和线程绑定到单个 CPU 或 CPU 集合。这可以增加进程的 CPU 高速缓存热度，从而提高其内存 I/O 性能。对于 NUMA 系统，它还可以提高内存局部性，进一步提高性能。

通常有两种执行方式

■ CPU 绑定：将进程配置为仅在单个 CPU 上运行，或仅在定义集中的一个 CPU 上运行。
■ 独占 CPU 集：对一组只能由分配给它们的进程使用的 CPU 进行分区。这可以进一步提高 CPU 缓存的热度，因为当进程空闲时，其他进程无法使用这些 CPU。

在基于 Linux 的系统上，可以使用 cpusets 实现独占 CPU 集方法。部分 6.9， 调整中提供了配置示例

# #6.5.11 微基准测试
用于 CPU 微基准测试的工具通常测量多次执行简单作所花费的时间。该作可能基于：

■ CPU 指令：整数运算、浮点运算、内存加载和存储、分支和其他指令 
■ 内存访问：调查不同 CPU 缓存的延迟和主内存吞吐量 
■ 高级语言：类似于 CPU 指令测试，但以更高级别的解释或编译语言编写
■ 操作系统：测试受 CPU 限制的系统库和系统调用函数，例如 getpid（2） 和进程创建

CPU 基准测试的一个早期示例是美国国家物理实验室 （National Physical Laboratory） 的 Whetstone，它于 1972 年用 Algol 60 编写，旨在模拟科学工作负载。Dhrystone 基准测试于 1984 年开发，用于模拟当时的整数工作负载，并成为比较 CPU 性能的流行方法。这些以及各种 Unix 基准测试（包括进程创建和管道吞吐量）都包含在一个名为 UnixBench 的集合中，该集合最初来自莫纳什大学，由 BYTE 杂志 [Hinnant 84] 出版。已经创建了更新的 CPU 基准测试来测试压缩速度、质数计算、加密和编码。

无论您使用哪种基准测试，在比较系统之间的结果时，了解真正测试的内容都很重要。前面列出的基准测试通常最终会测试编译器版本之间编译器优化的差异，而不是基准测试代码或 CPU 速度。许多基准测试以单线程方式执行，其结果在具有多个 CPU 的系统中失去了意义。（4 个 CPU 的系统可能比 8 个 CPU 系统进行基准测试稍快，但当提供足够的并行可运行线程时，后者可能会提供更大的吞吐量。

有关基准测试的更多信息，请参见第 12 章 基准测试

# 6.6 观测工具
本节介绍适用于基于 Linux 的操作系统的 CPU 性能可观测性工具。有关使用它们时要遵循的方法，请参阅上一节。

Table 6.8 中列出了本节中的工具。

这是支持部分 6.5， 方法的一系列工具和功能。我们从传统的 CPU 统计工具开始，然后继续介绍使用代码路径分析、CPU 周期分析和跟踪工具进行更深入分析的工具。一些传统工具可能在其他类 Unix操作系统上可用（有时源自），包括：uptime（1）、vmstat（8）、mpstat（1）、sar（1）、ps（1）、top（1） 和 time（1）。跟踪工具基于 BPF 以及 BCC 和 bpftrace 前端（第 15 章），包括：profile（8）、cpudist（8）、runqlat（8）、runqlen（8）、softirqs（8） 和 hardirqs（8）。

有关其功能的完整参考，请参阅每个工具的文档，包括其手册页。

# #6.6.1 uptime
uptime（1） 是打印系统负载平均值的几个命令之一：

# $ uptime
  9:04pm  up 268 day(s), 10:16,  2 users,  load average: 7.76, 8.32, 8.60

最后三个数字是 1 分钟、5 分钟和 15 分钟的负载平均值。通过比较这三个数字，您可以确定负载在过去 15 分钟（左右）内是增加、减少还是稳定。了解以下内容可能很有用：如果您正在响应生产性能问题并发现负载正在减少，则您可能错过了该问题;如果负载增加，则问题可能会变得更糟！

以下部分更详细地介绍了负载平均值，但它们只是一个起点，因此在继续学习其他指标之前，您不应花超过 5 分钟的时间考虑它们。

# 负载平均值
负载平均值表示对系统资源的需求：越高意味着需求越多。在某些操作系统（例如 Solaris）上，负载平均值显示 CPU 需求，早期版本的 Linux 也是如此。但在 1993 年，Linux 更改了负载平均值以显示系统范围的需求：CPU、磁盘和其他资源。这是通过在线程状态中包含线程来实现的TASK_UNINTERRUPTIBLE线程状态，某些工具将其显示为状态 “D”（此状态在第 5 章 应用程序， 第 5.4.5 节 线程状态分析中提到）。

负载的度量方法是当前资源使用情况 （利用率） 加上排队的请求 （饱和度）。想象一个汽车收费站：您可以通过计算正在服务的汽车数量 （利用率） 和排队的汽车数量 （饱和度） 来测量一天中不同时间点的负载。

平均值是指数阻尼移动平均线，它反映了超出 1 分钟、5 分钟和 15 分钟时间的负载（时间实际上是指数移动和 [Myer 73] 中使用的常数）。图 6.17 显示了一个简单实验的结果，其中启动了一个 CPU 绑定的线程并绘制了负载平均值。

到 1 分钟、5 分钟和 15 分钟标记时，负载平均值已达到已知负载 1.0 的 61% 左右。

负载平均值是在早期 BSD 中引入 Unix 的，它基于早期操作系统（CTSS、Multics [Saltzer 70]、TENEX [Bobrow 72]）常用的调度程序平均队列长度和负载平均值。RFC 546 [Thomas 73] 中对它们进行了描述：

[1] TENEX 平均负载是衡量 CPU 需求的指标。负载平均值是给定时间段内可运行进程数的平均值。例如，每小时平均负载为 10 意味着（对于单个 CPU 系统）在该小时内的任何时间，人们可以预期看到 1 个进程正在运行，而其他 9 个进程已准备好运行（即，未因 I/O 而被阻塞）等待 CPU。

作为一个现代示例，考虑一个 64 CPU 的系统，平均负载为 128。如果负载仅为 CPU，则意味着平均每个 CPU 上始终有一个线程在运行，并且每个 CPU 上都有一个线程等待。CPU 平均负载为 20 的同一系统表示很大的余量，因为它可以在所有 CPU 都忙碌之前再运行 44 个 CPU 绑定的线程。（一些公司监控标准化负载平均指标，该指标自动除以 CPU 计数，从而允许在不知道 CPU 计数的情况下对其进行解释。

# 压力失速信息 （PSI）
在本书的第一版中，我介绍了如何为每种资源类型提供负载平均值以帮助解释。Linux 4.20 中现在添加了一个接口，它提供了这样的细分：压力失速信息 （PSI），它给出了 CPU、内存和 I/O 的平均值。平均值显示资源上某个内容停滞的时间百分比（仅饱和度）。这与表 6.9 中的负载平均值进行了比较

表 6.10 显示了不同场景的指标显示的内容：

例如，显示 2 个 CPU 和 3 个繁忙线程的场景：

# $ uptime
 07:51:13 up 4 days,  9:56,  2 users,  load average: 3.00, 3.00, 2.55
# $ cat /proc/pressure/cpu
 some avg10=50.00 avg60=50.00 avg300=49.70 total=1031438206

此 50.0 值表示线程 （“some”） 已停止 50% 的时间。io 和 memory 指标包括第二行，用于所有非空闲线程何时停止 （“full”）。PSI 最好地回答了这个问题：任务必须等待资源的可能性有多大？

无论您使用负载平均值还是 PSI，您都应该快速转向更详细的指标来了解负载，例如 vmstat（1） 和 mpstat（1） 提供的指标

# #6.6.2 vmstat
虚拟内存统计命令 vmstat（8） 在最后几列中打印系统范围的 CPU 平均值，在第一列中打印可运行的线程计数。以下是 Linux 版本的示例输出：

# $ vmstat 1
 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 15  0      0 451732  70588 866628    0    0     1    10   43   38  2  1 97  0  0
 15  0      0 450968  70588 866628    0    0     0   612 1064 2969 72 28  0  0  0
 15  0      0 450660  70588 866632    0    0     0     0  961 2932 72 28  0  0  0
 15  0      0 450952  70588 866632    0    0     0     0 1015 3238 74 26  0  0  0
 [...]

输出的第一行应该是 summary-since-boot。但是，在 Linux 上，procs 和 memory 列首先显示当前状态。（也许有一天它们会得到修复。与 CPU 相关的列包括：

■ r：运行队列长度 - 可运行线程的总数 
■ us： 用户时间百分比 
■ sy： 系统时间（内核） 百分比 
■ id： 空闲百分比 
■ wa： 等待 I/O 百分比，用于测量线程在磁盘上被阻止时的 CPU 空闲情况 I/O 
■ st：被盗百分比，对于虚拟化环境，显示为其他租户提供服务所花费的 CPU 时间

所有这些值都是所有 CPU 的系统范围平均值，但 r 除外，它是总数

在 Linux 上，r 列是等待的任务总数加上正在运行的任务数。对于其他操作系统（例如 Solaris），r 列仅显示正在等待的任务，而不显示正在运行的任务。1979 年由 Bill Joy 和 Ozalp Babaoglu 为 BSD 编写的原始 vmstat（1） 以 RQ 列开头，表示可运行和正在运行的进程的数量，就像目前的 Linux vmstat（8） 一样。

# #6.6.3 mpstat
多处理器统计工具 mpstat（1） 可以报告每个 CPU 的统计信息。以下是 Linux 版本的输出示例

# $ mpstat -P ALL 1
 Linux 5.3.0-1009-aws (ip-10-0-239-218)  02/01/20        _x86_64_        (2 CPU)
 18:00:32  CPU   %usr  %nice   %sys %iowait   %irq  %soft %steal %guest %gnice  %idle
 18:00:33  all  32.16   0.00  61.81    0.00   0.00   0.00   0.00   0.00   0.00   6.03
 18:00:33    0  32.00   0.00  64.00    0.00   0.00   0.00   0.00   0.00   0.00   4.00
 18:00:33    1  32.32   0.00  59.60    0.00   0.00   0.00   0.00   0.00   0.00   8.08
 18:00:33  CPU   %usr  %nice   %sys %iowait   %irq  %soft %steal %guest %gnice  %idle
 18:00:34  all  33.83   0.00  61.19    0.00   0.00   0.00   0.00   0.00   0.00   4.98
 18:00:34    0  34.00   0.00  62.00    0.00   0.00   0.00   0.00   0.00   0.00   4.00
 18:00:34    1  33.66   0.00  60.40    0.00   0.00   0.00   0.00   0.00   0.00   5.94
 [...]

-P ALL 选项用于打印每个 CPU 的报告。默认情况下，mpstat（1） 只打印系统范围的摘要行 （all）。这些列是：

■ CPU：逻辑 CPU ID，或全部用于摘要 
■ %usr：用户时间，不包括 %nice 
■ %nice：优先级较高的进程的用户时间 
■ %sys：系统时间（内核） 
■ %iowait：I/O 等待 
■ %irq：硬件中断 CPU 使用率 
■ %soft：软件中断 CPU 使用率 
■ %steal：为其他租户提供服务所花费的时间 
■ %guest：在来宾虚拟机中花费的 CPU 时间 
■ %gnice： 运行 niced 客户机 
■ %idle 的 CPU 时间：空闲

键列为 %usr、%sys 和 %idle。这些指标标识每个 CPU 的 CPU 使用率，并显示用户时间/内核时间比率（参见第 6.3.9 节 用户时间/内核时间）。这还可以识别“热”CPU，即以 100% 利用率 （%usr %sys） 运行而其他未运行的 CPU，这可能是由单线程应用程序工作负载或设备中断映射引起的。

请注意，此工具和其他获取相同内核统计信息（/proc/stat 等）的工具报告的 CPU 时间，这些统计信息的准确性取决于内核配置。请参阅第 4 章 可观测性工具， 第 4.3.1 节 /proc 中的 CPU 统计准确性标题。

# #6.6.4 sar
系统活动报告程序 sar（1） 可用于观察当前活动，并且可以配置为存档和报告历史统计信息。它在第 4 章 可观测性工具， 第 4.4 节 sar 中介绍过，并在其他章节中适当地提到。

Linux 版本提供了以下 CPU 分析选项：
■ -P ALL：与 mpstat 相同 -P ALL 
■ -u：与 mpstat（1） 的默认输出相同：仅系统范围的平均值 
■ -q：包括 runq-sz 的运行队列大小（等待加运行，与 vmstat（1） 的 r 相同）和负载平均值

sar（1） 数据收集，以便可以从过去观察这些指标。有关更多详细信息，请参见 第 4.4 节 sar。

# #6.6.5 ps
进程状态命令 ps（1） 可以列出所有进程的详细信息，包括 CPU 使用率统计信息。例如：

# $ ps aux
 USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
 root         1  0.0  0.0  23772  1948 ?        Ss    2012   0:04 /sbin/init
 root         2  0.0  0.0      0     0 ?        S     2012   0:00 [kthreadd]
 root         3  0.0  0.0      0     0 ?        S     2012   0:26 [ksoftirqd/0]
 root         4  0.0  0.0      0     0 ?        S     2012   0:00 [migration/0]
 root         5  0.0  0.0      0     0 ?        S     2012   0:00 [watchdog/0]
 [...]
 web      11715 11.3  0.0 632700 11540 pts/0    Sl   01:36   0:27 node indexer.js
 web      11721 96.5  0.1 638116 52108 pts/1    Rl+  01:37   3:33 node proxy.js
 [...]

这种作方式起源于 BSD，可以通过 aux 选项前没有破折号来识别。这些列表列出了所有用户 （a），并带有扩展的用户详细信息 （u），并包括没有终端的进程 （x）。终端显示在电传打字机 （TTY） 列中。

与 SVR4 不同的样式使用前面带有破折号的选项：

# $ ps -ef
 UID        PID  PPID  C STIME TTY          
TIME CMD
 root         1     0  0 Nov13 ?        00:00:04 /sbin/init
 root         2     0  0 Nov13 ?        00:00:00 [kthreadd]
 root         3     2  0 Nov13 ?        00:00:00 [ksoftirqd/0]
 root         4     2  0 Nov13 ?        00:00:00 [migration/0]
 root         5     2  0 Nov13 ?        00:00:00 [watchdog/0]
 [...]

这将列出每个进程 （-e） 以及完整的详细信息 （-f）。ps（1） 有各种其他选项可用，包括 -o 来自定义输出和显示的列。

CPU 使用率的关键列是 TIME 和 %CPU（前面的示例）

TIME 列显示进程（用户系统）自创建以来消耗的总 CPU 时间，单位为小时：分钟：秒。

在 Linux 上，第一个示例中的 %CPU 列显示进程生命周期内的平均 CPU 利用率，所有 CPU 的总和。始终受 CPU 限制的单线程进程将报告 100%。双线程 CPU 密集型进程将报告 200%。其他操作系统可能会将 %CPU 规范化为 CPU 计数，以便其最大值为 100%，并且它们可能只显示最近或当前的 CPU 使用率，而不是整个生命周期内的平均值。在 Linux 上，要查看进程的当前 CPU 使用率，可以使用 top（1）。

# #6.6.6 top
top（1） 由 William LeFebvre 于 1984 年为 BSD 创建。他受到 VMS 命令 MONITOR PROCESS/TOPCPU 的启发，该命令显示了 CPU 消耗量最大的作业，其中包含 CPU 百分比和 ASCII 条形图直方图（但不包括数据列）。

top（1） 命令监视正在运行的顶级进程，并定期更新屏幕。例如，在 Linux 上：

# $ top
 top - 01:38:11 up 63 days,  1:17,  2 users,  load average: 1.57, 1.81, 1.77
 Tasks: 256 total,   2 running, 254 sleeping,   0 stopped,   0 zombie
 Cpu(s):  2.0%us,  3.6%sy,  0.0%ni, 94.2%id,  0.0%wa,  0.0%hi,  0.2%si,  0.0%st
 Mem:  49548744k total, 16746572k used, 32802172k free,   182900k buffers
 Swap: 100663292k total,        0k used, 100663292k free, 14925240k cached
  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
11721 web       20   0  623m  50m 4984 R   93  0.1   0:59.50 node    
11715 web       20   0  619m  20m 4916 S   25  0.0   0:07.52 node     
   10 root      20   0     0    0    0 S    1  0.0 248:52.56 ksoftirqd/2
   51 root      20   0     0    0    0 S    0  0.0   0:35.66 events/0
11724 admin     20   0 19412 1444  960 R    0  0.0   0:00.07 top    
    1 root      20   0 23772 1948 1296 S    0  0.0   0:04.35 init  

系统范围的摘要位于顶部，进程/任务列表位于底部，默认情况下按顶部 CPU 使用者排序。系统范围的摘要包括负载平均值和 CPU 状态：%us、%sy、%ni、%id、%wa、%hi、%si、%st。如前所述，这些状态等同于 mpstat（1） 打印的状态，并且是所有 CPU 的平均值。

CPU 使用率由 TIME 和 %CPU 列显示。TIME 是进程以百分之一秒的分辨率消耗的总 CPU 时间。例如，“1：36.53” 表示总共 1 分 36.53 秒的 CPU 启动时间。某些版本的 top（1） 提供了可选的 “cumulative time” 模式，其中包括已退出的子进程的 CPU 时间

%CPU 列显示当前屏幕更新间隔的总 CPU 利用率。在 Linux 上，这不会由 CPU 计数标准化，因此双线程 CPU 绑定进程将报告 200%;top（1） 称其为 “Irix 模式”，因为它在 IRIX 上的行为。这可以切换到 “Solaris 模式” （通过按 I 切换模式），将 CPU 使用率除以 CPU 计数。在这种情况下，16 CPU 服务器上的双线程进程将报告 CPU 为 12.5%。

尽管 top（1） 通常是初级性能分析师的工具，但您应该意识到 top（1） 本身的 CPU 使用率可能会变得很大，因此 top（1） 会成为 CPU 消耗最多的进程！这是由于它用于读取 /proc 的系统调用—open（2）、read（2）、close（2）— 并在许多进程中调用这些调用。其他操作系统上的某些 top（1） 版本通过保持文件描述符打开并调用 pread（2） 来减少开销。

top（1） 有一个变体叫做 htop（1），它为 CPU 使用率提供了更多的交互功能、自定义和 ASCII 条形图。它还调用了四倍的 syscall，从而进一步扰乱了系统。我很少使用它。

由于 top（1） 会拍摄 /proc 的快照，因此它可能会错过在拍摄快照之前退出的短期进程。这通常发生在软件构建期间，其中 CPU 可能会因构建过程中的许多短期工具而重载。Linux 的 top（1） 的一个变体，称为 atop（1），它使用进程记帐来捕获短期进程的存在，它将其包含在其显示中。

# #6.6.7 pidstat
Linux 中的 pidstat（1） 工具按进程或线程打印 CPU 使用率，包括用户和系统时间的细分。默认情况下，仅打印活动进程的滚动输出。例如：

# $ pidstat 1
 Linux 2.6.35-32-server (dev7)   11/12/12        _x86_64_        (16 CPU)
 22:24:42          PID    %usr %system  %guest    %CPU   CPU  Command
 22:24:43         7814    0.00    1.98    0.00    1.98     3  tar
 22:24:43         7815   97.03    2.97    0.00  100.00    11  gzip
 22:24:43          PID    %usr %system  %guest    %CPU   CPU  Command
 22:24:44          448    0.00    1.00    0.00    1.00     0  kjournald
 22:24:44         7814    0.00    2.00    0.00    2.00     3  tar
 22:24:44         7815   97.00    3.00    0.00  100.00    11  gzip
 22:24:44         7816    0.00    2.00    0.00    2.00     2  pidstat
 [...]

此示例捕获了一个系统备份，其中包括一个 tar（1） 命令从文件系统中读取文件，以及一个 gzip（1） 命令来压缩文件。正如预期的那样，gzip（1） 的用户时间很长，因为它在压缩代码中会受到 CPU 限制。tar（1） 命令在内核中花费更多时间，从文件系统中读取数据

-p ALL 选项可用于打印所有进程，包括那些空闲的进程。-t 打印每个线程的统计信息。其他 pidstat（1） 选项包含在本书的其他章节中。

# #6.6.8 time,ptime
time（1） 命令可用于运行程序和报告 CPU 使用率。它在操作系统的 /usr/bin 下提供，并作为内置的 shell 提供。

此示例在 cksum（1） 命令上运行两次时间，计算大文件的校验和：

# $ time cksum ubuntu-19.10-live-server-amd64.iso
 1044945083 883949568 ubuntu-19.10-live-server-amd64.iso
 real     0m5.590s
 user     0m2.776s
 sys      0m0.359s
# $ time cksum ubuntu-19.10-live-server-amd64.iso 
1044945083 883949568 ubuntu-19.10-live-server-amd64.iso
 real     0m2.857s
 user     0m2.733s
 sys      0m0.114s

第一次运行需要 5.6 秒，其中 2.8 秒处于用户模式，用于计算校验和。系统时间为 0.4 秒，跨越读取文件所需的系统调用。缺少 2.4 秒 （5.6 – 2.8 – 0.4），这可能是磁盘 I/O 读取所花费的时间被阻止，因为此文件仅部分缓存。第二次运行完成得更快，在 2.9 秒内，几乎没有阻塞时间。这是意料之中的，因为该文件可能会完全缓存在主内存中以供第二次运行。

在 Linux 上，/usr/bin/time 版本支持详细详细信息。例如：

 $ /usr/bin/time -v cp fileA fileB
        Command being timed: "cp fileA fileB"
        User time (seconds): 0.00
        System time (seconds): 0.26
        Percent of CPU this job got: 24%
        Elapsed (wall clock) time (h:mm:ss or m:ss): 0:01.08
        Average shared text size (kbytes): 0
        Average unshared data size (kbytes): 0
        Average stack size (kbytes): 0
        Average total size (kbytes): 0
        Maximum resident set size (kbytes): 3792
        Average resident set size (kbytes): 0
        Major (requiring I/O) page faults: 0
        Minor (reclaiming a frame) page faults: 294
        Voluntary context switches: 1082
        Involuntary context switches: 1
        Swaps: 0
        File system inputs: 275432
        File system outputs: 275432
        Socket messages sent: 0
        Socket messages received: 0
        Signals delivered: 0
        Page size (bytes): 4096
        Exit status: 0

shell 内置版本中通常不提供 -v 选项。

# #6.6.9 turbostat
turbostat（1） 是一个基于模型特定寄存器 （MSR） 的工具，它显示 CPU 的状态，通常以 linux-tools-common 包的形式提供。MSR 在第 4 章 可观测性工具， 第 4.3.10 节 其他可观测性源中提到过。以下是一些示例输出：

 # turbostat
 turbostat version 17.06.23 - Len Brown <lenb@kernel.org>
 CPUID(0): GenuineIntel 22 CPUID levels; family:model:stepping 0x6:8e:a (6:142:10)
 CPUID(1): SSE3 MONITOR SMX EIST TM2 TSC MSR ACPI-TM TM
 CPUID(6): APERF, TURBO, DTS, PTM, HWP, HWPnotify, HWPwindow, HWPepp, No-HWPpkg, EPB
 cpu0: MSR_IA32_MISC_ENABLE: 0x00850089 (TCC EIST No-MWAIT PREFETCH TURBO)
 CPUID(7): SGX
 cpu0: MSR_IA32_FEATURE_CONTROL: 0x00040005 (Locked SGX)
 CPUID(0x15): eax_crystal: 2 ebx_tsc: 176 ecx_crystal_hz: 0
 TSC: 2112 MHz (24000000 Hz * 176 / 2 / 1000000)
 CPUID(0x16): base_mhz: 2100 max_mhz: 4200 bus_mhz: 100
 [...]

  Core    CPU     Avg_MHz  Busy%   Bzy_MHz  TSC_MHz  IRQ     SMI     C1      C1E      
        C3      C6       C7s     C8       C9       C10     C1%     C1E%    C3%
        C6%     C7s%     C8%     C9%      C10%     CPU%c1  CPU%c3  CPU%c6  CPU%c7
        CoreTmp PkgTmp   GFX%rc6 GFXMHz   Totl%C0  Any%C0  GFX%C0  CPUGFX% Pkg%pc2
        Pkg%pc3 Pkg%pc6  Pkg%pc7 Pkg%pc8  Pkg%pc9  Pk%pc10 PkgWatt CorWatt GFXWatt
        RAMWatt PKG_%    RAM_%
 [...]
 0       0       97       2.70    3609     2112     1370    0       41      293
        41      453      0       693      0        311     0.24    1.23    0.15
        5.35    0.00     39.33   0.00     50.97    7.50    0.18    6.26    83.37
        52      75       91.41   300      118.58   100.38  8.47    8.30    0.00
        0.00    0.00     0.00    0.00     0.00     0.00    17.69   14.84   0.65
        1.23    0.00     0.00
 [...]

turbostat（8） 首先打印有关 CPU 和 MSR 的信息，这些信息可以超过 50 行输出，在这里被截断。然后，它以默认的 5 秒间隔打印所有 CPU 和每个 CPU 的指标的间隔摘要。在此示例中，此间隔摘要输出的宽度为 389 个字符，并且各行已换行 5 次，因此难以阅读。这些列包括 CPU 编号 （CPU）、以 MHz 为单位的平均时钟速率 （Avg_MHz）、C 状态信息、温度 （*Tmp） 和功率 （*Watt）。

# #6.6.10 showboost
在 Netflix云上提供 turbostat（8） 之前，我开发了 showboost（1） 来显示 CPU 时钟速率和每个间隔的摘要。showboost（1） 是 “show turbo boost” 的缩写，也使用 MSR。一些示例输出

 # showboost
 Base CPU MHz : 3000
 Set CPU MHz  : 3000
 Turbo MHz(s) : 3400 3500
 Turbo Ratios : 113% 116%
 CPU 0 summary every 1 seconds...
 TIME       C0_MCYC      C0_ACYC        UTIL  RATIO    MHz
 21:41:43   3021819807   3521745975     100%   116%   3496
 21:41:44   3021682653   3521564103     100%   116%   3496
 21:41:45   3021389796   3521576679     100%   116%   3496
 [...]

此输出显示 CPU0 上的时钟速率为 3496 MHz。基本 CPU 频率为 3000 MHz：通过 Intel turbo boost 达到 3496。输出中还列出了可能的 Turbo Boost 级别或“步骤”：3400 和 3500 MHz。

showboost（8） 在我的 msr-cloud-tools 仓库 [Gregg 20d] 中，我开发这些工具是为了在云中使用而得名的。因为我只让它在 Netflix 环境中工作，所以由于 CPU 的差异，它可能在其他地方无法工作，在这种情况下，请尝试 turboboost（1）。

# #6.6.11 pmcarch
pmcarch（8） 显示了 CPU 周期性能的高级视图。它是一个基于 PMC 的工具，基于 PMC 的 Intel“架构集”，因此得名（PMC 在第 4 章 可观察性工具， 第 4.3.9 节 硬件计数器 （PMC）中进行了解释）。在某些云环境中，这些架构 PMC 是唯一可用的 PMC（例如，一些 AWS EC2 实例）。一些 sam ple 输出：

# pmcarch
 K_CYCLES   K_INSTR    IPC BR_RETIRED   BR_MISPRED  BMR% LLCREF    LLCMISS     LLC%
 96163187   87166313  0.91 19730994925  679187299   3.44 656597454 174313799  73.45
 93988372   87205023  0.93 19669256586  724072315   3.68 666041693 169603955  74.54
 93863787   86981089  0.93 19548779510  669172769   3.42 649844207 176100680  72.90
 93739565   86349653  0.92 19339320671  634063527   3.28 642506778 181385553  71.77
 [...]

该工具会打印原始计数器以及一些百分比比率。列包括：

■ K_CYCLES：CPU 周期 x 1000 
■ K_INSTR：CPU 指令数 x 1000 
■ IPC：每周期指令数 
■ BMR%：分支预测错误率，以百分比表示 
■ LLC%：最后一级缓存命中率，以百分比表示

IPC 在第 6.3.7 节 IPC、CPI 中进行了解释，并提供了示例值。提供的其他比率 BMR% 和 LLC% 提供了一些关于 IPC 可能较低以及失速周期可能在哪里的见解。

我为我的 pmc-cloud-tools 仓库开发了 pmcarch（8），它也有 cpucache（8） 来统计更多的 CPU 缓存 [Gregg 20e]。这些工具采用解决方法并使用特定于处理器的 PMC，以便它们在 AWS EC2 云上运行，而可能无法在其他地方工作。即使这对你不起作用，它也提供了一些有用的 PMC 示例，你可以直接使用 perf（1） 来插桩（第 6.6.13 节，perf）。

# #6.6.12 tlbstat
tlbstat（8） 是 pmc-cloud-tools 的另一个工具，它显示 TLB 缓存统计信息。输出示例：

# tlbstat -C0 1
 K_CYCLES   K_INSTR      IPC DTLB_WALKS ITLB_WALKS K_DTLBCYC  K_ITLBCYC  DTLB% ITLB%
 2875793    276051      0.10 89709496   65862302   787913     650834     27.40 22.63
 2860557    273767      0.10 88829158   65213248   780301     644292     27.28 22.52
 2885138    276533      0.10 89683045   65813992   787391     650494     27.29 22.55
 2532843    243104      0.10 79055465   58023221   693910     573168     27.40 22.63
 [...]

此特定输出显示了解决 Meltdown CPU 漏洞的 KPTI 补丁的最坏情况（KPTI 性能影响在第 3 章操作系统， 第 3.4.3 节 KPTI （Meltdown）中进行了总结）。KPTI 在系统调用和其他事件中刷新 TLB 缓存，从而导致 TLB 遍历期间出现停顿循环：这如最后两列所示。在此输出中，CPU 大约一半的时间花在 TLB 遍历上，并且预计运行应用程序工作负载的速度大约是原来的一半。

列包括：
■ K_CYCLES：CPU 周期 × 1000 
■ K_INSTR：CPU 指令 × 1000
■ IPC：每周期指令数 
■ DTLB_WALKS：数据 TLB 行走（计数） 
■ ITLB_WALKS：指令 TLB 行走（计数） 
■ K_DTLBCYC：至少一个 PMH 处于活动状态的周期，数据 TLB 行走× 1000 
■ K_ITLBCYC：至少一个 PMH 处于活动状态的周期。TLB 遍历× 1000 
■ DTLB%：数据 TLB 活动周期数与总周期数的比率 
■ ITLB%：指令 TLB 活动周期数与总周期数的比率 ITLB%：指令 TLB 活动周期数与总周期数的比率

与 pmcarch（8） 一样，由于处理器的差异，此工具可能不适用于您的环境。尽管如此，它仍然是一个有用的思想来源。

# #6.6.13 perf
perf（1） 是官方的 Linux 分析器，一个具有许多功能的多功能工具。第 13 章提供了 perf（1） 的摘要。本节介绍其在 CPU 分析中的使用情况。

# 单行
以下单行代码都很有用，并演示了用于 CPU 分析的不同 perf（1） 功能。以下部分将更详细地介绍一些内容。

指定命令的 CPU 上函数示例，频率为 99 赫兹：
# perf record -F 99 command

系统范围内的 CPU 堆栈跟踪示例（通过帧指针）持续 10 秒：
#  perf record -F 99 -a -g -- sleep 10

PID 的 CPU 堆栈跟踪示例，使用 dwarf （dbg info） 展开堆栈：
# perf record -F 99 -p PID --call-graph dwarf -- sleep 10

通过 exec 记录新的进程事件：
# perf record -e sched:sched_process_exec -a

使用堆栈跟踪记录上下文切换事件 10 秒：
# perf record -e sched:sched_switch -a -g -- sleep 10

10 秒的 CPU 迁移示例：
# perf record -e migrations -a -- sleep 10

记录所有 CPU 迁移 10 秒：
# perf record -e migrations -a -c 1 -- sleep 10

将 perf.data 显示为文本报表，其中包含合并的数据以及计数和百分比：
# perf report -n --stdio

列出所有 perf.data 事件，并带有 data 标头（推荐）：
# perf script --header

显示整个系统的 PMC 统计信息，持续 5 秒：
# perf stat -a -- sleep 5

显示命令的 CPU 最后一级高速缓存 （LLC） 统计信息：
# perf stat -e LLC-loads,LLC-load-misses,LLC-stores,LLC-prefetches command

每秒显示系统范围内的内存总线吞吐量
# perf stat -e uncore_imc/data_reads/,uncore_imc/data_writes/ -a -I 1000

显示每秒上下文切换的速率：
#  perf stat -e sched:sched_switch -a -I 1000

显示每秒非自愿上下文切换的速率（之前的状态为 TASK_RUNNING）：
# perf stat -e sched:sched_switch --filter 'prev_state == 0' -a -I 1000

显示每秒模式切换和上下文切换的速率：
# perf stat -e cpu_clk_unhalted.ring0_trans,cs -a -I 1000

记录调度程序配置文件 10 秒：
# perf sched record -- sleep 10

从调度程序配置文件显示每个进程的调度程序延迟：
# perf sched latency

列出计划程序配置文件中的每个事件计划程序延迟：
#  perf sched timehist

有关更多 perf（1） 单行代码，请参见 第 13 章 perf， 第 13.2 节 单行代码。

# 系统范围的 CPU 分析
perf（1） 可用于分析 CPU 调用路径，总结内核和用户空间的 CPU 时间。这是由 record 命令执行的，该命令将 sample 捕获到 perf.data 文件。然后，可以使用 report 命令查看文件的内容。它通过使用最准确的计时器来工作： 如果可用，则基于 CPU cycle，否则基于软件（cpu-clock 事件）。

在以下示例中，所有 CPU （-a） 都使用调用堆栈 （-g） 以 99 Hz （-F 99） 的速度进行采样 10 秒 （sleep 10）。report 的 --stdio 选项用于打印所有输出，而不是在交互模式下作

 # perf record -a -g -F 99 -- sleep 10
 [ perf record: Woken up 20 times to write data ]
 [ perf record: Captured and wrote 5.155 MB perf.data (1980 samples) ]
 # perf report --stdio
 [...]
 # Children      Self  Command          Shared Object              Symbol 
# ........  ........  ...............  .........................  ...................
 ...................................................................
 #
    29.49%     0.00%  mysqld           libpthread-2.30.so         [.] start_thread
            |
            ---start_thread
               0x55dadd7b473a
               0x55dadc140fe0
               |          
                --29.44%--do_command
                          |          
                          |--26.82%--dispatch_command
                          |          |          
                          |           --25.51%--mysqld_stmt_execute
                          |                     |          
                          |                      --25.05%-
Prepared_statement::execute_loop
                          |                                |          
                          |                                 --24.90%-
Prepared_statement::execute
                          |                                           |          
                          |                                            --24.34%-
mysql_execute_command
                          |                                                      |
 [...]

完整输出长很多页，按样本计数降序排列。这些样本计数以百分比形式提供，显示 CPU 时间的花费位置。在这个例子中，29.44% 的时间花在 do_command（） 及其子项上，包括 mysql_execute_command（） 。这些内核和进程符号仅在其 debuginfo 文件可用时可用;其他明智的十六进制地址也会显示出来。

在 Linux 4.4 中，堆栈顺序从 callee（从 on-CPU 函数开始并列出祖先）更改为 caller（从 parent 函数开始并列出 children）。您可以使用 -g 切换回 callee：

 # perf report -g callee --stdio
 [...]
    19.75%     0.00%  mysqld           mysqld                     [.] 
Sql_cmd_dml::execute_inner
            |
            ---Sql_cmd_dml::execute_inner
               Sql_cmd_dml::execute
               mysql_execute_command
               Prepared_statement::execute
               Prepared_statement::execute_loop
               mysqld_stmt_execute
               dispatch_command
               do_command
               0x55dadc140fe0
               0x55dadd7b473a
               start_thread
 [...]

要了解配置文件，您可以尝试两种排序。如果您无法在命令行中快速理解它，请尝试使用火焰图等可视化效果。

# CPU 火焰图
通过使用 Linux 5.8.11 中添加的 flamegraph 报告，可以从相同的 perf.data 配置文件生成 CPU 火焰图，例如：

 # perf record -F 99 -a -g -- sleep 10
 # perf script report flamegraph

这将使用 /usr/share/d3-flame-graph/ d3-flamegraph-base.html中的 d3-flame-graph 模板文件创建一个火焰图（如果您没有此文件，它可以通过 d3-flame-graph 软件 [Spier 20b] 构建）。这些也可以组合为一个命令：

 # perf script flamegraph -a -F 99 sleep 10

对于旧版本的 Linux，你可以使用我原来的 flamegraph 软件来可视化 perf 脚本报告的 samples。步骤（也包含在第 5 章中）是：

 # perf record -F 99 -a -g -- sleep 10
 # perf script --header > out.stacks
# $ git clone https://github.com/brendangregg/FlameGraph; cd FlameGraph
# $ ./stackcollapse-perf.pl < ../out.stacks | ./flamegraph.pl --hash > out.svg

out.svg 文件是 CPU 火焰图，可以在 Web 浏览器中加载。它包括用于交互性的 JavaScript：单击可缩放，Ctrl-F 可搜索。请参见第 6.5.4 节 性能分析，其中说明了图 6.15 中的这些步骤。

您可以修改这些步骤，将 perf 脚本直接通过管道传输到 stackcollapse-perf.pl，从而避免使用 out.stacks 文件。但是，我发现这些文件存档以供以后参考并与其他工具（例如 FlameScope）一起使用很有用。

# 选择
flamegraph.pl 支持各种选项，包括：
--title TEXT： 设置标题。
--subtitle TEXT： 设置副标题.
--width NUM： 设置图片宽度（默认 1200 像素）。
--countname TEXT：更改计数标签（默认为 “samples”）。
--colors PALETTE：设置框架颜色的调色板。其中一些使用搜索词或注释为不同的代码路径使用不同的色调。选项包括 hot（默认）、mem、io、java、js、perl、red、green、blue、yellow。
--bgcolors COLOR：设置背景颜色。渐变选项包括黄色（默认）、蓝色、绿色、灰色;对于单色（非渐变），请使用 “#rrggbb”
--hash：颜色由函数名称哈希键控以保持一致性。
--reverse：生成一个堆栈反转的火焰图，从 leaf 合并到 root。
--inverted：翻转 y 轴以生成冰柱图。
--flamechart：生成火焰图（x 轴上的时间）。

例如，这是我用于 Java CPU 火焰图的选项集：

# $ ./flamegraph.pl --colors=java --hash 
    --title="CPU Flame Graph, $(hostname), $(date)" < ...

这包括火焰图中的主机名和日期。

有关解释火焰图的信息，请参见部分 6.7.3， 火焰图。

# 进程 CPU 性能分析
除了跨所有 CPU 进行性能分析外，还可以使用 -p PID 来定位单个进程，并且 perf（1） 可以直接执行命令并对其进行性能分析：

# perf record -F 99 -g command

命令前经常插入一个 “--” 来停止 perf（1） 处理命令中的命令行选项。

# 计划程序延迟
sched 命令记录并报告调度程序统计信息。例如

 # perf sched record -- sleep 10
 [ perf record: Woken up 63 times to write data ]
 [ perf record: Captured and wrote 125.873 MB perf.data (1117146 samples) ]
 # perf sched latency ------------------------------------------------------------------------------------
Task                 | Runtime ms  | Switches | Average delay ms | Maximum delay ms |------------------------------------------------------------------------------------
jbd2/nvme0n1p1-:175  |    0.209 ms |        3 | avg:    0.549 ms | max:    1.630 ms |
 kauditd:22           |    0.180 ms |        6 | avg:    0.463 ms | max:    2.300 ms |
 oltp_read_only.:(4)  | 3969.929 ms |   184629 | avg:    0.007 ms | max:    5.484 ms |
 mysqld:(27)          | 8759.265 ms |    96025 | avg:    0.007 ms | max:    4.133 ms |
 bash:21391           |    0.275 ms |        1 | avg:    0.007 ms | max:    0.007 ms |
 [...]------------------------------------------------------------------------------------
TOTAL:               |  12916.132 ms |   281395 |------------------------------------------------

此延迟报告总结了每个进程的平均和最大调度程序延迟（也称为运行队列延迟）。虽然 oltp_read_only 和 mysqld 进程有许多上下文切换，但它们的平均和最大调度程序延迟仍然很低。（为了适应此处的输出宽度，我省略了最后的“Maximum delay at”列。

计划程序事件很频繁，因此这种类型的跟踪会产生大量的 CPU 和存储开销。在本例中，perf.data 文件仅经过 10 秒的跟踪就达到了 125 MB。调度器事件的速率可能会淹没 perf（1） 的每 CPU 环形缓冲区，从而导致事件丢失：如果发生这种情况，报告将在最后说明这一点。请小心此开销，因为它可能会干扰 生产应用程序。

perf（1） sched 也有 map 和 timehist 报告，用于以不同的方式显示调度器配置文件。timehist 报告显示每个事件的详细信息：

 # perf sched timehist
 Samples do not have callchains.
          time    cpu  task name                     wait time  sch delay   run time
                       [tid/pid]                        (msec)     (msec)     (msec)-------------- ------  ----------------------------  ---------  ---------  --------
 437752.840756 [0000]  mysqld[11995/5187]                0.000      0.000      0.000   
 437752.840810 [0000]  oltp_read_only.[21483/21482]      0.000      0.000      0.054 
 437752.840845 [0000]  mysqld[11995/5187]                0.054      0.000      0.034   
 437752.840847 [0000]  oltp_read_only.[21483/21482]      0.034      0.002      0.002
 [...]
 437762.842139 [0001]  sleep[21487]                  10000.080      0.004      0.127

此报告显示每个上下文切换事件，以及休眠时间（等待时间）、调度程序延迟（sch delay）和 CPU 花费时间（运行时），均以毫秒为单位。最后一行显示了用于设置 perf 记录持续时间的虚拟 sleep（1） 命令，该记录休眠了 10 秒。

# PMC （硬件事件）
stat 子命令对事件进行计数并生成摘要，而不是将事件记录到 perf.data。默认情况下，perf stat 会对多个 PMC 进行计数，以显示 CPU 周期的高级摘要。例如，总结 gzip（1） 命令：

# $ perf stat gzip ubuntu-19.10-live-server-amd64.iso 
 Performance counter stats for 'gzip ubuntu-19.10-live-server-amd64.iso':
      25235.652299      task-clock (msec)         #    0.997 CPUs utilized          
               142      context-switches          #    0.006 K/sec                  
                25      cpu-migrations            #    0.001 K/sec                  
               128      page-faults               #    0.005 K/sec                  
    94,817,146,941      cycles                    #    3.757 GHz                    
   152,114,038,783      instructions              #    1.60  insn per cycle         
    28,974,755,679      branches                  # 1148.167 M/sec                  
     1,020,287,443      branch-misses             #    3.52% of all branches        
      25.312054797 seconds time elapsed

统计信息包括 cycle 和 instruction count 以及 IPC。如前所述，这是一个非常有用的高级指标，用于确定发生的周期类型以及其中有多少是 stall cycles。在这种情况下，IPC 1.6 为 “good”。

下面是一个测量 IPC 的系统范围示例，这次是从 Shopify 基准测试到调查 NUMA 调整，最终将应用程序吞吐量提高了 20-30%。这些命令在所有 CPU 上测量 30 秒。

以前：

 # perf stat -a -- sleep 30
 [...]
      404,155,631,577      instructions              #    0.72  insns per cycle         
[100.00%]
 [...]


NUMA 优化后：

# perf stat -a -- sleep 30
 [...]
   490,026,784,002      instructions              #    0.89  insns per cycle         
[100.00%]
 [...]

IPC 从 0.72 提高到 0.89：24%，与最终获胜持平。（参见第 16 章，案例研究，了解测量 IPC 的另一个生产示例。

# 硬件事件选择
还有更多可以计数的硬件事件。您可以使用 perf list 列出它们：

# perf list
 [...]
  branch-instructions OR branches                    [Hardware event]
  branch-misses                                      [Hardware event]
  bus-cycles                                         [Hardware event]
  cache-misses                                       [Hardware event]
  cache-references                                   [Hardware event]
  cpu-cycles OR cycles                               [Hardware event]
  instructions                                       [Hardware event]
  ref-cycles                                         [Hardware event]
 [...]
  LLC-load-misses                                    [Hardware cache event]
  LLC-loads                                          [Hardware cache event]
  LLC-store-misses                                   [Hardware cache event]
  LLC-stores                                         [Hardware cache event]
 [...]

查找“Hardware event（硬件事件）”和“Hardware cache event（硬件缓存事件）”。对于某些处理器，您会发现额外的 PMC 组;第 13 章 perf， Section 13.3 perf 事件中提供了更长的示例。可用的选项取决于处理器架构，并记录在流程手册（例如，英特尔软件开发人员手册）中。

可以使用 –e 指定这些事件。例如（这来自 Intel Xeon）：

# $ perf stat -e instructions,cycles,L1-dcache-load-misses,LLC-load-misses,dTLB-load
misses gzip ubuntu-19.10-live-server-amd64.iso
 Performance counter stats for 'gzip ubuntu-19.10-live-server-amd64.iso':
   152,226,453,131      instructions              #    1.61  insn per cycle         
    94,697,951,648      cycles  
     2,790,554,850      L1-dcache-load-misses                                       
         9,612,234      LLC-load-misses                                             
           357,906      dTLB-load-misses                                            
      25.275276704 seconds time elapsed

除了指令和周期外，此示例还测量了以下内容

■ L1-dcache-load-misses：1 级数据缓存加载未命中。这为您提供了在从 Level 1 缓存返回一些负载后由应用程序引起的内存负载的度量。它可以与其他 L1 事件计数器进行比较，以确定缓存命中率。
■ LLC-load-misses：最后一级缓存加载未命中。在最后一级之后，这将访问主内存，因此这是主内存负载的度量。这与 L1-dcache-load-misses 之间的差异可以了解超过 1 级的 CPU 缓存的有效性，但需要其他计数器来实现完整性。
■ dTLB-load-misses：数据转换后备缓冲区未命中。这显示了 MMU 缓存工作负载的页面映射的有效性，并且可以测量内存工作负载（工作集）的大小。

可以检查许多其他计数器。perf（1） 支持描述性名称（就像本例中使用的那些）和十六进制值。后者对于您在处理器手册中找到的深奥计数器可能是必需的，因为没有提供描述性名称。

# 软件跟踪
perf 还可以记录和计算软件事件。列出一些与 CPU 相关的事件：

 # perf list
 [...]
  context-switches OR cs                             [Software event]
  cpu-migrations OR migrations                       [Software event]
 [...]
  sched:sched_kthread_stop                           [Tracepoint event]
  sched:sched_kthread_stop_ret                       [Tracepoint event]
  sched:sched_wakeup                                 [Tracepoint event]
  sched:sched_wakeup_new                             [Tracepoint event]
  sched:sched_switch                                 [Tracepoint event]
 [...]

以下示例使用上下文切换软件事件来跟踪应用程序何时离开 CPU，并收集一秒钟的调用堆栈：

# perf record -e sched:sched_switch -a -g -- sleep 1
 [ perf record: Woken up 46 times to write data ]
 [ perf record: Captured and wrote 11.717 MB perf.data (50649 samples) ]
 # perf report --stdio
 [...]
    16.18%    16.18%  prev_comm=mysqld prev_pid=11995 prev_prio=120 prev_state=S ==> 
next_comm=swapper/1 next_pid=0 next_prio=120
            |
            ---__sched_text_start
               schedule
               schedule_hrtimeout_range_clock
               schedule_hrtimeout_range
               poll_schedule_timeout.constprop.0
               do_sys_poll
               __x64_sys_ppoll
               do_syscall_64
               entry_SYSCALL_64_after_hwframe
               ppoll
               vio_socket_io_wait
               vio_read
               my_net_read
               Protocol_classic::read_packet
               Protocol_classic::get_command
               do_command
               start_thread
 [...]

这个截断的输出显示 mysql 上下文通过 poll（2） 切换到套接字上的块。要进一步了解，请参见第 5 章 应用程序， 第 5.4.2 节 离线 CPU 分析中的非 CPU 分析方法，以及第 5.5.3 节 offcputime 中的支持工具。

第 9 章 磁盘 包括另一个使用 perf（1） 进行静态跟踪的示例：块 I/O 跟踪点。第 10 章 网络 包括一个使用 perf（1） 对 tcp_sendmsg（） 内核函数进行动态插桩的示例。

# 硬件跟踪
perf（1） 还可以使用硬件跟踪进行每条指令的分析，如果 proces sor 支持的话。这是一个低级高级活动，此处未介绍，但在第 13 章 perf， 第 13.13 节 其他命令中再次提及

# 文档
有关 perf（1） 的更多信息，请参见第 13 章 perf。另请参阅其手册页、tools/perf/Documentation 下的 Linux 内核源代码中的文档、我的“perf 示例”页面 [Gregg 20f]、“perf 教程”[perf 15] 和“非官方的 Linux 性能事件网页”[Weaver 11]

# #6.6.14 profile
profile（8） 是一个 BCC 工具，它以定时的时间间隔对堆栈跟踪进行采样并报告频率计数。这是 BCC 中用于了解 CPU 消耗的最有用的工具，因为它对几乎所有消耗 CPU 资源的代码路径进行求和。（参见 Section 6.6.19 中的 hardirqs（8） 工具了解更多的 CPU 使用者。profile（8） 的开销比 perf（1） 低，因为只有堆栈跟踪摘要被传递给用户空间。这种开销差异如图 6.15 所示。profile（8） 在 第 5 章 应用程序 ， 第 5.5.2 节 配置文件中也进行了总结，以用作应用程序分析器。

默认情况下，profile（8） 在所有 CPU 上以 49 赫兹的频率对用户和内核堆栈跟踪进行采样。这可以使用 options 进行自定义，并且设置将在输出开始时打印。例如：

# profile
 Sampling at 49 Hertz of all threads by user + kernel stack... Hit Ctrl-C to end.
 ^C
 [...]
    finish_task_switch
    __sched_text_start
    schedule
    schedule_hrtimeout_range_clock
    schedule_hrtimeout_range
    poll_schedule_timeout.constprop.0
    do_sys_poll
    __x64_sys_ppoll
    do_syscall_64
    entry_SYSCALL_64_after_hwframe
    ppoll
    vio_socket_io_wait(Vio*, enum_vio_io_event)
    vio_read(Vio*, unsigned char*, unsigned long)
    my_net_read(NET*)
    Protocol_classic::read_packet()
    Protocol_classic::get_command(COM_DATA*, enum_server_command*)
    do_command(THD*)
    start_thread
    -                mysqld (5187)
        151

输出将堆栈跟踪显示为函数列表，后跟短划线 （“-”） 和括号中的过程名称和 PID，最后是该堆栈跟踪的计数。堆栈跟踪按频率计数顺序打印，从最不频繁到最频繁。

此示例中的完整输出长度为 8,261 行，此处已截断，以仅显示最后一个、最频繁的堆栈跟踪。它显示调度器函数在 CPU 上，从 poll（2） 代码路径调用。此特定堆栈跟踪在跟踪时采样了 151 次。

profile（8） 支持各种选项，包括：

■ -U：仅包括用户级堆栈
■ -K：仅包括内核级堆栈
■ -a：包括帧注释（例如，内核帧的“_[k]”）
■ -d：包括内核/用户堆栈之间的分隔符
■ -f：以折叠格式提供输出
■ -p PID：仅分析此过程
■ --stack-storage-size SIZE：唯一堆栈跟踪的数量（默认 16,384）

如果 profile（8） 打印这种类型的警告：

警告：无法显示 5 堆栈跟踪。

这意味着已超出堆栈存储。您可以使用 --stack-storage-size 选项增加它。

# 分析 CPU 火焰图
-f 选项提供适合通过我的火焰图软件导入的输出。示例说明：

 # profile -af 10 > out.stacks
 # git clone https://github.com/brendangregg/FlameGraph; cd FlameGraph
 # ./flamegraph.pl --hash < out.stacks > out.svg

然后，可以在 Web 浏览器中加载 out.svg 文件。

profile（8） 和以下工具（runqlat（8）、runqlen（8）、softirqs（8）、hardirqs（8））是来自 BCC 存储库的基于 BPF 的工具，这将在第 15 章中介绍。

# #6.6.15 cpudist
cpudist（8）12 是一个 BCC 工具，用于显示每个线程唤醒的 CPU 时间分布。这可用于帮助描述 CPU 工作负载，为以后的优化和设计决策提供详细信息。例如，从 2 个 CPU 数据库实例

# cpudist 10 1
 Tracing on-CPU time... Hit Ctrl-C to end.
     usecs               : count     distribution
         0 -> 1          : 0        |                                        |
         2 -> 3          : 135      |                                        |
         4 -> 7          : 26961    |********                                |
         8 -> 15         : 123341   |****************************************|
        16 -> 31         : 55939    |******************                      |
        32 -> 63         : 70860    |**********************                  |
        64 -> 127        : 12622    |****                                    |
       128 -> 255        : 13044    |****                                    |
       256 -> 511        : 3090     |*                                       |
       512 -> 1023       : 2        |                                        |
      1024 -> 2047       : 6        |                                        |
      2048 -> 4095       : 1        |                                        |
      4096 -> 8191       : 2        |                                        |

输出显示，数据库通常在 CPU 上花费 4 到 63 微秒。这很短。

选项包括：

■ -m：以毫秒为单位打印输出
■ -O：显示 off-CPU 时间而不是 on-CPU 时间
■ -P：按进程打印直方图
■ -p PID：仅跟踪此进程 ID

这可以与 profile（8） 结合使用，以总结应用程序在 CPU 上运行的时间以及它正在做什么。

# #6.6.16 runqlat
runqlat（8）13 是一个 BCC 和 bpftrace 工具，用于测量 CPU 调度器延迟，通常称为运行队列延迟（即使不再使用运行队列实现）。它对于识别和量化 CPU 饱和问题非常有用，因为对 CPU 资源的需求超过了它们的服务能力。runqlat（8） 测量的度量是每个线程（任务）等待其启动 CPU 所花费的时间。

下面显示了在 2 CPU MySQL 数据库云实例上运行的 BCC runqlat（8），该实例在整个系统范围内以大约 15% 的 CPU 利用率运行。runqlat（8） 的参数是 “10 1” 来设置 10 秒的间隔，并且只输出一次：

 # runqlat 10 1
 Tracing run queue latency... Hit Ctrl-C to end.
     usecs               : count     distribution
         0 -> 1          : 9017     |*****                                   |
         2 -> 3          : 7188     |****                                    |
         4 -> 7          : 5250     |***                                     |
         8 -> 15         : 67668    |****************************************|
        16 -> 31         : 3529     |**                                      |
        32 -> 63         : 315      |                                        |
        64 -> 127        : 98       |                                        |
       128 -> 255        : 99       |                                        |
       256 -> 511        : 9        |                                        |
       512 -> 1023       : 15       |                                        |
      1024 -> 2047       : 6        |                                        |
      2048 -> 4095       : 2        |                                        |
      4096 -> 8191       : 3        |                                        |
      8192 -> 16383      : 1        |                                        |
     16384 -> 32767      : 1        |                                        |
     32768 -> 65535      : 2        |                                        |
     65536 -> 131071     : 88       |                                        |

对于这样一个轻负载的系统，输出可能令人惊讶：在 65 到 131 毫秒范围内有 88 个事件，似乎存在很高的调度延迟。事实证明，此实例受到管理程序的 CPU 限制，从而注入了计划程序延迟。

选项包括：

■ -m：以毫秒为单位打印输出
■ -P：打印每个进程 ID 的直方图
■ --pidnss：打印每个 PID 命名空间的直方图
■ -p PID：仅跟踪此进程 ID
■ -T：在输出中包含时间戳

runqlat（8） 的工作原理是检测调度程序唤醒和上下文切换事件，以确定从唤醒到运行的时间。这些事件在繁忙的生产系统上可能非常频繁，每秒超过 100 万个事件。尽管 BPF 经过优化，但按照这些速率，即使每个事件增加 1 微秒也会导致明显的开销。请谨慎使用，并考虑改用 runqlen（8）。

# #6.6.17 runqlen
runqlen（8）14 是一个 BCC 和 bpftrace 工具，用于采样 CPU 运行队列的长度，计算有多少任务正在等待轮到它们，并将其呈现为线性直方图。这可用于进一步描述运行队列延迟问题，或作为更便宜的近似值。由于它在所有 CPU 上使用 99 赫兹的采样，因此开销通常可以忽略不计。另一方面，runqlat（8） 对每个上下文 switch 进行采样，每秒可以达到数百万个事件。

下面显示了在 2 个 CPU MySQL 数据库实例上运行的 BCC 的 runqlen（8），该实例在整个系统范围内的 CPU 利用率约为 15%（与前面使用 runqlat（8） 显示的实例相同）。runqlen（8） 的参数是 “10 1” 来设置 10 秒的间隔，并且只输出一次：

 # runqlen 10 1
 Sampling run queue length... Hit Ctrl-C to end.
     runqlen       : count     distribution
        0          : 1824     |****************************************|
        1          : 158      |***                                     |

此输出显示，在大部分时间里，运行队列长度为 0，大约 8% 的时间运行队列长度为 1，这意味着线程需要等待轮到它们。

选项包括：

■ -C：打印每个 CPU 的直方图
■ -O：打印运行队列占用率
■ -T：在输出中包含时间戳

Run queue occupancy （运行队列占用率） 是一个单独的指标，显示线程等待的时间百分比。当需要单个指标进行监控、警报和绘图时，这有时很有用。

# #6.6.18 softirqs
softirqs（8） 是一个 BCC 工具，它显示处理软 IRQ（软中断）所花费的时间。软中断的系统范围时间可以从不同的工具中轻松获得。例如，mpstat（1） 将其显示为 %soft。还有 /proc/softirqs 来显示软 IRQ 事件的计数。BCC softirqs（8） 工具的不同之处在于它可以显示每个软 IRQ 的时间而不是事件计数。

例如，从 2 个 CPU 数据库实例和 10 秒跟踪：

 # softirqs 10 1
 Tracing soft irq event time... Hit Ctrl-C to end.
 SOFTIRQ          TOTAL_usecs
 net_tx                     9
 rcu                      751
 sched                   3431
 timer                   5542
 tasklet                11368
 net_rx                 12225

此输出显示 net_rx 花费的时间最长，总计 12 毫秒。这为在典型 CPU 配置文件中可能不可见的 CPU 使用者提供了见解，因为这些例程可能无法被 CPU 性能分析器中断。

选项包括：
■ -d：将 IRQ 时间显示为直方图 
■ - T：在输出中包含时间戳

-d 选项可用于显示每个 IRQ 事件的时间分布。

# #6.6.19 hardirqs
hardirqs（8）是一个 BCC 工具，它显示处理硬 IRQ（硬中断）所花费的时间。硬中断中的系统范围时间可以从不同的工具中轻松获得。例如，mpstat（1） 将其显示为 %irq。还有 /proc/interrupts 来显示硬 IRQ 事件的计数。BCC hardirqs（8） 工具的不同之处在于它可以显示每个硬 IRQ 的时间而不是事件计数。

例如，从 2 个 CPU 数据库实例和 10 秒跟踪：

# hardirqs 10 1
 Tracing hard irq event time... Hit Ctrl-C to end.
 HARDIRQ                    TOTAL_usecs
 nvme0q2                             35
 ena-mgmnt@pci:0000:00:05.0          72
 ens5-Tx-Rx-1                       326
 nvme0q1                            878
 ens5-Tx-Rx-0                      5922

输出显示，在跟踪时，为 ens5-Tx-Rx-0 IRQ（网络）提供服务花费了 5.9 毫秒。与 softirqs（8） 一样，这可以显示通常不包含在 CPU 分析中的 CPU 使用者。

hardirqs（8） 与 softirqs（8） 有类似的选项。

# #6.6.20 bpftrace
bpftrace 是一种基于 BPF 的跟踪器，它提供了一种高级编程语言，允许创建强大的单行代码和简短脚本。它非常适合根据其他工具的线索进行自定义应用程序分析。在 bpftrace 存储库 [Iovisor 20a] 中有早期工具 runqlat（8） 和 runqlen（8） 的 bpftrace 版本。

bpftrace 在第 15 章中进行了解释。本节介绍一些用于 CPU 分析的单行示例。

# 单行
以下单行代码非常有用，并演示了不同的 bpftrace 功能。

使用参数跟踪新进程
# bpftrace -e 'tracepoint:syscalls:sys_enter_execve { join(args->argv); }'

按进程对 syscall 进行计数：
# bpftrace -e 'tracepoint:raw_syscalls:sys_enter { @[pid, comm] = count(); }'

按 syscall 探测名称对 syscall 进行计数：
# bpftrace -e 'tracepoint:syscalls:sys_enter_* { @[probe] = count(); }'

99 赫兹的运行进程名称示例：
# bpftrace -e 'profile:hz:99 { @[comm] = count(); }'

系统范围内 49 赫兹的示例用户和内核堆栈，进程名称为：
# bpftrace -e 'profile:hz:49 { @[kstack, ustack, comm] = count(); }'

49 赫兹的用户级堆栈示例，对于 PID 189：
# bpftrace -e 'profile:hz:49 /pid == 189/ { @[ustack] = count(); }'

对于 PID 189，用户级堆栈以 49 赫兹深度 5 帧进行采样：
# bpftrace -e 'profile:hz:49 /pid == 189/ { @[ustack(5)] = count(); }'

49 赫兹的用户级堆栈示例，适用于名为 “mysqld” 的进程：
# bpftrace -e 'profile:hz:49 /comm == "mysqld"/ { @[ustack] = count(); }'

对内核 CPU 调度程序跟踪点进行计数
#  bpftrace -e 'tracepont:sched:* { @[probe] = count(); }'

计算上下文切换事件的 CPU 外内核堆栈：
# bpftrace -e 'tracepont:sched:sched_switch { @[kstack] = count(); }'

对以 “vfs_” 开头的内核函数调用进行计数：
#  bpftrace -e 'kprobe:vfs_* { @[func] = count(); }'

通过 pthread_create（） 跟踪新线程：
# bpftrace -e 'u:/lib/x86_64-linux-gnu/libpthread-2.27.so:pthread_create {printf("%s by %s (%d)\n", probe, comm, pid); }'

# 例子
下面显示了 bpftrace 以 49 赫兹的频率分析 MySQL 数据库服务器，并仅收集用户堆栈的前三个级别

 # bpftrace -e 'profile:hz:49 /comm == "mysqld"/ { @[ustack(3)] = count(); }'
 Attaching 1 probe...
 ^C
 [...]
 @[
    my_lengthsp_8bit(CHARSET_INFO const*, char const*, unsigned long)+32
    Field::send_to_protocol(Protocol*) const+194
    THD::send_result_set_row(List<Item>*)+203
 ]: 8
 @[
    ppoll+166
    vio_socket_io_wait(Vio*, enum_vio_io_event)+22
    vio_read(Vio*, unsigned char*, unsigned long)+236
 ]: 10
 [...]

输出被截断为仅包含两个堆栈，采样 8 次和 10 次。这两者都显示联网所花费的 CPU 时间。

# 调度内部
如果需要，您可以开发自定义工具来显示 CPU 计划程序的行为。首先尝试跟踪点。列出它们：

 # bpftrace -l 'tracepoint:sched:*'
 tracepoint:sched:sched_kthread_stop
 tracepoint:sched:sched_kthread_stop_ret
 tracepoint:sched:sched_waking
 tracepoint:sched:sched_wakeup
 tracepoint:sched:sched_wakeup_new
 tracepoint:sched:sched_switch
 tracepoint:sched:sched_migrate_task
 tracepoint:sched:sched_process_free
 [...]

每个参数都有可以使用 -lv 列出的参数。如果跟踪点不足，请考虑使用 kprobes 的动态检测。列出以 “sched” 开头的内核函数的 kprobe 目标：

 # bpftrace -lv 'kprobe:sched*'
 kprobe:sched_itmt_update_handler
 kprobe:sched_set_itmt_support
 kprobe:sched_clear_itmt_support
 kprobe:sched_set_itmt_core_prio
 kprobe:schedule_on_each_cpu
 kprobe:sched_copy_attr
 kprobe:sched_free_group
 [...]

在这个内核版本 （5.3） 上，有 24 个 sched 跟踪点和 104 个以 “sched” 开头的可能 kprobe。

由于计划程序事件可能很频繁，因此检测它们可能会消耗大量开销。请谨慎行事，并找到减少此开销的方法：使用地图来汇总统计数据，而不是打印每个事件的详细信息，并尽可能少地跟踪事件。

# #6.6.21 其他工具
表 6.11 列出了本书其他章节和 BPF 性能工具 [Gregg 19] 中包含的 CPU 可观察性工具。

其他 Linux CPU 可观测性工具和源包括：

■ oprofile：John Levon 的原始 CPU 分析工具。
■ atop：包括更多系统范围的统计信息，并使用进程记帐来捕获短期进程的存在。
■ /proc/cpuinfo：可以读取此文件以查看处理器详细信息，包括时钟速度和特性标志。
■ lscpu：显示 CPU 架构信息。
■ lstopo：显示硬件拓扑（由 hwloc 软件包提供）。
■ cpupower：显示处理器电源状态。
■ getdelays.c：这是延迟核算可观察性的一个示例，包括每个进程的 CPU 调度程序延迟。第 4 章 可观测性工具中对此进行了演示。
■ valgrind：内存调试和分析工具包 [Valgrind 20]。它包含 callgrind，这是一个跟踪函数调用和收集调用图的工具，可以使用 kcachegrind 进行可视化;cachegrind 用于分析给定程序的硬件缓存使用情况。

图 6.18 显示了一个 SVG 的 lstopo（1） 输出示例。

此 lstopo（1） 可视化显示哪些逻辑 CPU 映射到哪些 CPU 内核（例如，CPU 0 和 4 映射到内核 0）。

另一个值得展示的工具是 cpupower（1） 的输出：

 # cpupower idle-info
 CPUidle driver: intel_idle
 CPUidle governor: menu
 analyzing CPU 0:
 Number of idle states: 9
 Available idle states: POLL C1 C1E C3 C6 C7s C8 C9 C10
 POLL:
 Flags/Description: CPUIDLE CORE POLL IDLE
 Latency: 0
 Usage: 80442
 Duration: 36139954
 C1:
 Flags/Description: MWAIT 0x00
 Latency: 2
 Usage: 3832139
 Duration: 542192027
 C1E:
 Flags/Description: MWAIT 0x01
 Latency: 10
 Usage: 10701293
 Duration: 1912665723
 [...]
 C10:
 Flags/Description: MWAIT 0x60
 Latency: 890
 Usage: 7179306
 Duration: 48777395993

这不仅列出了处理器电源状态，还提供了一些统计数据：Usage 显示进入状态的次数，Duration 是处于该状态所花费的时间（以微秒为单位），Latency 是退出延迟（以微秒为单位）。这只显示 CPU 0：你可以从它们的 /sys 文件中看到所有 CPU，例如，持续时间可以从 /sys/devices/system/cpu/ cpu*/cpuidle/state0/time [Wysocki 19] 中读取。

还有一些用于 CPU 性能分析的复杂产品，特别是 Intel vTune [22] 和 AMD uprof [23]。

# GPUS
目前还没有一套全面的 GPU 分析工具。GPU 供应商通常会发布仅适用于他们自己产品的特定工具。示例包括

■ nvidia-smi、nvperf 和 Nvidia Visual Profiler：适用于 Nvidia GPU 
■ intel_gpu_top 和 Intel vTune：适用于 Intel GPU 
■ radeontop：适用于 Radeon GPU

这些工具提供基本的可观测性统计数据，例如指令率和 GPU 资源利用率。其他可能的可观察性来源是 PMC 和跟踪点 （try perf list | grep gpu）。

GPU 分析与 CPU 分析不同，因为 GPU 没有显示代码路径祖先的堆栈跟踪。相反，分析器可以检测 API 和内存传输调用及其计时。

# 6.7 可视化
CPU 使用率历来被可视化为利用率或负载平均值的折线图，包括原始的 X11 加载工具 （xload（1））。这样的折线图是显示变化的有效方法，因为可以直观地比较星等。它们还可以显示随时间变化的模式，如第 2 章 方法论， 第 2.9 节 监视所示。

但是，每个 CPU 利用率的折线图不能随我们今天看到的 CPU 数量而扩展，尤其是对于涉及数万个 CPU 的云计算环境，10,000 条线的图表可能会变成油漆

绘制为折线图的其他统计数据（包括平均值、标准差、最大值和百分位数）提供一些值并进行缩放。但是，CPU 利用率通常是双峰的，由一些空闲或接近空闲的 CPU 和一些利用率为 100% 的 CPU 组成，这些统计数据并不能有效地传达这一点。通常需要研究完全分布。利用率热图使这成为可能。

以下部分介绍了 CPU 利用率热图、CPU 亚秒级偏移热图、火焰图和 FlameScope。我创建了这些可视化类型来解决企业和云性能分析中的问题

# #6.7.1 利用率热图
利用率与时间的关系可以表示为热图，每个像素的饱和度（暗度）显示该利用率和时间范围内的 CPU 数量 [Gregg 10a]。热图在第 2 章 方法中介绍。

图 6.19 显示了运行公共云环境的整个数据中心的 CPU 利用率。它包括 300 多台物理服务器和 5312 个 CPU

此热图底部较暗的阴影显示大多数 CPU 的利用率在 0% 到 30% 之间。但是，顶部的实线显示，随着时间的推移，也有一些 CPU 的利用率为 100%。这条线是深色的事实表明，多个 CPU 处于 100%，而不仅仅是一个。

# #6.7.2 亚秒级偏移热图
此热图类型允许在一秒钟内检查活动。CPU 活动通常以微秒或毫秒为单位;将此数据报告为一整秒的平均值可能会消除有用的信息。亚秒级偏移量热图将亚秒级偏移量放在 y 轴上，饱和度显示每个偏移量处的非空闲 CPU 数量。这将每一秒可视化为一列，从下到上 “绘制” 它。

图 6.20 显示了云数据库 （Riak） 的 CPU 亚秒级偏移热图

此热图的有趣之处不在于 CPU 忙于为数据库提供服务的时间，而在于它们不忙于为数据库提供服务的时间，由白色列表示。这些间隙的持续时间也很有趣：数百毫秒，在此期间，没有一个数据库线程在 CPU 上。这导致发现了一个锁定问题，即整个数据库一次被阻止数百毫秒。

如果我们使用折线图检查这些数据，则每秒 CPU 利用率的下降可能会被视为可变负载，无需进一步调查。

# #6.7.3 火焰图
分析堆栈跟踪是解释 CPU 使用率的有效方法，可显示哪些内核或用户级别的代码路径负责。但是，它可以生成数千页的输出。CPU 火焰图可视化了配置文件堆栈帧，以便可以更快、更清楚地了解 CPU 使用率 [Gregg 16b]。图 6.21 中的示例显示了使用 perf（1） 作为 CPU 火焰图的 Linux 内核 pro。

火焰图可以从任何包含堆栈跟踪的 CPU 配置文件构建，包括来自 perf（1）、profile（8）、bpftrace 等的配置文件。火焰图还可以可视化 CPU 配置文件以外的配置文件。本节介绍由 flamegraph.pl [Gregg 20g] 生成的 CPU 火焰图。（还有许多其他实现，包括我的同事 Martin Spier [Spier 20a] 创建的 d3 火焰图。

# 特性
CPU 火焰图具有以下特性：
■ 每个框代表堆栈中的一个函数（“堆栈帧”）。 
■ y 轴显示堆栈深度（堆栈上的帧数）。最上面的框显示 CPU 上的函数。这下面的一切都是祖先。函数下面的函数是其父级，就像前面显示的堆栈跟踪一样。 
■ x 轴跨越样本总体。它不像大多数图表那样显示时间从左到右的流逝。从左到右的顺序没有意义（它按字母顺序排序）。框的宽度显示函数在 CPU 上的总时间或属于在 CPU 上的祖先的一部分（基于样本计数）。较宽的 box 函数可能比narrow box 函数，或者它们可能只是被更频繁地调用。不显示调用计数（也无法通过采样了解）。

如果多个线程并行运行和采样，则样本计数可能会超过运行时间。

# 颜色
可以根据不同的方案对框架进行着色。图 6.21 中所示的默认值为每一帧使用随机暖色，这有助于在视觉上区分相邻的塔。多年来，我添加了更多的配色方案。我发现以下内容对 Flame Graph 最终用户最有用：

■ 色调：色调表示代码类型。17 例如，红色表示本机用户级代码，橙色表示本机内核级代码，黄色表示 C++，绿色表示解释函数，浅绿色表示内联函数，依此类推，具体取决于您使用的语言。洋红色用于突出显示搜索匹配项。一些开发人员自定义了火焰图，使其始终以某种色调突出显示自己的代码，使其突出显示。
■ 饱和度：饱和度从函数名称中散列。它提供了一些颜色变化，有助于区分相邻的塔，同时为函数名称保留相同的颜色，以便更轻松地比较多个火焰图。
■ 背景颜色：背景颜色提供火焰图类型的视觉提醒。例如，您可以将黄色用于 CPU 火焰图，将蓝色用于非 CPU 或 I/O 火焰图，将绿色用于内存火焰图。

另一个有用的配色方案是用于 IPC （每周期指令数） 火焰图的配色方案，其中通过使用从蓝色到白色再到红色的渐变对每一帧进行着色来可视化另一个维度 IPC。

# 互动
火焰图是交互式的。我最初的 flamegraph.pl 生成了一个带有嵌入式 JavaScript 例程的 SVG，在浏览器中打开时，您可以将鼠标悬停在元素上以显示底部的细节，以及其他交互功能。在图 6.21 示例中，start_xmit（） 被突出显示，这表明它存在于 72.55% 的采样堆栈中。

您还可以单击以 zoom18 和 Ctrl-F 搜索 19 术语。搜索时，还会显示累积百分比，以指示包含该搜索词的堆栈跟踪出现的频率。这使得计算特定代码区域中的配置文件量变得微不足道。例如，您可以搜索 “tcp_” 来显示内核 TCP 代码中的内容

# 解释
要详细解释火焰图，请考虑图 6.22 中所示的简单合成 CPU 火焰图。

顶部边缘已用一行突出显示：这显示了直接在 CPU 上运行的函数。func_c（） 在 70% 的时间内直接处于 CPU 状态，func_b（） 在 20% 的时间内处于 CPU 状态，func_e（） 在 10% 的时间内处于 CPU 状态。其他函数 func_a（） 和 func_d（） 从未直接在 CPU 上采样。

要阅读火焰图，请寻找最宽的塔并首先了解它们。在图 6.22 中，它是代码路径 func_a（） -> func_b（） -> func_c（）。在图 6.21 火焰图中，它是以 iowrite16（） 平台结束的代码路径。

对于包含数千个样本的大型配置文件，可能存在仅采样几次的代码路径，并且打印在如此狭窄的塔中，以至于没有空间包含函数名称。事实证明这是一个好处：您的注意力自然会被具有清晰功能名称的较宽塔所吸引，查看它们有助于您首先了解配置文件的大部分内容

请注意，对于递归函数，每个级别由单独的帧显示

第 6.5.4 节 性能分析包含了更多的解释技巧，第 6.6.13 节 perf 展示了如何使用 perf（1） 创建它们。

# #6.7.4 FlameScope （火焰示波器）
FlameScope 是 Netflix 开发的一个开源工具，它结合了前两个可视化方法：亚秒偏移热图和火焰图 [Gregg 18b]。亚秒级偏移热图显示 CPU 配置文件，可以选择包括亚秒范围的范围来显示该范围的火焰图。图 6.23 显示了带有注释和说明的 FlameScope 热图。

FlameScope 适用于研究问题或扰动和方差。这些可能太小而无法在 CPU 配置文件中看到，CPU 配置文件会立即显示完整的配置文件：在 30 秒的配置文件中，100 毫秒的 CPU 扰动将仅跨越火焰图宽度的 0.3%。在 FlameScope 中，100 毫秒的扰动将显示为热图高度的 1/10 的垂直条带。在图 6.23 的例子中可以看到几个这样的扰动。选中后，将仅显示这些时间范围的 CPU 火焰图，其中显示负责的代码路径。

FlameScope 是开源的 [Netflix 19]，并已被用于在 Netflix 上获得许多性能胜利。

# 6.8 实验
本节介绍用于主动测试 CPU 性能的工具。有关背景信息，请参见 Section 6.5.11， Micro Benchmarking。

当使用这些工具时，最好让 mpstat（1） 持续运行以确认 CPU 使用率和并行度。

# #6.8.1 临时
虽然这很简单，而且不能测量任何东西，但它可能是一个有用的已知工作负载，用于确认可观测性工具显示它们声称显示的内容。这将创建一个受 CPU 限制的单线程工作负载（“hot on one CPU”）：

 # while :; do :; done &

这是一个在后台执行无限循环的 Bourne shell 程序。一旦你不再需要它，它就需要被杀死。

# #6.8.2 系统工作台
SysBench 系统基准测试套件具有一个简单的 CPU 基准测试工具，用于计算素数。例如：

 # sysbench --num-threads=8 --test=cpu --cpu-max-prime=100000 run
 sysbench 0.4.12:  multi-threaded system evaluation benchmark
 Running the test with following options:
 Number of threads: 8
 Doing CPU performance benchmark
 Threads started!
 Done.
 Maximum prime number checked in CPU test: 100000
 Test execution summary:
    total time:                          30.4125s
    total number of events:              10000
    total time taken by event execution: 243.2310
    per-request statistics:
         min:                                 24.31ms
         avg:                                 24.32ms
         max:                                 32.44ms
         approx.  95 percentile:              24.32ms
 Threads fairness:
    events (avg/stddev):           1250.0000/1.22
    execution time (avg/stddev):   30.4039/0.01 

这执行了 8 个线程，最大素数为 100,000。运行时间为 30.4 秒，可用于与其他系统或配置的结果进行比较（假设很多事情，例如使用相同的编译器选项来构建软件;参见第 12 章 基准测试）。

# 6.9 Tuning
对于 CPU，最大的性能优势通常是那些消除不必要工作的性能，这是一种有效的调优形式。第 6.5 节 方法和第 6.6 节 可观测性工具 介绍了许多分析和识别所执行工作的方法，帮助您找到任何不必要的工作。还引入了其他优化方法：优先级优化和 CPU 绑定。本节包括这些示例和其他优化示例。

优化的细节（可用选项及其设置内容）取决于进程类型、操作系统版本和预期的工作负载。下面按类型组织，提供了哪些选项可能可用以及如何调整这些选项的示例。前面的方法部分提供了有关何时以及为何调整这些可调参数的指导。

# #6.9.1 编译器选项
编译器及其为代码优化提供的选项可能会对 CPU 性能产生巨大影响。常用选项包括 64 位而不是 32 位编译，以及选择优化级别。编译器优化在 第 5 章 应用程序 中讨论。

# #6.9.2 调度优先级和类
nice（1） 命令可用于调整进程优先级。正 nice 值会降低优先级，负 nice 值（只有超级用户才能设置）会增加优先级。范围为 -20 到 19。例如：

# $ nice -n 19 command

以 nice 值 19 运行命令，这是 nice 可以设置的最低优先级。要更改已运行的进程的优先级，请使用 renice（1）

在 Linux 上，chrt（1） 命令可以直接显示和设置调度优先级和调度策略。例如：

# $ chrt -b command

将在 SCHED_BATCH中运行该命令（请参见部分 6.4.2， 软件中的调度类）。nice（1） 和 chrt（1） 也可以指向 PID 而不是启动命令（参见它们的手册页）。

调度优先级也可以使用 setpriority（2） 系统调用直接设置，优先级和调度策略可以使用 sched_setscheduler（2） 系统调用来设置。

# #6.9.3 调度选项
您的内核可能会提供可调参数来控制调度程序行为，尽管这些参数不太可能需要调整。

在 Linux 系统上，各种 CONFIG 选项在较高级别控制调度程序行为，并且可以在内核编译期间进行设置。表 6.12 显示了 Ubuntu 19.10 和 Linux 5.3 内核的示例。

还有调度器 sysctl（8） 可调参数可以在正在运行的系统上实时设置，包括表 6.13 中列出的那些，默认值来自同一个 Ubuntu 系统。

这些 sysctl（8） 可调参数也可以在 /proc/sys/sched 中设置。

# #6.9.4 扩展调控器
Linux 支持不同的 CPU 缩放调节器，这些调节器通过 soft ware（内核）控制 CPU 时钟频率。这些可以通过 /sys 文件进行设置。例如，对于 CPU 0：

 # cat /sys/devices/system/cpu/cpufreq/policy0/scaling_available_governors
 performance powersave
 # cat /sys/devices/system/cpu/cpufreq/policy0/scaling_governor 
powersave

这是一个未调谐系统的示例：电流调节器是 “powersave”，它将使用较低的 CPU 频率来节省功率。这可以设置为 “performance” 以始终使用最大频率。例如：

# echo performance > /sys/devices/system/cpu/cpufreq/policy0/scaling_governor

必须对所有 CPU 执行此作 （policy0..此策略目录还包含用于直接设置频率 （scaling_setspeed） 和确定可能频率范围 （scaling_min_freq、scaling_max_freq） 的文件。

将 CPU 设置为始终以最大频率运行可能会给环境带来巨大的成本。如果此设置没有显著的性能改进，请考虑继续使用 powersave 设置以保护地球。对于有权访问电源 MSR 的主机（云客户机可以对其进行筛选），您还可以使用这些 MSR 来检测在有和没有最大 CPU 频率设置的情况下消耗的功率，以量化（20 的一部分）环境成本。

# #6.9.5 电源状态
处理器电源状态可以使用 cpupower（1） 工具启用和禁用。如前面的 Section 6.6.21， Other Tools 中所示，较深的睡眠状态可能具有较高的退出延迟（显示 C10 为 890 μs）。可以使用 -d 禁用单个状态，而 -D latency 将禁用退出延迟高于给定值（以微秒为单位）的所有状态。这允许您微调可以使用的低功耗状态，从而禁用那些延迟过长的状态。

# #6.9.6 CPU 绑定
一个进程可以绑定到一个或多个 CPU，这可能会通过提高缓存热量和内存局部性来提高其性能。

在 Linux 上，可以使用 taskset（1） 命令执行此作，该命令使用 CPU 掩码或范围来设置 CPU 关联性。例如：

# $ taskset -pc 7-10 10790
 pid 10790's current affinity list: 0-15
 pid 10790's new affinity list: 7-10

这会将 PID 10790 设置为仅在 CPU 7 到 10 上运行。

numactl（8） 命令还可以设置 CPU 绑定和内存节点绑定（参见第 7 章 内存， 第 7.6.4 节 NUMA 绑定）。

# #6.9.7 独占 CPU 设置
Linux 提供了 cpusets，它允许对 CPU 进行分组并为其分配进程。这可以像 CPU 绑定一样提高性能，但可以通过将 cpuset 设为独占来进一步提高性能，从而防止其他进程使用它。权衡是系统其余部分的可用 CPU 减少。

以下注释示例创建独占集：

# mount -t cgroup -ocpuset cpuset /sys/fs/cgroup/cpuset  # may not be necessary
 # cd /sys/fs/cgroup/cpuset
 # mkdir prodset
                    # create a cpuset called "prodset"
 # cd prodset
 # echo 7-10 > cpuset.cpus
 # echo 1 > cpuset.cpu_exclusive
 # echo 1159 > tasks
          # assign CPUs 7-10
    # make prodset exclusive
         # assign PID 1159 to prodset

有关参考，请参见 cpuset（7） 手册页。

在创建 CPU 集时，您可能还希望研究哪些 CPU 将继续为内部 rupts 提供服务。irqbalance（1） 守护进程将尝试在 CPU 之间分配中断以提高性能。您可以通过 /proc/irq/IRQ/smp_affinity 文件通过 IRQ 手动设置 CPU 关联性。

# #6.9.8 资源控制
除了将进程与整个 CPU 相关联外，现代操作系统还提供资源控制，用于精细分配 CPU 使用率。

对于 Linux，有控制组 （cgroups），它还可以控制进程或进程组的资源使用情况。可以使用份额来控制 CPU 使用率，并且 CFS 调度程序允许施加固定限制（CPU 带宽），即每个间隔分配微秒级的 CPU 周期。

第 11 章 云计算 介绍了管理操作系统虚拟化租户的 CPU 使用情况的使用案例，包括如何协同使用份额和限制。

# #6.9.9 安全引导选项
针对 Meltdown 和 Spectre 安全漏洞的各种内核缓解措施具有降低性能的副作用。在某些情况下，安全性不是必需的，但高性能是必需的，并且您希望禁用这些缓解措施。因为不建议这样做（由于安全风险），所以我不会在此处包含所有选项;但你应该知道它们存在。它们是 grub 命令行选项，包括 nospectre_v1 和nospectre_v2。这些记录在 Documentation/admin-guide/kernel-parameters.txt 的 Linux 源代码 [Linux 20f] 中;摘录

nospectre_v1    [PPC] Disable mitigations for Spectre Variant 1 (bounds
                        check bypass). With this option data leaks are possible
                        in the system.
        nospectre_v2    [X86,PPC_FSL_BOOK3E,ARM64] Disable all mitigations for
                        the Spectre variant 2 (indirect branch prediction)
                        vulnerability. System may allow data leaks with this
                        option.
还有一个网站列出了它们：https://make-linux-fast-again.com。此 Web 站点缺少内核文档中列出的警告。

# #6.9.10 处理器选项（BIOS Tuning）
处理器通常提供启用、禁用和调整处理器级功能的设置。在 x86 系统上，通常在启动时通过 BIOS 设置菜单访问这些内容。

默认情况下，这些设置通常提供最佳性能，无需调整。我今天调整这些的最常见原因是禁用 Intel Turbo Boost，以便 CPU 基准标记以一致的时钟速率执行（请记住，对于生产用途，应启用 Turbo Boost 以获得稍微更快的性能）。

# 6.10 练习
1. 回答以下有关 CPU 术语的问题： 
■ 进程和处理器有什么区别？
■ 什么是硬件线程？
■ 什么是运行队列？
■ 用户时间和内核时间有什么区别？

2. 回答以下概念性问题：
■ 描述 CPU 利用率和饱和度。
■ 描述指令管道如何提高 CPU 吞吐量。
■ 描述处理器指令宽度如何提高 CPU 吞吐量。
■ 描述多进程和多线程模型的优势

3. 回答以下更深层次的问题：
■ 描述当系统 CPU 因可运行工作而过载时会发生什么，包括对应用程序性能的影响
■ 当没有可运行的工作要执行时，CPU 做什么？
■ 当收到可疑的 CPU 性能问题时，请说出您在调查早期将使用的两种方法，并解释原因。

4. 为您的环境制定以下程序： 
■ CPU 资源的 USE 方法检查表。包括如何获取每个指标（例如，要执行哪个命令）以及如何解释结果。在安装或使用其他软件产品之前，请尝试使用现有的操作系统可观测性工具。
■ CPU 资源的工作负载特征检查表。包括如何获取每个指标，并首先尝试使用现有的操作系统可观测性工具。

5. 执行以下任务：
■ 计算以下系统的负载平均值，其负载处于稳定状态，没有明显的磁盘/锁定负载： 
   ■ 系统有 64 个 CPU。
   ■ 系统范围的 CPU 利用率为 50%。
   ■ 系统范围的 CPU 饱和度（以平均可运行线程和排队线程的总数来衡量）为 2.0。
■ 选择一个应用程序，并分析其用户级 CPU 使用率。显示哪些代码路径消耗的 CPU 最多

6. （可选，高级）开发 bustop（1） — 一个显示物理总线或互连利用率的工具 — 其演示方式类似于 iostat（1）：总线列表、每个方向的吞吐量列和利用率。如果可能，请包括 saturation 和 error metrics。这将需要使用 PMC

# 6.11 引用
 [Saltzer 70] Saltzer, J., and Gintell, J., “The Instrumentation of Multics,” Communications of 
the ACM, August 1970.
 [Bobrow 72] Bobrow, D. G., Burchfiel, J. D., Murphy, D. L., and Tomlinson, R. S., “TENEX: 
A Paged Time Sharing System for the PDP-10*,” Communications of the ACM, March 1972.
 [Myer 73] Myer, T. H., Barnaby, J. R., and Plummer, W. W., TENEX Executive Manual, Bolt, 
Baranek and Newman, Inc., April 1973.
 [Thomas 73] Thomas, B., “RFC 546: TENEX Load Averages for July 1973,” Network Working 
Group, http://tools.ietf.org/html/rfc546, 1973.
 [TUHS 73] “V4,” The Unix Heritage Society, http://minnie.tuhs.org/cgi-bin/utree.pl?file=V4, 
materials from 1973.
 [Hinnant 84] Hinnant, D., “Benchmarking UNIX Systems,” BYTE magazine 9, no. 8, August 
1984.
 [Bulpin 05] Bulpin, J., and Pratt, I., “Hyper-Threading Aware Process Scheduling Heuristics,” 
USENIX, 2005. 
  [Corbet 06a] Corbet, J., “Priority inheritance in the kernel,” LWN.net, http://lwn.net/
 Articles/178253, 2006.
 [Otto 06] Otto, E., “Temperature-Aware Operating System Scheduling,” University of Virginia 
(Thesis), 2006. 
[Ruggiero 08] Ruggiero, J., “Measuring Cache and Memory Latency and CPU to Memory 
Bandwidth,” Intel (Whitepaper), 2008.
 [Intel 09] “An Introduction to the Intel QuickPath Interconnect,” Intel (Whitepaper), 2009.
 [Levinthal 09] Levinthal, D., “Performance Analysis Guide for Intel® Core™ i7 Processor and 
Intel® Xeon™ 5500 Processors,” Intel (Whitepaper), 2009.
 [Gregg 10a] Gregg, B., “Visualizing System Latency,” Communications of the ACM, July 2010.
 [Weaver 11] Weaver, V., “The Unofficial Linux Perf Events Web-Page,” http://web.eece.maine.edu/
 ~vweaver/projects/perf_events, 2011.
 [McVoy 12] McVoy, L., “LMbench - Tools for Performance Analysis,” http://www.bitmover.
 com/lmbench, 2012.
 [Stevens 13] Stevens, W. R., and Rago, S., Advanced Programming in the UNIX Environment, 3rd 
Edition, Addison-Wesley 2013.
 [Perf 15] “Tutorial: Linux kernel profiling with perf,” perf wiki, https://perf.wiki.kernel.org/
 index.php/Tutorial, last updated 2015.
 [Gregg 16b] Gregg, B., “The Flame Graph,” Communications of the ACM, Volume 59, Issue 6, 
pp. 48–57, June 2016.
 [ACPI 17] Advanced Configuration and Power Interface (ACPI) Specification, https://uefi.org/
 sites/default/files/resources/ACPI%206_2_A_Sept29.pdf, 2017.
 [Gregg 17b] Gregg, B., “CPU Utilization Is Wrong,” http://www.brendangregg.com/
 blog/2017-05-09/cpu-utilization-is-wrong.html, 2017.
 [Gregg 17c] Gregg, B., “Linux Load Averages: Solving the Mystery,” http://
 www.brendangregg.com/blog/2017-08-08/linux-load-averages.html, 2017.
 [Mulnix 17] Mulnix, D., “Intel® Xeon® Processor Scalable Family Technical Overview,” 
https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical
overview, 2017.
 [Gregg 18b] Gregg, B., “Netflix FlameScope,” Netflix Technology Blog, https://
 netflixtechblog.com/netflix-flamescope-a57ca19d47bb, 2018.
 [Ather 19] Ather, A., “General Purpose GPU Computing,” http://
 techblog.cloudperf.net/2019/12/general-purpose-gpu-computing.html, 2019.
 [Gregg 19] Gregg, B., BPF Performance Tools: Linux System and Application Observability, 
Addison-Wesley, 2019.
 [Intel 19a] Intel 64 and IA-32 Architectures Software Developer’s Manual, Combined Volumes 1, 
2A, 2B, 2C, 3A, 3B, and 3C. Intel, 2019.
  [Intel 19b] Intel 64 and IA-32 Architectures Software Developer’s Manual, Volume 3B, System 
Programming Guide, Part 2. Intel, 2019.
 [Netflix 19] “FlameScope Is a Visualization Tool for Exploring Different Time Ranges as 
Flame Graphs,” https://github.com/Netflix/flamescope, 2019.
 [Wysocki 19] Wysocki, R., “CPU Idle Time Management,” Linux documentation, https://
 www.kernel.org/doc/html/latest/driver-api/pm/cpuidle.html, 2019.
 [AMD 20] “AMD μProf,” https://developer.amd.com/amd-uprof, accessed 2020.
 [Gregg 20d] Gregg, B., “MSR Cloud Tools,” https://github.com/brendangregg/msr-cloud
tools, last updated 2020.
 [Gregg 20e] Gregg, B., “PMC (Performance Monitoring Counter) Tools for the Cloud,” 
https://github.com/brendangregg/pmc-cloud-tools, last updated 2020.
 [Gregg 20f] Gregg, B., “perf Examples,” http://www.brendangregg.com/perf.html, accessed 
2020.
 [Gregg 20g] Gregg, B., “FlameGraph: Stack Trace Visualizer,” https://github.com/
 brendangregg/FlameGraph, last updated 2020.
 [Intel 20a] “Product Specifications,” https://ark.intel.com, accessed 2020.
 [Intel 20b] “Intel® VTune™ Profiler,” https://software.intel.com/content/www/us/en/develop/
 tools/vtune-profiler.html, accessed 2020.
 [Iovisor 20a] “bpftrace: High-level Tracing Language for Linux eBPF,” https://github.com/
 iovisor/bpftrace, last updated 2020.
 [Linux 20f] “The Kernel’s Command-Line Parameters,” Linux documentation, https://
 www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html, accessed 2020.
 [Spier 20a] Spier, M., “A D3.js Plugin That Produces Flame Graphs from Hierarchical Data,” 
https://github.com/spiermar/d3-flame-graph, last updated 2020.
 [Spier 20b] Spier, M., “Template,” https://github.com/spiermar/d3-flame-graph#template, 
last updated 2020.
 [Valgrind 20] “Valgrind Documentation,” http://valgrind.org/docs/manual, May 2020.
 [Verma 20] Verma, A., “CUDA Cores vs Stream Processors Explained,” https://
 graphicscardhub.com/cuda-cores-vs-stream-processors, 2020.
